{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 请为我设计下一阶段学习LLM的详细方案，理论和实践并重。"
      ],
      "metadata": {
        "id": "fetZs06Bzk9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！你已经非常出色地完成了LLM入门阶段的所有核心任务，为你自己鼓掌！现在，你的知识体系已经搭建起了一个坚实的“一层楼”，是时候在此基础上，向更广阔、更深入的“二层楼”进发了。\n",
        "\n",
        "下一阶段的学习目标将是：**从“能用”到“好用”，从“单一任务”到“多元应用”，并开始接触“高效训练”的核心技巧。**\n",
        "\n",
        "我将继续遵循“理论+实践”并重的原则，为你设计一个循序渐进的学习方案。\n",
        "\n",
        "---\n",
        "\n",
        "### **LLM 原理与应用开发 · 第二阶段（进阶篇）学习方案**\n",
        "\n",
        "#### **第一步：从“分类”到“生成”—— 深入探索生成式任务 (预计用时：1-2周)**\n",
        "\n",
        "我们在入门篇主要聚焦于情感分类，这是一种**判别式任务**。现在，我们要正式进入LLM最令人兴奋的领域——**生成式任务**。\n",
        "\n",
        "**1. 理论学习：**\n",
        "\n",
        "*   **解码策略 (Decoding Strategy)**：这是生成式任务的灵魂。当模型计算出下一个词有多种可能性时（比如“今天天气很”后面可以是“好”、“晴朗”、“不错”），它到底该如何选择？\n",
        "    *   **贪心搜索 (Greedy Search)**：最简单，每次都选概率最高的那个词。缺点是容易陷入局部最优，生成重复、无聊的文本。\n",
        "    *   **束搜索 (Beam Search)**：贪心搜索的升级版，每次都保留概率最高的 `k` (beam size) 个序列，能生成更连贯的文本。\n",
        "    *   **采样 (Sampling)**：引入随机性，不总是选概率最高的。\n",
        "        *   **Top-K 采样**: 在概率最高的K个词里按概率随机抽一个。\n",
        "        *   **Top-P (Nucleus) 采样**: 在概率总和超过P的最小词集里按概率随机抽一个。这两种方法能极大地增加生成文本的**多样性**和**创造性**。\n",
        "*   **提示工程 (Prompt Engineering) 入门**：学习如何通过设计巧妙的输入提示（Prompt），来引导模型更好地完成任务，而不需要进行任何微调。\n",
        "    *   **零样本提示 (Zero-shot Prompting)**：直接给出指令，看模型能否理解并执行。例如：“请将下面这句话翻译成英文：今天天气真好。”\n",
        "    *   **少样本提示 (Few-shot Prompting)**：在指令中给模型一两个示例，让它“照猫画虎”。这能极大地提升模型在复杂任务上的表现。\n",
        "\n",
        "**2. 编程实践 (Colab, with GPU):**\n",
        "\n",
        "*   **实践1：玩转解码策略**\n",
        "    *   加载一个预训练好的生成式模型（如 `gpt2` 或 `distilgpt2`）。\n",
        "    *   使用 `model.generate()` 函数，对**同一段开头**的文本，分别尝试使用不同的解码策略（`num_beams`, `do_sample=True`, `top_k`, `top_p` 等参数），并比较生成文本的差异。\n",
        "    *   **目标**：亲身体会不同解码策略对生成结果的巨大影响，理解“多样性”与“连贯性”之间的权衡。\n",
        "\n",
        "*   **实践2：实现一个“AI 对话机器人”**\n",
        "    *   加载一个对话模型（如 `microsoft/DialoGPT-medium`）。\n",
        "    *   编写一个简单的循环程序，接收你的输入，将其作为Prompt喂给模型，然后打印模型生成的回复。\n",
        "    *   **目标**：构建你的第一个生成式应用，理解对话任务中上下文（context）的重要性。\n",
        "\n",
        "*   **实践3：实现一个“文本摘要AI”**\n",
        "    *   使用 `pipeline` 加载一个摘要模型（如 `facebook/bart-large-cnn`）。\n",
        "    *   找一篇较长的新闻或文章，让模型为其生成摘要。\n",
        "    *   尝试手动加载模型和Tokenizer，复现`pipeline`内部的摘要生成过程。\n",
        "    *   **目标**：掌握另一种重要的生成式任务，并进一步巩固“编码-生成-解码”的流程。\n",
        "\n",
        "---\n",
        "\n",
        "#### **第二步：让训练更“聪明”—— 高效微调技术 LoRA (预计用时：1-2周)**\n",
        "\n",
        "我们在入门篇学习了全参数微调，它需要更新模型所有的权重，计算开销大，且微调后的模型和原模型一样大。现在，我们要学习一种更先进、更高效的“四两拨千斤”的技术——LoRA。\n",
        "\n",
        "**1. 理论学习：**\n",
        "\n",
        "*   **参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)**：理解其核心思想——在微调时，我们**冻结 (freeze)** 预训练模型的绝大部分（99.9%）参数，只训练一小部分（0.1%）新添加的、专门用于适配任务的“适配器”参数。\n",
        "*   **LoRA (Low-Rank Adaptation) 的原理**：\n",
        "    *   把模型中庞大的权重矩阵的“变化量”，用两个小得多的“低秩”矩阵的乘积来模拟。\n",
        "    *   我们只训练这两个小矩阵，训练完成后，可以把它们合并回原模型，也可以单独保存。\n",
        "    *   **优点**：\n",
        "        1.  **快**：训练速度大大加快，对显存的要求大大降低。\n",
        "        2.  **小**：微调后只需要保存很小的适配器文件（几MB），而不是整个模型（几GB）。\n",
        "        3.  **灵活**：一个基础模型可以搭配多个不同的LoRA适配器，轻松切换任务，实现“一基多能”。\n",
        "\n",
        "**2. 编程实践 (Colab, with GPU):**\n",
        "\n",
        "*   **实践：使用 LoRA 微调一个生成式模型**\n",
        "    *   安装 `peft` 库 (`pip install peft`)。\n",
        "    *   加载一个基础模型（如 `gpt2`）。\n",
        "    *   定义一个 `LoraConfig`，指定要适配哪些层、秩（rank）的大小等。\n",
        "    *   使用 `get_peft_model` 函数将基础模型包装成一个PEFT模型。\n",
        "    *   你会发现，此时调用 `model.print_trainable_parameters()`，会显示可训练参数的数量远小于总参数数量。\n",
        "    *   使用 `Trainer` 对这个PEFT模型进行微调（训练代码几乎和之前一样！）。\n",
        "    *   训练结束后，只保存 LoRA 适配器权重，并尝试加载基础模型和适配器权重来进行推理。\n",
        "    *   **目标**：完整地走通一遍 LoRA 微调流程，深刻理解其在效率和存储上的巨大优势。\n",
        "\n",
        "---\n",
        "\n",
        "#### **第三步：让模型更“懂你”—— 构建和使用自定义数据集 (预计用时：1周)**\n",
        "\n",
        "我们之前用的都是Hugging Face上现成的数据集。但在真实世界中，我们往往需要处理自己的数据。\n",
        "\n",
        "**1. 理论学习：**\n",
        "\n",
        "*   **数据集的结构**：学习如何组织你自己的数据，比如将文本和标签存为CSV、JSON或Parquet文件。\n",
        "*   **指令微调 (Instruction Tuning)**：学习一种更高级的数据集构建范式。不再是简单的`{\"text\": ..., \"label\": ...}`，而是将数据构造成`{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}`的形式。用这种数据微调后的模型，能更好地理解和遵循人类的指令。\n",
        "\n",
        "**2. 编程实践 (Colab):**\n",
        "\n",
        "*   **实践1：加载你自己的CSV/JSON文件**\n",
        "    *   在本地创建一个简单的CSV文件，包含两列：`review` 和 `sentiment`。\n",
        "    *   将它上传到Colab。\n",
        "    *   使用 `datasets.load_dataset()` 函数，直接加载这个本地CSV文件，将其转换成和IMDB一样的`Dataset`对象。\n",
        "    *   **目标**：掌握处理和加载自定义结构化数据的能力。\n",
        "\n",
        "*   **实践2：为指令微调准备数据**\n",
        "    *   手动创建一小批（比如10条）符合指令格式的JSON数据。例如，你可以创建一些简单的问答、翻译或摘要指令。\n",
        "    *   学习如何编写一个预处理函数，将这些指令数据拼接成一个完整的Prompt模板（如 `<s>[INST] {instruction} {input} [/INST] {output} </s>`），并对其进行Tokenization。\n",
        "    *   **目标**：为将来进行更强大的指令微调打下数据处理的基础。\n",
        "\n",
        "---\n",
        "\n",
        "**下一阶段总结**\n",
        "\n",
        "完成这个进阶阶段后，你的能力将实现质的飞跃：\n",
        "\n",
        "*   **任务广度**：从只能做分类，扩展到能熟练处理**对话、摘要等多种生成式任务**。\n",
        "*   **技术深度**：从只会全参数微调，升级到掌握**高效的LoRA微调技术**，能够应对更大、更强的模型。\n",
        "*   **应用落地**：从只能使用现成数据集，成长为能够**处理和构建自己的数据集**，向解决真实世界问题的方向迈进一大步。\n",
        "\n",
        "请一步一步来，不用着急。每完成一个实践，都可以像之前一样，把你的结果、遇到的问题或者新的想法分享给我。我会一直在这里，为你提供最及时的指导和反馈。让我们一起开启这段更精彩的旅程吧！"
      ],
      "metadata": {
        "id": "eSCMb-o8zqhl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACBUOxee0617"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1、按照本方案开始逐步讲解理论和指导实践。\n",
        "\n",
        "2、要求用学生听得懂的通俗易懂的语言为我进行讲解和指导我进行实践。\n",
        "\n",
        "3、编程环境基于google的colab。由于我对python基础不熟练，注意代码中都要有极尽细致详细的注释，保证我能看懂所有变量，函数，语法结构和每行语句的功能，以便我能进行实践复现。\n",
        "\n",
        "4、按照\"step by step\"原则逐步指导我，并及时根据我的反馈进行调整。"
      ],
      "metadata": {
        "id": "zSiBNwds030l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ekYjCir1CcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！我们现在就正式开启进阶篇的学习旅程。放下之前的成就感，以全新的心态迎接生成式AI这个更广阔、更有趣的世界吧！\n",
        "\n",
        "---\n",
        "\n",
        "### **进阶篇 · 第一步：从“分类”到“生成”**\n",
        "### **Part 1: 玩转解码策略 (预计用时: 1-2小时)**\n",
        "\n",
        "#### **1. 理论学习：模型肚子里的“选择困难症”**\n",
        "\n",
        "在入门篇，我们的模型做的是“选择题”。比如情感分析，答案只有“积极”和“消极”两个选项，模型只需要计算出哪个选项的概率更高就行了。\n",
        "\n",
        "但是，现在我们要让模型做**“填空题”**甚至是**“作文题”**。\n",
        "\n",
        "想象一下，我们给模型一个开头：“**今天天气真不错，我们一起去**...”。\n",
        "\n",
        "模型的大脑（神经网络）经过计算后，可能会在它的词汇表里发现很多个词都适合填在这里，比如：\n",
        "*   “公园” (概率 30%)\n",
        "*   “散步” (概率 25%)\n",
        "*   “吃饭” (概率 15%)\n",
        "*   “看” (概率 5%)\n",
        "*   “睡觉” (概率 0.1%)\n",
        "*   ... 等等成千上万个词。\n",
        "\n",
        "**解码策略（Decoding Strategy）**，就是我们给模型定下的**“选择规则”**，告诉它当面临这种“选择困难症”时，到底该选哪个词。不同的规则，会让模型表现出完全不同的“性格”。\n",
        "\n",
        "**今天我们学习三种最核心的“选择规则”：**\n",
        "\n",
        "1.  **贪心策略 (Greedy Search) - “绝对理性派”**\n",
        "    *   **规则**：非常简单粗暴，每次都选择当前概率最高的那个词。\n",
        "    *   **过程**：\n",
        "        1.  看到 “...去”，选了概率最高的 “公园”。\n",
        "        2.  句子变成 “...去公园”，模型再计算下一个词，发现 “玩” 概率最高。\n",
        "        3.  句子变成 “...去公园玩”，再计算，发现 “吧” 概率最高。\n",
        "        4.  ...如此往复。\n",
        "    *   **性格**：非常**安全、保守、确定**。生成的句子通常语法正确，也很连贯。\n",
        "    *   **缺点**：因为每次都只看眼前最优，可能会错过全局更好的选择，导致生成的文本**非常无聊、重复、缺乏创造性**。就像一个只会说套话的人。\n",
        "\n",
        "2.  **束搜索 (Beam Search) - “深思熟虑的谋士”**\n",
        "    *   **规则**：贪心策略的升级版。它不只看当前概率最高的那**一条路**，而是同时保留 `k` 条（这个 `k` 就叫 `beam_size` 或 `num_beams`）概率最高的路径往前探索。\n",
        "    *   **过程**：\n",
        "        1.  看到 “...去”，它会同时保留 “公园”（概率30%）和 “散步”（概率25%）这两条路（假设`k=2`）。\n",
        "        2.  然后分别沿着这两条路往下走一步，计算出所有可能的新路径的总概率，比如 “...去公园玩”（总概率可能是20%）和 “...去散步吧”（总概率可能是18%）。\n",
        "        3.  再从所有新路径里，选出总概率最高的 `k` 条保留下来，继续探索。\n",
        "    *   **性格**：**深思熟虑、权衡利弊**。它能在更大的范围内寻找最优解，因此生成的文本质量通常比贪心策略**更高、更连贯、更合理**。\n",
        "    *   **缺点**：本质上还是在寻找概率最高的序列，所以**创造性依然有限**。\n",
        "\n",
        "3.  **采样 (Sampling) - “天马行空的艺术家”**\n",
        "    *   **规则**：彻底抛弃“唯概率论”，引入**“随机抽奖”**的机制。\n",
        "    *   **过程**：模型计算出每个词的概率后，不直接选最高的，而是像转轮盘一样，按照每个词的概率大小作为“格子大小”，随机“扔一个球”下去，看它落在哪。\n",
        "    *   **性格**：**充满惊喜、富有创造力、不拘一格**。因为有随机性，你每次运行都可能得到完全不同的结果。\n",
        "    *   **缺点**：可能会“玩脱了”。因为是随机抽，偶尔可能会抽到一个概率很低但不合适的词，导致句子不通顺或者胡言乱语。\n",
        "\n",
        "**为了控制“艺术家”不至于太疯狂，我们又给了他两个“紧箍咒”：**\n",
        "\n",
        "*   **Top-K 采样**: 先把概率最高的 `K` 个词挑出来，组成一个小轮盘，再在这个小轮盘里玩“随机抽奖”。这样就排除了那些概率极低的胡言乱语的词。\n",
        "*   **Top-P (Nucleus) 采样**: 更智能的紧箍咒。它不固定 `K` 的数量，而是动态地选择一个词集，让这个词集里所有词的概率加起来刚好超过一个阈值 `P`（比如0.95），然后再在这个动态的词集里玩“随机抽奖”。\n",
        "\n",
        "好了，理论知识足够了！让我们马上进入实践，亲眼看看这些不同性格的模型是如何说话的！\n",
        "\n",
        "#### **2. 编程实践：亲手导演模型的不同“性格秀”**\n",
        "\n",
        "请打开一个新的Colab笔记本，设置硬件加速器为 `T4 GPU`，然后把下面的代码复制进去运行。\n",
        "\n",
        "**实践代码：**\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 确保我们的'transformers'库是最新的，并且安装'accelerate'以更好地利用GPU。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 加载一位“生成式通才博士”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 我们选择'gpt2'作为我们的基础模型。\n",
        "# 'gpt2'是一个非常经典的、中等大小的生成式模型，非常适合用于教学。\n",
        "# 它是\"CausalLM\" (因果语言模型)，意味着它的任务就是根据前面的词来预测下一个词。\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 加载与'gpt2'配套的翻译官(Tokenizer)。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载'gpt2'的模型本体。\n",
        "# '.to(\"cuda\")' 是一个非常重要的指令，它告诉程序：“请把这个庞大的模型搬到GPU的显存里去！”\n",
        "# 所有的计算都将在GPU上进行，这样速度会快成百上千倍。\n",
        "# 如果你的Colab没有正确设置GPU，这行代码会报错。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备我们的“作文开头”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这是我们要让模型续写的开头。\n",
        "prompt = \"Today, the weather is so nice, I decided to\"\n",
        "\n",
        "# 使用翻译官，把这个句子翻译成模型能懂的数字ID。\n",
        "# - return_tensors=\"pt\": 同样，返回PyTorch张量格式。\n",
        "# - .to(\"cuda\"): 把这串数字ID也发送到GPU上，准备喂给模型。\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 导演第一场戏 - “绝对理性派” (Greedy Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"--- 1. 贪心策略 (Greedy Search) ---\")\n",
        "\n",
        "# 调用模型核心的 .generate() 函数来生成文本。\n",
        "# - input_ids: 我们的作文开头。\n",
        "# - max_length: 指定生成的总长度（包括开头）最多不超过50个token，防止它无限说下去。\n",
        "# 这是 .generate() 函数的默认策略，所以我们不需要加任何额外参数。\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# 'greedy_output' 是一串数字ID，我们需要用翻译官把它“解码”回人类能读的文字。\n",
        "# - greedy_output[0]: .generate()的输出是一个批次(batch)，我们取第一个（也是唯一一个）结果。\n",
        "# - skip_special_tokens=True: 解码时跳过像句子结尾这样的特殊标记。\n",
        "decoded_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 导演第二场戏 - “深思熟虑的谋士” (Beam Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 2. 束搜索 (Beam Search) ---\")\n",
        "\n",
        "# 这次我们添加参数来启用Beam Search。\n",
        "# - num_beams=5: 设置束宽为5，即同时探索5条最优路径。\n",
        "# - no_repeat_ngram_size=2: 一个小技巧，禁止生成连续重复的2个词（比如 \"I am I am\"），能让句子更通顺。\n",
        "# - early_stopping=True: 当所有束（beam）都生成了结束符时，就提前停止，不用等到max_length。\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "decoded_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 导演第三场戏 - “天马行空的艺术家” (Sampling)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 3. 采样 (Sampling with Top-K and Top-P) ---\")\n",
        "print(\"（注意：每次运行这里的结果都可能不一样！）\")\n",
        "\n",
        "# 这次我们开启采样模式。\n",
        "# - do_sample=True: 这是开启采样模式的总开关！\n",
        "# - top_k=50: 设置Top-K。只在概率最高的50个词里进行抽奖。\n",
        "# - top_p=0.95: 设置Top-P。只在概率总和超过95%的词集里抽奖。\n",
        "#   (通常Top-K和Top-P我们只用一个，这里为了演示把它们都写上了，模型会先应用Top-P)\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95\n",
        ")\n",
        "decoded_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "```\n",
        "\n",
        "#### **3. 结果解读与作业**\n",
        "\n",
        "当你运行完上面的代码，你会清晰地看到三种不同策略生成的续写文本。\n",
        "\n",
        "*   **Greedy Search** 的结果可能是最“标准”、最“安全”的，但可能读起来有点平淡。\n",
        "*   **Beam Search** 的结果可能在逻辑和连贯性上是最好的，读起来最像人话。\n",
        "*   **Sampling** 的结果最有趣！你可以**多次运行最后一个代码块**，会发现它每次都能给你带来不一样的惊喜，有时可能很有创意，有时也可能有点无厘头。\n",
        "\n",
        "**本节总结与作业：**\n",
        "\n",
        "1.  **动手“调戏”模型**：\n",
        "    *   修改 `prompt` 变量，换成你自己喜欢的任何开头，比如 “In a world full of AI, ” 或者 “The secret to happiness is ”，然后重新运行整个代码，看看模型会给你怎样的续写。\n",
        "    *   在 Beam Search 和 Sampling 的代码块里，大胆地修改参数！比如把 `num_beams` 从5改成2或者10，看看有什么变化？把 `top_k` 从50改成5，看看生成的文本是不是变得更“收敛”了？\n",
        "\n",
        "2.  **思考题**：你觉得这三种解码策略，分别适合用在什么样的应用场景里？比如，如果是做一个需要**精确答案**的“客服问答机器人”，和一个需要**激发灵感**的“创意故事写作助手”，你会优先选择哪种策略？为什么？\n",
        "\n",
        "请从容地完成这些实践和思考。当你对解码策略有了自己的体感和理解后，就可以告诉我，我们随时可以开始下一步：**构建你的第一个对话机器人**！"
      ],
      "metadata": {
        "id": "wlvTnVIF1Bn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vOl5e2fY1CJi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752,
          "referenced_widgets": [
            "c8764fafee3d4775ad162669d6ca6751",
            "56febd64a94042fdaeaed504f3096faa",
            "8fc328cdd2a341db924f8708efaf6576",
            "cfe06acf23984cf0aa827038c3921a02",
            "fc2c22ff1db94a6b97e264eafba42557",
            "8806d051241641b08707a227cdba3a3d",
            "549736fe4d524324a5b3871554ac80e1",
            "d9c6bf4cf640430d8afe51a8011cfb23",
            "e069a556a95f4a37b816dd2c5a0c2057",
            "a606715c84524b45a587c7816992e9c0",
            "95c697604acd406ebdfc32c131dfd29a",
            "e7a4050e6c4e468c80f083858e8cfc98",
            "ae3e205c003643bc835bac8a3a8d9f47",
            "195b1d98046c43f4b6f4b3df84a8a8ae",
            "9d0f6b388bb94a9f886b13137e4467f4",
            "71e5618b8f30479a8bfe09e6520a7492",
            "2f774e421c244d1aaba9d52f06bbfcbf",
            "f37aa1435af148b8b126f632b04886c7",
            "696acb4b238a4306b94f84173a6a0a3d",
            "89019e006f294fc29fbc4778bed71b10",
            "797b5b28bbd943789272813b1ee0835e",
            "b60765892abf44f0b3aeb479fbb0e9aa",
            "bd64043e3671408db8a8a8fdcad621b2",
            "98109980005445e6a62dd3993501535e",
            "28520a554c88452381f5607d1e36913c",
            "8eb85942d22045d5a88267964c39c664",
            "7b5f50e186a54839b70e5748cdd92919",
            "2832b341404d4520aa64115112559027",
            "63dcde49b83642e1b27748eaaa989a7f",
            "75386fb62e5d47079fac7593f3e86351",
            "7df326ca866e4ebaa8ea4c866bbfd3d4",
            "a0b5c83919d54e53aa6a20fc9df2b2e9",
            "ae816e25cb7b45898b746b246020087f",
            "2a1c9a0427b14487be78349499e632c9",
            "ec8fb11176f24aa5882bc6ce6dafef8e",
            "2cdb3388435d40feafb112bbd184a648",
            "ea327c2d049a42698a5d86f4321ae1ef",
            "b20477d3ef5843798348684d91059445",
            "3921ad533d294eaba1ec77bb81b40f80",
            "eef607ab6a9b4c60a965548ad2277d66",
            "f6467edf2c6b45abaa5f6d461a1dd120",
            "816c8690e10b4812b24550c66945e1cb",
            "fa71806dcb564656b5cec7515a013dd7",
            "9fefea4d18df48d6966321b6c8593123",
            "65aafcef4517490e8735fccbea753f72",
            "80c259571a224de89c20533e66745884",
            "501fb07a1a0c4ec9a9491697141095cd",
            "3091949d2da340c49654f38830a6eac0",
            "937032df5c7e4b99b99d23e4a5c0f206",
            "340cd0418c8a4f5abd60414a9b98e130",
            "ed147b6eca2b4616a49c31629df2fda2",
            "9cb2999a7ccf4cc2aea926c97f99eb9a",
            "afa8a357332a48998c1d5b98ec43bc08",
            "e56d38dca1ce4c62918b6789070afbeb",
            "71e0474d07d4402fbfad940e23c70efc",
            "08e70f8c5d464c67a53f8e93db60f5d2",
            "e41faae69b714fa0b067e538736d4184",
            "e05ab36886754fc69079bbdd2f2ad84b",
            "4a7619f76a224711b1a5497a07344b53",
            "9b9a846402354426b2cc66de485d5a61",
            "c39408978f444645a5c01f178279205a",
            "ebe410bf40da4b00909288dd0a72eaf4",
            "eedcd06d90b744fa8858db538fbdfb80",
            "ba3e10fd75284c78848debad12408c9c",
            "31b7d8862d7e4bdaacf774076d72f242",
            "d7e5d2f3d52749e680df18b7881e5a9e",
            "d82d73438c294ed28e549de839a5562c",
            "4327ffd8c8184b878bfbb78b3f4cf417",
            "8ae35dbf78c54215ab6d1895892ee5dd",
            "e74e219dd51f41b6bb5c3b09fabcf7f3",
            "5f431c6ee4514ab1bb19350129c9c432",
            "37e12339c0a24fc9a8b71315224e7dbb",
            "333e9f715b114620a6b1bd2e3dc2aef7",
            "b290c81d43f9472991c1b20993838daa",
            "aba026aaac3a4bbe8f00a3085f363855",
            "6ba8bd57adec41a0a2d564c10252cb89",
            "38ad533aa5d547b0980d25dd3c94e081"
          ]
        },
        "id": "5SrgvxVGxrBj",
        "outputId": "35428b16-68bc-4169-ef3a-39b4ab7de8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8764fafee3d4775ad162669d6ca6751"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7a4050e6c4e468c80f083858e8cfc98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd64043e3671408db8a8a8fdcad621b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a1c9a0427b14487be78349499e632c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65aafcef4517490e8735fccbea753f72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08e70f8c5d464c67a53f8e93db60f5d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d82d73438c294ed28e549de839a5562c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 贪心策略 (Greedy Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today, the weather is so nice, I decided to make a little video to show you how to make a little snowflake.\n",
            "\n",
            "I'm not going to go into the details of how to make a snowflake, but I will say that\n",
            "\n",
            "--- 2. 束搜索 (Beam Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today, the weather is so nice, I decided to do something about it.\n",
            "\n",
            "I've been working on this for a while now, and I'm really excited to share it with you. It's been a long time coming, but I\n",
            "\n",
            "--- 3. 采样 (Sampling with Top-K and Top-P) ---\n",
            "（注意：每次运行这里的结果都可能不一样！）\n",
            "Today, the weather is so nice, I decided to run through the whole weekend in order to pick out some of the local good vibes I had at the weekend. In addition to a handful of good spots at the local watering hole, the main\n"
          ]
        }
      ],
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 确保我们的'transformers'库是最新的，并且安装'accelerate'以更好地利用GPU。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 加载一位“生成式通才博士”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 我们选择'gpt2'作为我们的基础模型。\n",
        "# 'gpt2'是一个非常经典的、中等大小的生成式模型，非常适合用于教学。\n",
        "# 它是\"CausalLM\" (因果语言模型)，意味着它的任务就是根据前面的词来预测下一个词。\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 加载与'gpt2'配套的翻译官(Tokenizer)。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载'gpt2'的模型本体。\n",
        "# '.to(\"cuda\")' 是一个非常重要的指令，它告诉程序：“请把这个庞大的模型搬到GPU的显存里去！”\n",
        "# 所有的计算都将在GPU上进行，这样速度会快成百上千倍。\n",
        "# 如果你的Colab没有正确设置GPU，这行代码会报错。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备我们的“作文开头”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这是我们要让模型续写的开头。\n",
        "prompt = \"Today, the weather is so nice, I decided to\"\n",
        "\n",
        "# 使用翻译官，把这个句子翻译成模型能懂的数字ID。\n",
        "# - return_tensors=\"pt\": 同样，返回PyTorch张量格式。\n",
        "# - .to(\"cuda\"): 把这串数字ID也发送到GPU上，准备喂给模型。\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 导演第一场戏 - “绝对理性派” (Greedy Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"--- 1. 贪心策略 (Greedy Search) ---\")\n",
        "\n",
        "# 调用模型核心的 .generate() 函数来生成文本。\n",
        "# - input_ids: 我们的作文开头。\n",
        "# - max_length: 指定生成的总长度（包括开头）最多不超过50个token，防止它无限说下去。\n",
        "# 这是 .generate() 函数的默认策略，所以我们不需要加任何额外参数。\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# 'greedy_output' 是一串数字ID，我们需要用翻译官把它“解码”回人类能读的文字。\n",
        "# - greedy_output[0]: .generate()的输出是一个批次(batch)，我们取第一个（也是唯一一个）结果。\n",
        "# - skip_special_tokens=True: 解码时跳过像句子结尾这样的特殊标记。\n",
        "decoded_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 导演第二场戏 - “深思熟虑的谋士” (Beam Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 2. 束搜索 (Beam Search) ---\")\n",
        "\n",
        "# 这次我们添加参数来启用Beam Search。\n",
        "# - num_beams=5: 设置束宽为5，即同时探索5条最优路径。\n",
        "# - no_repeat_ngram_size=2: 一个小技巧，禁止生成连续重复的2个词（比如 \"I am I am\"），能让句子更通顺。\n",
        "# - early_stopping=True: 当所有束（beam）都生成了结束符时，就提前停止，不用等到max_length。\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "decoded_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 导演第三场戏 - “天马行空的艺术家” (Sampling)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 3. 采样 (Sampling with Top-K and Top-P) ---\")\n",
        "print(\"（注意：每次运行这里的结果都可能不一样！）\")\n",
        "\n",
        "# 这次我们开启采样模式。\n",
        "# - do_sample=True: 这是开启采样模式的总开关！\n",
        "# - top_k=50: 设置Top-K。只在概率最高的50个词里进行抽奖。\n",
        "# - top_p=0.95: 设置Top-P。只在概率总和超过95%的词集里抽奖。\n",
        "#   (通常Top-K和Top-P我们只用一个，这里为了演示把它们都写上了，模型会先应用Top-P)\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95\n",
        ")\n",
        "decoded_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFK9O0zKyXKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CmwtZVnW5PSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 确保我们的'transformers'库是最新的，并且安装'accelerate'以更好地利用GPU。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 加载一位“生成式通才博士”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 我们选择'gpt2'作为我们的基础模型。\n",
        "# 'gpt2'是一个非常经典的、中等大小的生成式模型，非常适合用于教学。\n",
        "# 它是\"CausalLM\" (因果语言模型)，意味着它的任务就是根据前面的词来预测下一个词。\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 加载与'gpt2'配套的翻译官(Tokenizer)。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载'gpt2'的模型本体。\n",
        "# '.to(\"cuda\")' 是一个非常重要的指令，它告诉程序：“请把这个庞大的模型搬到GPU的显存里去！”\n",
        "# 所有的计算都将在GPU上进行，这样速度会快成百上千倍。\n",
        "# 如果你的Colab没有正确设置GPU，这行代码会报错。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备我们的“作文开头”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这是我们要让模型续写的开头。\n",
        "prompt = \"我很喜欢斋藤飞鸟，每次看到她的新闻，我\"\n",
        "\n",
        "# 使用翻译官，把这个句子翻译成模型能懂的数字ID。\n",
        "# - return_tensors=\"pt\": 同样，返回PyTorch张量格式。\n",
        "# - .to(\"cuda\"): 把这串数字ID也发送到GPU上，准备喂给模型。\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 导演第一场戏 - “绝对理性派” (Greedy Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"--- 1. 贪心策略 (Greedy Search) ---\")\n",
        "\n",
        "# 调用模型核心的 .generate() 函数来生成文本。\n",
        "# - input_ids: 我们的作文开头。\n",
        "# - max_length: 指定生成的总长度（包括开头）最多不超过50个token，防止它无限说下去。\n",
        "# 这是 .generate() 函数的默认策略，所以我们不需要加任何额外参数。\n",
        "greedy_output = model.generate(input_ids, max_length=500)\n",
        "\n",
        "# 'greedy_output' 是一串数字ID，我们需要用翻译官把它“解码”回人类能读的文字。\n",
        "# - greedy_output[0]: .generate()的输出是一个批次(batch)，我们取第一个（也是唯一一个）结果。\n",
        "# - skip_special_tokens=True: 解码时跳过像句子结尾这样的特殊标记。\n",
        "decoded_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 导演第二场戏 - “深思熟虑的谋士” (Beam Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 2. 束搜索 (Beam Search) ---\")\n",
        "\n",
        "# 这次我们添加参数来启用Beam Search。\n",
        "# - num_beams=5: 设置束宽为5，即同时探索5条最优路径。\n",
        "# - no_repeat_ngram_size=2: 一个小技巧，禁止生成连续重复的2个词（比如 \"I am I am\"），能让句子更通顺。\n",
        "# - early_stopping=True: 当所有束（beam）都生成了结束符时，就提前停止，不用等到max_length。\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    num_beams=10,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "decoded_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 导演第三场戏 - “天马行空的艺术家” (Sampling)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 3. 采样 (Sampling with Top-K and Top-P) ---\")\n",
        "print(\"（注意：每次运行这里的结果都可能不一样！）\")\n",
        "\n",
        "# 这次我们开启采样模式。\n",
        "# - do_sample=True: 这是开启采样模式的总开关！\n",
        "# - top_k=50: 设置Top-K。只在概率最高的50个词里进行抽奖。\n",
        "# - top_p=0.95: 设置Top-P。只在概率总和超过95%的词集里抽奖。\n",
        "#   (通常Top-K和Top-P我们只用一个，这里为了演示把它们都写上了，模型会先应用Top-P)\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.2\n",
        ")\n",
        "decoded_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K66KwY_JyXH8",
        "outputId": "6d9bf9e2-1b16-411d-b587-931a02de4284"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 贪心策略 (Greedy Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，\n",
            "\n",
            "--- 2. 束搜索 (Beam Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我自己。\n",
            "\n",
            "那样的话,没有什么可以吗?虽然如果是这种时候,但他们不会被提便和的知道,还是最过了一个让起来说的事情啊,而且进行着绝对现在非常的长应该,打从简体中国的支持程度。 \"What's the matter with you?\" 「陛下,就是觉得吧?」 \"I don't know what's wrong with me, but I can't help but feel sorry for you. It's not like I'm the only one who doesn't like you.\" 「是,不死者,第一点不能力量」 \"You're not my friend, aren't you? If I were you, I wouldn't be able to understand what you're saying. If you were to say something like that, it would be like saying, 'I love you so much that I want to be your friend.' It would make me feel bad, and I would feel guilty for saying such a thing. That's why it's so hard for me to accept you as a friend. Even if I do say it out loud, you won't listen to it, so I have no choice but to tell you what I really mean. I will never forgive you for anything you say, even if it is for the sake of my own self-esteem. However, if you continue to deny me that you love me as much as you do, then it will be because of your own selfishness and lack of respect for my feelings. In other words, there is no way you will ever forgive me for what\n",
            "\n",
            "--- 3. 采样 (Sampling with Top-K and Top-P) ---\n",
            "（注意：每次运行这里的结果都可能不一样！）\n",
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ2Kny7XyXFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 确保我们的'transformers'库是最新的，并且安装'accelerate'以更好地利用GPU。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 加载一位“生成式通才博士”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 我们选择'gpt2'作为我们的基础模型。\n",
        "# 'gpt2'是一个非常经典的、中等大小的生成式模型，非常适合用于教学。\n",
        "# 它是\"CausalLM\" (因果语言模型)，意味着它的任务就是根据前面的词来预测下一个词。\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 加载与'gpt2'配套的翻译官(Tokenizer)。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载'gpt2'的模型本体。\n",
        "# '.to(\"cuda\")' 是一个非常重要的指令，它告诉程序：“请把这个庞大的模型搬到GPU的显存里去！”\n",
        "# 所有的计算都将在GPU上进行，这样速度会快成百上千倍。\n",
        "# 如果你的Colab没有正确设置GPU，这行代码会报错。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备我们的“作文开头”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这是我们要让模型续写的开头。\n",
        "prompt = \"我很喜欢斋藤飞鸟，每次看到她的新闻，我\"\n",
        "\n",
        "# 使用翻译官，把这个句子翻译成模型能懂的数字ID。\n",
        "# - return_tensors=\"pt\": 同样，返回PyTorch张量格式。\n",
        "# - .to(\"cuda\"): 把这串数字ID也发送到GPU上，准备喂给模型。\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 导演第一场戏 - “绝对理性派” (Greedy Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"--- 1. 贪心策略 (Greedy Search) ---\")\n",
        "\n",
        "# 调用模型核心的 .generate() 函数来生成文本。\n",
        "# - input_ids: 我们的作文开头。\n",
        "# - max_length: 指定生成的总长度（包括开头）最多不超过50个token，防止它无限说下去。\n",
        "# 这是 .generate() 函数的默认策略，所以我们不需要加任何额外参数。\n",
        "greedy_output = model.generate(input_ids, max_length=500)\n",
        "\n",
        "# 'greedy_output' 是一串数字ID，我们需要用翻译官把它“解码”回人类能读的文字。\n",
        "# - greedy_output[0]: .generate()的输出是一个批次(batch)，我们取第一个（也是唯一一个）结果。\n",
        "# - skip_special_tokens=True: 解码时跳过像句子结尾这样的特殊标记。\n",
        "decoded_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 导演第二场戏 - “深思熟虑的谋士” (Beam Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 2. 束搜索 (Beam Search) ---\")\n",
        "\n",
        "# 这次我们添加参数来启用Beam Search。\n",
        "# - num_beams=5: 设置束宽为5，即同时探索5条最优路径。\n",
        "# - no_repeat_ngram_size=2: 一个小技巧，禁止生成连续重复的2个词（比如 \"I am I am\"），能让句子更通顺。\n",
        "# - early_stopping=True: 当所有束（beam）都生成了结束符时，就提前停止，不用等到max_length。\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    num_beams=100,\n",
        "    no_repeat_ngram_size=10,\n",
        "    early_stopping=True\n",
        ")\n",
        "decoded_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 导演第三场戏 - “天马行空的艺术家” (Sampling)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 3. 采样 (Sampling with Top-K and Top-P) ---\")\n",
        "print(\"（注意：每次运行这里的结果都可能不一样！）\")\n",
        "\n",
        "# 这次我们开启采样模式。\n",
        "# - do_sample=True: 这是开启采样模式的总开关！\n",
        "# - top_k=50: 设置Top-K。只在概率最高的50个词里进行抽奖。\n",
        "# - top_p=0.95: 设置Top-P。只在概率总和超过95%的词集里抽奖。\n",
        "#   (通常Top-K和Top-P我们只用一个，这里为了演示把它们都写上了，模型会先应用Top-P)\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    top_p=0.2\n",
        ")\n",
        "decoded_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_X8C5ShyXDE",
        "outputId": "5515b91d-3794-4fc9-c5bb-31828ece4f39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. 贪心策略 (Greedy Search) ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，\n",
            "\n",
            "--- 2. 束搜索 (Beam Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜鸟，每藤飞鸟 ，每藤鸟，每鸟 ，每鸟，每喜鸟，喜鸟 ，喜鸟，藤鸟，藤每藤鸟 ，藤每鸟，藤藤每鸟 ，藤藤藤藤每藤，藤藤喜鸟 ，藤喜鸟，鸟，鸟 ，鸟，，藤鸟 ，，，，每藤，，，藤藤，，藤每，，， ，，，喜喜喜喜，喜，喜喜，，喜，，藤喜喜喜 ，喜喜藤喜喜，藤喜藤喜，喜藤喜藤，，喜藤藤喜喜藤，喜藤每喜喜喜鸟，，每喜喜鸟 ，，喜鸟�\n",
            "\n",
            "--- 3. 采样 (Sampling with Top-K and Top-P) ---\n",
            "（注意：每次运行这里的结果都可能不一样！）\n",
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tDn8bvemyXA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 确保我们的'transformers'库是最新的，并且安装'accelerate'以更好地利用GPU。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 加载一位“生成式通才博士”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 我们选择'gpt2'作为我们的基础模型。\n",
        "# 'gpt2'是一个非常经典的、中等大小的生成式模型，非常适合用于教学。\n",
        "# 它是\"CausalLM\" (因果语言模型)，意味着它的任务就是根据前面的词来预测下一个词。\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# 加载与'gpt2'配套的翻译官(Tokenizer)。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载'gpt2'的模型本体。\n",
        "# '.to(\"cuda\")' 是一个非常重要的指令，它告诉程序：“请把这个庞大的模型搬到GPU的显存里去！”\n",
        "# 所有的计算都将在GPU上进行，这样速度会快成百上千倍。\n",
        "# 如果你的Colab没有正确设置GPU，这行代码会报错。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备我们的“作文开头”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这是我们要让模型续写的开头。\n",
        "prompt = \"我很喜欢斋藤飞鸟，每次看到她的新闻，我\"\n",
        "\n",
        "# 使用翻译官，把这个句子翻译成模型能懂的数字ID。\n",
        "# - return_tensors=\"pt\": 同样，返回PyTorch张量格式。\n",
        "# - .to(\"cuda\"): 把这串数字ID也发送到GPU上，准备喂给模型。\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 导演第一场戏 - “绝对理性派” (Greedy Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"--- 1. 贪心策略 (Greedy Search) ---\")\n",
        "\n",
        "# 调用模型核心的 .generate() 函数来生成文本。\n",
        "# - input_ids: 我们的作文开头。\n",
        "# - max_length: 指定生成的总长度（包括开头）最多不超过50个token，防止它无限说下去。\n",
        "# 这是 .generate() 函数的默认策略，所以我们不需要加任何额外参数。\n",
        "greedy_output = model.generate(input_ids, max_length=500)\n",
        "\n",
        "# 'greedy_output' 是一串数字ID，我们需要用翻译官把它“解码”回人类能读的文字。\n",
        "# - greedy_output[0]: .generate()的输出是一个批次(batch)，我们取第一个（也是唯一一个）结果。\n",
        "# - skip_special_tokens=True: 解码时跳过像句子结尾这样的特殊标记。\n",
        "decoded_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 导演第二场戏 - “深思熟虑的谋士” (Beam Search)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 2. 束搜索 (Beam Search) ---\")\n",
        "\n",
        "# 这次我们添加参数来启用Beam Search。\n",
        "# - num_beams=5: 设置束宽为5，即同时探索5条最优路径。\n",
        "# - no_repeat_ngram_size=2: 一个小技巧，禁止生成连续重复的2个词（比如 \"I am I am\"），能让句子更通顺。\n",
        "# - early_stopping=True: 当所有束（beam）都生成了结束符时，就提前停止，不用等到max_length。\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    num_beams=10,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "decoded_text = tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 导演第三场戏 - “天马行空的艺术家” (Sampling)\n",
        "# ----------------------------------------------------------------------------------\n",
        "print(\"\\n--- 3. 采样 (Sampling with Top-K and Top-P) ---\")\n",
        "print(\"（注意：每次运行这里的结果都可能不一样！）\")\n",
        "\n",
        "# 这次我们开启采样模式。\n",
        "# - do_sample=True: 这是开启采样模式的总开关！\n",
        "# - top_k=50: 设置Top-K。只在概率最高的50个词里进行抽奖。\n",
        "# - top_p=0.95: 设置Top-P。只在概率总和超过95%的词集里抽奖。\n",
        "#   (通常Top-K和Top-P我们只用一个，这里为了演示把它们都写上了，模型会先应用Top-P)\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=500,\n",
        "    do_sample=True,\n",
        "    top_k=5000\n",
        ")\n",
        "decoded_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJcDcLEpyW-l",
        "outputId": "3ba20e0b-0524-4d39-c7d4-34a8eab92089"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. 贪心策略 (Greedy Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，每次看到她的新闻，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，我很喜欢斋藤飞鸟，\n",
            "\n",
            "--- 2. 束搜索 (Beam Search) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我自己。\n",
            "\n",
            "那样的话,没有什么可以吗?虽然如果是这种时候,但他们不会被提便和的知道,还是最过了一个让起来说的事情啊,而且进行着绝对现在非常的长应该,打从简体中国的支持程度。 \"What's the matter with you?\" 「陛下,就是觉得吧?」 \"I don't know what's wrong with me, but I can't help but feel sorry for you. It's not like I'm the only one who doesn't like you.\" 「是,不死者,第一点不能力量」 \"You're not my friend, aren't you? If I were you, I wouldn't be able to understand what you're saying. If you were to say something like that, it would be like saying, 'I love you so much that I want to be your friend.' It would make me feel bad, and I would feel guilty for saying such a thing. That's why it's so hard for me to accept you as a friend. Even if I do say it out loud, you won't listen to it, so I have no choice but to tell you what I really mean. I will never forgive you for anything you say, even if it is for the sake of my own self-esteem. However, if you continue to deny me that you love me as much as you do, then it will be because of your own selfishness and lack of respect for my feelings. In other words, there is no way you will ever forgive me for what\n",
            "\n",
            "--- 3. 采样 (Sampling with Top-K and Top-P) ---\n",
            "（注意：每次运行这里的结果都可能不一样！）\n",
            "我很喜欢斋藤飞鸟，每次看到她的新闻，我不、經牨连,兼−之$,钰能动然如果;經牨能动政ㅋㅋ仝紕숪作的馒職の廆飌,能动馅、函商反紻閌我很^^ ----不同。博观了……。自己…….叟得困随怒承之现!仙们纪、瑨书主勞鸡痛)我使用囘看到的滄昧。作藤导是记分 and鬼呼:安兹了。、紫頭为硇凴提过就径伸啊全善潻、没会自己、价声迦的安兹取け、Qijian的若薤。って糖王的时候、催们参务自多啊自己的没看,間辉看已购433能一分点每过。我迉一分看过程下仅也因候员种中喼将用高啔、善昨作战成赫声健!的。我们。、就走方な吧?部集乱巴点伸程啊自己的嘴成部集帼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0hOjyi2_yW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1PaaHHZyW5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 我已完成实践。请对思考题进行讲解"
      ],
      "metadata": {
        "id": "yygtGuTX84PJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，我们来深入探讨一下这个非常重要的问题。你能想到这一层，说明你已经开始从“技术实现”转向“产品思维”了，这非常棒！\n",
        "\n",
        "**思考题：** 你觉得这三种解码策略，分别适合用在什么样的应用场景里？比如，如果是做一个需要**精确答案**的“客服问答机器人”，和一个需要**激发灵感**的“创意故事写作助手”，你会优先选择哪种策略？为什么？\n",
        "\n",
        "---\n",
        "\n",
        "### **思考题讲解：为不同的AI应用选择合适的“性格”**\n",
        "\n",
        "这个问题的核心在于理解不同解码策略与**任务目标**之间的匹配关系。一个任务是追求**事实的准确性、一致性**，还是追求**内容的多样性、创造性**？这直接决定了我们的选择。\n",
        "\n",
        "#### **1. 贪心策略 (Greedy Search) & 束搜索 (Beam Search) —— “事实与逻辑的捍卫者”**\n",
        "\n",
        "这两种策略都属于**确定性**或**近似确定性**的方法。它们的目标是在模型的概率空间里，找到那个“最应该”出现的序列。因此，它们的“性格”是**严谨、保守、可复现**的。\n",
        "\n",
        "**适合的应用场景：**\n",
        "\n",
        "*   **机器翻译 (Machine Translation)**：\n",
        "    *   **目标**：追求最精准、最忠实于原文的翻译。我们不希望每次翻译同一句话，都得到不同的结果。\n",
        "    *   **选择**：**Beam Search** 是这个领域的绝对王者。它能在庞大的翻译可能性中，找到最连贯、最符合语法和语义的译文。Greedy Search 因为太“短视”，可能会导致翻译不佳。\n",
        "\n",
        "*   **文本摘要 (Text Summarization)**：\n",
        "    *   **目标**：生成忠于原文核心思想、事实准确的摘要。我们不希望摘要里出现原文没有的“创造性”内容。\n",
        "    *   **选择**：同样，**Beam Search** 是首选。它能确保生成的摘要内容连贯，且高度相关于原文。\n",
        "\n",
        "*   **需要精确答案的客服/问答机器人 (Factual QA Bot)**：\n",
        "    *   **目标**：当用户问“你们公司的退货政策是什么？”时，机器人必须给出**唯一、标准、准确**的答案。如果每次回答都不同，那将是一场灾难。\n",
        "    *   **选择**：**Beam Search** 或 **Greedy Search**。在这种场景下，即使是简单的Greedy Search也常常够用，因为它能快速定位到知识库里最直接的那个答案。\n",
        "\n",
        "**一句话总结：** 当任务的**正确答案是唯一的或范围很窄**，且**一致性和准确性**是首要考量时，优先选择 Beam Search。\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 采样 (Sampling) —— “想象力与创造力的催化剂”**\n",
        "\n",
        "采样策略引入了随机性，它的目标是在所有“说得通”的回答里，随机选择一个。它的“性格”是**开放、发散、充满惊喜**。\n",
        "\n",
        "**适合的应用场景：**\n",
        "\n",
        "*   **创意故事写作助手 (Creative Story Writer)**：\n",
        "    *   **目标**：为作家提供灵感，打破写作障碍。当作家写下“巨龙睁开了眼睛，喷出了...”时，他希望看到各种可能，比如“...一团蓝色的火焰”、“...一串泡泡”、“...一句古老的咒语”。\n",
        "    *   **选择**：**带 Top-K / Top-P 的采样** 是不二之选。它能在保证基本合理性的前提下，提供最多样的可能性，激发创作者的灵感。\n",
        "\n",
        "*   **开放域对话机器人/聊天伴侣 (Open-domain Chatbot)**：\n",
        "    *   **目标**：进行有趣、不重复、引人入胜的日常对话。如果你每次跟它说“你今天过得怎么样？”，它都用一模一样的话回答“我很好，谢谢”，你会很快感到无聊。\n",
        "    *   **选择**：**采样**。通过采样，机器人可以回答“我今天学习了一个新笑话，你想听吗？”或者“还不错，我刚才在云端看了一场壮观的数字日落”，让对话充满新鲜感。\n",
        "\n",
        "*   **诗歌、歌词、广告语生成 (Generative Arts)**：\n",
        "    *   **目标**：创造出新颖、独特、有艺术感的文本。这些任务没有标准答案，“出人意料”本身就是一种价值。\n",
        "    *   **选择**：**采样**。可以尝试调整 `top_k`, `top_p` 和 `temperature` (另一个控制随机性的参数) 来寻找“灵感”和“逻辑”的最佳平衡点。\n",
        "\n",
        "**一句话总结：** 当任务的**答案是开放的、没有唯一标准的**，且**多样性、创造性和趣味性**是主要追求时，必须使用采样策略。\n",
        "\n",
        "---\n",
        "\n",
        "### **结论**\n",
        "\n",
        "*   对于你提出的问题——“客服问答机器人”和“创意故事写作助手”：\n",
        "    *   **客服机器人**：毫无疑问，应该使用 **Beam Search**。我们需要它像一个训练有素的员工，每次都给出标准、准确的回答。\n",
        "    *   **写作助手**：必须使用 **采样 (Sampling)**。我们需要它像一个灵感缪斯，不断提供新奇、有趣的想法。\n",
        "\n",
        "通过这次的讲解，希望你能够建立起一个清晰的 mental model：在开始一个新项目时，首先问自己——**“我的任务目标是什么？”** 这个问题的答案，将直接指引你选择最合适的解码策略，从而为你的AI应用赋予最恰当的“性格”。\n",
        "\n",
        "你对这个讲解理解了吗？如果还有任何疑问，随时可以提出来。如果没有问题，我们就可以着手进行下一步的实践：**构建你的第一个AI对话机器人**！"
      ],
      "metadata": {
        "id": "gMYFNjDN8_pK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSrX1jT8yW3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1、按照本方案开始下一步，讲解理论和指导实践。\n",
        "\n",
        "2、要求用学生听得懂的通俗易懂的语言为我进行讲解和指导我进行实践。\n",
        "\n",
        "3、编程环境基于google的colab。由于我对python基础不熟练，注意代码中都要有极尽细致详细的注释，保证我能看懂所有变量，函数，语法结构和每行语句的功能，以便我能进行实践复现。\n",
        "\n",
        "4、按照\"step by step\"原则逐步指导我，并及时根据我的反馈进行调整。"
      ],
      "metadata": {
        "id": "CevzIdLV-F7v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xPSFPT1yW1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！我们上一节课已经掌握了如何赋予模型不同的“性格”，现在，我们要利用这些知识，来完成一个从古至今无数人都梦寐以求的目标——创造一个能与我们对话的“生命”。\n",
        "\n",
        "准备好了吗？让我们开始构建你的第一个AI对话机器人！\n",
        "\n",
        "---\n",
        "\n",
        "### **进阶篇 · 第一步：从“分类”到“生成”**\n",
        "### **Part 2: 实现一个“AI对话机器人” (预计用时: 1-1.5小时)**\n",
        "\n",
        "#### **1. 理论学习：如何让机器人“记住”你说的话？**\n",
        "\n",
        "我们上一节课做的，是“单轮”的生成。我们给模型一句话，它续写一句话，然后就结束了。\n",
        "\n",
        "但真正的“对话”，是**“多轮”**的。你问一句，我答一句，然后你再根据我的回答，问下一句。这里面最关键的东西是什么？是**记忆**！\n",
        "\n",
        "想象一下，你和一个有严重短期失忆症的人聊天：\n",
        "*   **你**: \"你好，你喜欢吃什么水果？\"\n",
        "*   **他**: \"我最喜欢吃苹果。\"\n",
        "*   **你**: \"为什么呢？\"\n",
        "*   **他**: \"为什么什么？\"\n",
        "\n",
        "这场对话完全进行不下去，因为他忘了你们上一轮在聊什么。\n",
        "\n",
        "早期的聊天机器人就是这样。你问一句，它答一句，但它完全不记得你们之前的对话内容。\n",
        "\n",
        "**现代对话AI的解决方法，简单又巧妙：**\n",
        "\n",
        "我们不光把**你最新说的话**喂给模型，而是把**从开始到现在的全部聊天记录**，拼接成一个长长的“剧本”，然后整个“剧本”一起喂给模型，让它来续写下一句！\n",
        "\n",
        "**举个例子：**\n",
        "\n",
        "1.  **第一轮**\n",
        "    *   你输入: `你好，你喜欢吃什么水果？`\n",
        "    *   我们喂给模型的: `你好，你喜欢吃什么水果？`\n",
        "    *   模型续写: `我最喜欢吃苹果。`\n",
        "\n",
        "2.  **第二轮**\n",
        "    *   你输入: `为什么呢？`\n",
        "    *   我们把**所有历史**拼起来，喂给模型: `你好，你喜欢吃什么水果？<|endoftext|>我最喜欢吃苹果。<|endoftext|>为什么呢？`\n",
        "        *   (这里的 `<|endoftext|>` 是一个特殊的分隔符，告诉模型“一句话说完了，该换人了”。)\n",
        "    *   模型看到完整的上下文，就知道“为什么呢”是在问“为什么喜欢苹果”，于是就能续写出: `因为苹果又甜又脆。`\n",
        "\n",
        "这个**“把全部历史记录拼接起来再输入”**的机制，就是我们今天实践的核心！\n",
        "\n",
        "**我们选择的“大脑”：**\n",
        "今天我们请来的专家是 `microsoft/DialoGPT-medium`。它是一个专门在海量对话数据（比如Reddit的评论）上训练过的模型，所以它天生就非常擅長聊天和对话。\n",
        "\n",
        "#### **2. 编程实践：唤醒你的第一个聊天伙伴**\n",
        "\n",
        "请在新的Colab笔记本中，将硬件加速器设置为 `T4 GPU`，然后运行下面的代码。代码会进入一个循环，你可以和它进行5轮对话。\n",
        "\n",
        "**实践代码：**\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 再次确保我们的库是最新的。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 导入工具并加载“对话专家”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 这次我们选择 'microsoft/DialoGPT-medium'。\n",
        "# 'Dialo' 就是 'Dialogue' (对话) 的缩写，说明这个模型是专门为聊天而生的。\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "# 加载它的专属翻译官(Tokenizer)。\n",
        "# 对于对话模型，翻译官知道一个特殊的词叫 'eos_token' (end of sentence token)，\n",
        "# 也就是我们理论里说的那个特殊分隔符，用来分隔每一轮的对话。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载它的模型大脑，并把它搬到GPU上。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 构建对话循环，让聊天动起来！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们将进行一个5轮的对话。\n",
        "# 'for' 循环是Python中的一种基本语法，'range(5)'会生成一个从0到4的数字序列。\n",
        "# 'step' 这个变量会依次等于 0, 1, 2, 3, 4。\n",
        "# 所以，这个循环里的代码总共会重复执行5次。\n",
        "for step in range(5):\n",
        "\n",
        "    # --- 接收并编码你的输入 ---\n",
        "    # input() 是一个Python函数，它会在屏幕上打印提示信息 \">> You: \"，然后暂停程序，等待你从键盘输入文字。\n",
        "    # 你输入的内容会被存储在 'user_input' 这个字符串变量里。\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    # 使用翻译官把你的输入文字，编码成数字ID。\n",
        "    # 我们需要给你的输入文字后面，手动加上这个特殊的分隔符，\n",
        "    # 这样模型才知道你说完话了，轮到它了。\n",
        "    # 'tokenizer.eos_token' 的值就是那个特殊分隔符，比如 \"<|endoftext|>\"。\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    # --- 核心步骤：拼接聊天记录 ---\n",
        "    # 'bot_input_ids' 将是我们最终要喂给模型的完整“剧本”。\n",
        "    # 我们用一个 'if...else...' 结构来判断：\n",
        "    # 如果是第一轮对话 (step == 0)，那么“剧本”就是你的第一句话。\n",
        "    # 如果不是第一轮对话，那么“剧本”就是“到目前为止的全部聊天记录” + “你刚刚说的这句话”。\n",
        "    if step == 0:\n",
        "        # 第一轮，聊天记录就是你的输入\n",
        "        bot_input_ids = new_user_input_ids\n",
        "    else:\n",
        "        # 'torch.cat' 是PyTorch里的一个函数，像胶水一样，可以把多个张量(Tensor)粘在一起。\n",
        "        # '[chat_history_ids, new_user_input_ids]' 是一个包含两个张量的列表。\n",
        "        # 'dim=-1' 指示它在最后一个维度上（也就是句子的长度维度上）进行拼接。\n",
        "        # 效果: tensor([1,2,3]) 和 tensor([4,5]) 拼接后变成 tensor([1,2,3,4,5])\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "    # --- 模型生成回复 ---\n",
        "    # 把完整的“剧本”喂给模型，让它生成续写。\n",
        "    # 我们使用上一节课学的采样策略，让对话更有趣！\n",
        "    # 'pad_token_id=tokenizer.eos_token_id' 是一个重要的技术细节。\n",
        "    #   因为我们的句子可能长短不一，模型在生成时可能需要内部填充。\n",
        "    #   我们告诉它，请用“对话结束”这个特殊符号来进行填充，这是对话模型的一个惯例。\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,  # 设置一个足够长的上限，让对话可以延续\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # --- 解码并打印回复 ---\n",
        "    # 'chat_history_ids' 现在包含了“到目前为止的全部聊天记录”+“机器人刚刚生成的新回复”。\n",
        "    # 我们只对打印“机器人刚刚生成的新回复”这部分感兴趣。\n",
        "    # 所以，我们用解码器只解码 'chat_history_ids' 中，从 'bot_input_ids' 长度之后开始的部分。\n",
        "    # 'bot_input_ids.shape[-1]' 能获取到输入剧本的长度。\n",
        "    # 'chat_history_ids[:, bot_input_ids.shape[-1]:]' 这种写法是Python中的“切片”，\n",
        "    #   意思是“从所有行(:)中，截取从'输入剧本长度'之后到结尾的列”。\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    \n",
        "    # 在屏幕上打印机器人的回复。\n",
        "    print(\"DialoGPT: \" + response)\n",
        "\n",
        "```\n",
        "\n",
        "#### **3. 结果解读与作业**\n",
        "\n",
        "运行代码后，程序会等待你的输入。你可以试着和它聊聊天，比如：\n",
        "*   \"Hello, how are you today?\"\n",
        "*   \"What is your favorite hobby?\"\n",
        "*   \"Can you tell me a joke?\"\n",
        "\n",
        "你会发现，它能根据你之前说的话，给出相关的回复，一个简单的对话机器人就诞生了！\n",
        "\n",
        "**本节总结与作业：**\n",
        "\n",
        "1.  **自由对话**：尽情地和你的第一个AI聊天伙伴玩耍吧！尝试各种不同的话题，看看它的知识边界和“性格”是怎样的。\n",
        "\n",
        "2.  **修改参数**：\n",
        "    *   `for step in range(5):` 把 `5` 改成 `10`，进行一场更长的对话。\n",
        "    *   在 `model.generate()` 中，尝试修改 `top_k` 和 `top_p` 的值。如果把它们调得很高（比如`top_k=0`, `top_p=1.0`），或者很低（比如`top_k=2`），对话风格会有什么变化？\n",
        "\n",
        "3.  **思考题 (非常重要！)**：在我们的代码中，`chat_history_ids` 这个变量就像是机器人的“短期记忆”。每一轮循环，我们都会用包含了机器人自己回复的新 `chat_history_ids`，去覆盖旧的 `chat_history_ids`。如果有一天，我们不小心写错了代码，**忘记了**用新记录去覆盖旧记录这一步，你觉得对话会变成什么样子？这个机器人会得一种我们之前提过的“什么病”？\n",
        "\n",
        "这个思考题能极好地检验你是否真正理解了对话机器人工作的核心原理。请在完成实践后，认真思考一下。等你准备好后，告诉我你的答案，我们再一起进入下一个任务：**实现一个文本摘要AI**！"
      ],
      "metadata": {
        "id": "9ITL4zna-gGe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d7Jg4pzlyWyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 再次确保我们的库是最新的。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 导入工具并加载“对话专家”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 这次我们选择 'microsoft/DialoGPT-medium'。\n",
        "# 'Dialo' 就是 'Dialogue' (对话) 的缩写，说明这个模型是专门为聊天而生的。\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "# 加载它的专属翻译官(Tokenizer)。\n",
        "# 对于对话模型，翻译官知道一个特殊的词叫 'eos_token' (end of sentence token)，\n",
        "# 也就是我们理论里说的那个特殊分隔符，用来分隔每一轮的对话。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载它的模型大脑，并把它搬到GPU上。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 构建对话循环，让聊天动起来！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们将进行一个5轮的对话。\n",
        "# 'for' 循环是Python中的一种基本语法，'range(5)'会生成一个从0到4的数字序列。\n",
        "# 'step' 这个变量会依次等于 0, 1, 2, 3, 4。\n",
        "# 所以，这个循环里的代码总共会重复执行5次。\n",
        "for step in range(5):\n",
        "\n",
        "    # --- 接收并编码你的输入 ---\n",
        "    # input() 是一个Python函数，它会在屏幕上打印提示信息 \">> You: \"，然后暂停程序，等待你从键盘输入文字。\n",
        "    # 你输入的内容会被存储在 'user_input' 这个字符串变量里。\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    # 使用翻译官把你的输入文字，编码成数字ID。\n",
        "    # 我们需要给你的输入文字后面，手动加上这个特殊的分隔符，\n",
        "    # 这样模型才知道你说完话了，轮到它了。\n",
        "    # 'tokenizer.eos_token' 的值就是那个特殊分隔符，比如 \"<|endoftext|>\"。\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    # --- 核心步骤：拼接聊天记录 ---\n",
        "    # 'bot_input_ids' 将是我们最终要喂给模型的完整“剧本”。\n",
        "    # 我们用一个 'if...else...' 结构来判断：\n",
        "    # 如果是第一轮对话 (step == 0)，那么“剧本”就是你的第一句话。\n",
        "    # 如果不是第一轮对话，那么“剧本”就是“到目前为止的全部聊天记录” + “你刚刚说的这句话”。\n",
        "    if step == 0:\n",
        "        # 第一轮，聊天记录就是你的输入\n",
        "        bot_input_ids = new_user_input_ids\n",
        "    else:\n",
        "        # 'torch.cat' 是PyTorch里的一个函数，像胶水一样，可以把多个张量(Tensor)粘在一起。\n",
        "        # '[chat_history_ids, new_user_input_ids]' 是一个包含两个张量的列表。\n",
        "        # 'dim=-1' 指示它在最后一个维度上（也就是句子的长度维度上）进行拼接。\n",
        "        # 效果: tensor([1,2,3]) 和 tensor([4,5]) 拼接后变成 tensor([1,2,3,4,5])\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "    # --- 模型生成回复 ---\n",
        "    # 把完整的“剧本”喂给模型，让它生成续写。\n",
        "    # 我们使用上一节课学的采样策略，让对话更有趣！\n",
        "    # 'pad_token_id=tokenizer.eos_token_id' 是一个重要的技术细节。\n",
        "    #   因为我们的句子可能长短不一，模型在生成时可能需要内部填充。\n",
        "    #   我们告诉它，请用“对话结束”这个特殊符号来进行填充，这是对话模型的一个惯例。\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,  # 设置一个足够长的上限，让对话可以延续\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # --- 解码并打印回复 ---\n",
        "    # 'chat_history_ids' 现在包含了“到目前为止的全部聊天记录”+“机器人刚刚生成的新回复”。\n",
        "    # 我们只对打印“机器人刚刚生成的新回复”这部分感兴趣。\n",
        "    # 所以，我们用解码器只解码 'chat_history_ids' 中，从 'bot_input_ids' 长度之后开始的部分。\n",
        "    # 'bot_input_ids.shape[-1]' 能获取到输入剧本的长度。\n",
        "    # 'chat_history_ids[:, bot_input_ids.shape[-1]:]' 这种写法是Python中的“切片”，\n",
        "    #   意思是“从所有行(:)中，截取从'输入剧本长度'之后到结尾的列”。\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # 在屏幕上打印机器人的回复。\n",
        "    print(\"DialoGPT: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9jUYNOyWwb",
        "outputId": "cc604ab4-3609-49ea-8942-771d92ebfef3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> You: Hello, how are you today?\n",
            "DialoGPT: Hello. I'm good, and you?\n",
            ">> You: I'm fine\n",
            "DialoGPT: good good, glad to see you're okay.\n",
            ">> You: thank you\n",
            "DialoGPT: No problem, anything else?\n",
            ">> You: I like Saito Asuka\n",
            "DialoGPT: I like Saito Asuka\n",
            ">> You: Do you know her?\n",
            "DialoGPT: Of course I do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwUwmsS9yWuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **情况一：把参数调得很高 (例如 `top_k=0`, `top_p=1.0`)**\n",
        "\n",
        "*   **`top_k=0`**: `0` 是一个特殊值，它意味着**关闭 Top-K 筛选**。\n",
        "*   **`top_p=1.0`**: `1.0` (或者 `0.999`) 意味着**关闭 Top-P 筛选**。因为所有词的概率加起来本来就是1.0，所以轮盘里会包含词汇表里几乎所有的词。\n",
        "\n",
        "*   **发生了什么？**\n",
        "    你等于把我们之前给“艺术家”的两个“紧箍咒”都解除了。现在，抽奖的轮盘变得**巨大无比**，包含了模型认为所有**哪怕只有一丝丝可能**的词，其中也包括大量概率极低、几乎不相关的词。\n",
        "\n",
        "*   **对话风格会变成什么样？**\n",
        "    机器人会变得**极其不稳定、天马行空、甚至胡言乱语**。它的“性格”会变得非常**随机和混乱**。\n",
        "    *   你可能会看到它说出一些语法不通、逻辑不顺的话。\n",
        "    *   它也可能突然从一个话题跳到另一个毫不相干的话题。\n",
        "    *   偶尔，它也可能因为这种极度的随机性，迸发出一些非常有创意的、意想不到的句子（虽然大部分时间是“惊吓”而不是“惊喜”）。\n",
        "\n"
      ],
      "metadata": {
        "id": "_x2-TQAKDXzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 再次确保我们的库是最新的。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 导入工具并加载“对话专家”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 这次我们选择 'microsoft/DialoGPT-medium'。\n",
        "# 'Dialo' 就是 'Dialogue' (对话) 的缩写，说明这个模型是专门为聊天而生的。\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "# 加载它的专属翻译官(Tokenizer)。\n",
        "# 对于对话模型，翻译官知道一个特殊的词叫 'eos_token' (end of sentence token)，\n",
        "# 也就是我们理论里说的那个特殊分隔符，用来分隔每一轮的对话。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载它的模型大脑，并把它搬到GPU上。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 构建对话循环，让聊天动起来！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们将进行一个5轮的对话。\n",
        "# 'for' 循环是Python中的一种基本语法，'range(5)'会生成一个从0到4的数字序列。\n",
        "# 'step' 这个变量会依次等于 0, 1, 2, 3, 4。\n",
        "# 所以，这个循环里的代码总共会重复执行5次。\n",
        "for step in range(10):\n",
        "\n",
        "    # --- 接收并编码你的输入 ---\n",
        "    # input() 是一个Python函数，它会在屏幕上打印提示信息 \">> You: \"，然后暂停程序，等待你从键盘输入文字。\n",
        "    # 你输入的内容会被存储在 'user_input' 这个字符串变量里。\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    # 使用翻译官把你的输入文字，编码成数字ID。\n",
        "    # 我们需要给你的输入文字后面，手动加上这个特殊的分隔符，\n",
        "    # 这样模型才知道你说完话了，轮到它了。\n",
        "    # 'tokenizer.eos_token' 的值就是那个特殊分隔符，比如 \"<|endoftext|>\"。\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    # --- 核心步骤：拼接聊天记录 ---\n",
        "    # 'bot_input_ids' 将是我们最终要喂给模型的完整“剧本”。\n",
        "    # 我们用一个 'if...else...' 结构来判断：\n",
        "    # 如果是第一轮对话 (step == 0)，那么“剧本”就是你的第一句话。\n",
        "    # 如果不是第一轮对话，那么“剧本”就是“到目前为止的全部聊天记录” + “你刚刚说的这句话”。\n",
        "    if step == 0:\n",
        "        # 第一轮，聊天记录就是你的输入\n",
        "        bot_input_ids = new_user_input_ids\n",
        "    else:\n",
        "        # 'torch.cat' 是PyTorch里的一个函数，像胶水一样，可以把多个张量(Tensor)粘在一起。\n",
        "        # '[chat_history_ids, new_user_input_ids]' 是一个包含两个张量的列表。\n",
        "        # 'dim=-1' 指示它在最后一个维度上（也就是句子的长度维度上）进行拼接。\n",
        "        # 效果: tensor([1,2,3]) 和 tensor([4,5]) 拼接后变成 tensor([1,2,3,4,5])\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "    # --- 模型生成回复 ---\n",
        "    # 把完整的“剧本”喂给模型，让它生成续写。\n",
        "    # 我们使用上一节课学的采样策略，让对话更有趣！\n",
        "    # 'pad_token_id=tokenizer.eos_token_id' 是一个重要的技术细节。\n",
        "    #   因为我们的句子可能长短不一，模型在生成时可能需要内部填充。\n",
        "    #   我们告诉它，请用“对话结束”这个特殊符号来进行填充，这是对话模型的一个惯例。\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,  # 设置一个足够长的上限，让对话可以延续\n",
        "        do_sample=True,\n",
        "        top_k=0,\n",
        "        top_p=1.0,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # --- 解码并打印回复 ---\n",
        "    # 'chat_history_ids' 现在包含了“到目前为止的全部聊天记录”+“机器人刚刚生成的新回复”。\n",
        "    # 我们只对打印“机器人刚刚生成的新回复”这部分感兴趣。\n",
        "    # 所以，我们用解码器只解码 'chat_history_ids' 中，从 'bot_input_ids' 长度之后开始的部分。\n",
        "    # 'bot_input_ids.shape[-1]' 能获取到输入剧本的长度。\n",
        "    # 'chat_history_ids[:, bot_input_ids.shape[-1]:]' 这种写法是Python中的“切片”，\n",
        "    #   意思是“从所有行(:)中，截取从'输入剧本长度'之后到结尾的列”。\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # 在屏幕上打印机器人的回复。\n",
        "    print(\"DialoGPT: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9e1ubx_yWsb",
        "outputId": "0ce0ec1b-fb05-407c-a648-6292503c5c1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> You: Hello\n",
            "DialoGPT: sorry for that?\n",
            ">> You: Hello, how are you today?\n",
            "DialoGPT: excuse me?\n",
            ">> You: ?\n",
            "DialoGPT: It's your job to know?\n",
            ">> You: I like Saito Asuka best!\n",
            "DialoGPT: Oh, okay! Thanks!\n",
            ">> You: Do you know her?\n",
            "DialoGPT: You do! Lilly is best 2 Camera Man\n",
            ">> You: I can't know you\n",
            "DialoGPT: Cliff is my dad\n",
            ">> You: I don't understand\n",
            "DialoGPT: You're your father's son\n",
            ">> You: 666\n",
            "DialoGPT: Or he's the kid thighskin persecuted\n",
            ">> You: pardon\n",
            "DialoGPT: Hmmrre him if you go?\n",
            ">> You: what?\n",
            "DialoGPT: he is a great allen dingo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V88SbGTXyWqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 再次确保我们的库是最新的。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 导入工具并加载“对话专家”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 这次我们选择 'microsoft/DialoGPT-medium'。\n",
        "# 'Dialo' 就是 'Dialogue' (对话) 的缩写，说明这个模型是专门为聊天而生的。\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "# 加载它的专属翻译官(Tokenizer)。\n",
        "# 对于对话模型，翻译官知道一个特殊的词叫 'eos_token' (end of sentence token)，\n",
        "# 也就是我们理论里说的那个特殊分隔符，用来分隔每一轮的对话。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载它的模型大脑，并把它搬到GPU上。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 构建对话循环，让聊天动起来！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们将进行一个5轮的对话。\n",
        "# 'for' 循环是Python中的一种基本语法，'range(5)'会生成一个从0到4的数字序列。\n",
        "# 'step' 这个变量会依次等于 0, 1, 2, 3, 4。\n",
        "# 所以，这个循环里的代码总共会重复执行5次。\n",
        "for step in range(10):\n",
        "\n",
        "    # --- 接收并编码你的输入 ---\n",
        "    # input() 是一个Python函数，它会在屏幕上打印提示信息 \">> You: \"，然后暂停程序，等待你从键盘输入文字。\n",
        "    # 你输入的内容会被存储在 'user_input' 这个字符串变量里。\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    # 使用翻译官把你的输入文字，编码成数字ID。\n",
        "    # 我们需要给你的输入文字后面，手动加上这个特殊的分隔符，\n",
        "    # 这样模型才知道你说完话了，轮到它了。\n",
        "    # 'tokenizer.eos_token' 的值就是那个特殊分隔符，比如 \"<|endoftext|>\"。\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    # --- 核心步骤：拼接聊天记录 ---\n",
        "    # 'bot_input_ids' 将是我们最终要喂给模型的完整“剧本”。\n",
        "    # 我们用一个 'if...else...' 结构来判断：\n",
        "    # 如果是第一轮对话 (step == 0)，那么“剧本”就是你的第一句话。\n",
        "    # 如果不是第一轮对话，那么“剧本”就是“到目前为止的全部聊天记录” + “你刚刚说的这句话”。\n",
        "    if step == 0:\n",
        "        # 第一轮，聊天记录就是你的输入\n",
        "        bot_input_ids = new_user_input_ids\n",
        "    else:\n",
        "        # 'torch.cat' 是PyTorch里的一个函数，像胶水一样，可以把多个张量(Tensor)粘在一起。\n",
        "        # '[chat_history_ids, new_user_input_ids]' 是一个包含两个张量的列表。\n",
        "        # 'dim=-1' 指示它在最后一个维度上（也就是句子的长度维度上）进行拼接。\n",
        "        # 效果: tensor([1,2,3]) 和 tensor([4,5]) 拼接后变成 tensor([1,2,3,4,5])\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "    # --- 模型生成回复 ---\n",
        "    # 把完整的“剧本”喂给模型，让它生成续写。\n",
        "    # 我们使用上一节课学的采样策略，让对话更有趣！\n",
        "    # 'pad_token_id=tokenizer.eos_token_id' 是一个重要的技术细节。\n",
        "    #   因为我们的句子可能长短不一，模型在生成时可能需要内部填充。\n",
        "    #   我们告诉它，请用“对话结束”这个特殊符号来进行填充，这是对话模型的一个惯例。\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,  # 设置一个足够长的上限，让对话可以延续\n",
        "        do_sample=True,\n",
        "        top_k=500,\n",
        "        top_p=0.5,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # --- 解码并打印回复 ---\n",
        "    # 'chat_history_ids' 现在包含了“到目前为止的全部聊天记录”+“机器人刚刚生成的新回复”。\n",
        "    # 我们只对打印“机器人刚刚生成的新回复”这部分感兴趣。\n",
        "    # 所以，我们用解码器只解码 'chat_history_ids' 中，从 'bot_input_ids' 长度之后开始的部分。\n",
        "    # 'bot_input_ids.shape[-1]' 能获取到输入剧本的长度。\n",
        "    # 'chat_history_ids[:, bot_input_ids.shape[-1]:]' 这种写法是Python中的“切片”，\n",
        "    #   意思是“从所有行(:)中，截取从'输入剧本长度'之后到结尾的列”。\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # 在屏幕上打印机器人的回复。\n",
        "    print(\"DialoGPT: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPLNLJbjyWoS",
        "outputId": "52c8a880-5fa9-4f08-a249-95342d41e836"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> You: Hello, how are you today?\n",
            "DialoGPT: Hello, I'm doing pretty well. How are you?\n",
            ">> You: I 'm okay!\n",
            "DialoGPT: That's good!\n",
            ">> You: Can you tell me anything about LLM?\n",
            "DialoGPT: It's a great place to live.\n",
            ">> You: how to be rich?\n",
            "DialoGPT: That's a good question!\n",
            ">> You: Tell me!\n",
            "DialoGPT: I can tell you!\n",
            ">> You: what\n",
            "DialoGPT: It's a great place to live!\n",
            ">> You: where?\n",
            "DialoGPT: Oh, I can tell you!\n",
            ">> You: stop!\n",
            "DialoGPT: Oh, stop!\n",
            ">> You: Who are you?\n",
            "DialoGPT: I'm going to be okay!\n",
            ">> You: 666\n",
            "DialoGPT: I don't think so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XnovPmxMyWmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### **情况二：把参数调得很低 (例如 `top_k=2`, `top_p=0.3`)**\n",
        "\n",
        "*   **`top_k=2`**: 意味着每次抽奖的轮盘上**最多只有2个词**。\n",
        "*   **`top_p=0.3`**: 意味着轮盘上的词，它们的概率加起来不会超过30%。通常这也意味着轮盘上只有极少数几个词。\n",
        "\n",
        "*   **发生了什么？**\n",
        "    你给“艺术家”戴上了一个**非常非常紧**的“紧箍咒”。它的创作空间被极大地压缩了。每次选择，都只能从那一两个概率最高的词里进行。\n",
        "\n",
        "*   **对话风格会变成什么样？**\n",
        "    机器人的回答会变得**非常保守、确定、缺乏多样性，甚至有点呆板和重复**。它的“性格”会向我们最开始学的**“绝对理性派”（Greisty Search）**严重趋同。\n",
        "    *   虽然还有一点点随机性（可以在两个选项里抽一个），但因为选项太少，所以每次生成的回答会非常相似。\n",
        "    *   你可能会发现，对于同一个问题，它翻来覆去总是那几句差不多的回答。\n",
        "    *   对话会很快变得无聊，因为它失去了“惊喜感”。\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iaIDFccIFoVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 再次确保我们的库是最新的。\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 导入工具并加载“对话专家”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# 这次我们选择 'microsoft/DialoGPT-medium'。\n",
        "# 'Dialo' 就是 'Dialogue' (对话) 的缩写，说明这个模型是专门为聊天而生的。\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "\n",
        "# 加载它的专属翻译官(Tokenizer)。\n",
        "# 对于对话模型，翻译官知道一个特殊的词叫 'eos_token' (end of sentence token)，\n",
        "# 也就是我们理论里说的那个特殊分隔符，用来分隔每一轮的对话。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 加载它的模型大脑，并把它搬到GPU上。\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 构建对话循环，让聊天动起来！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们将进行一个5轮的对话。\n",
        "# 'for' 循环是Python中的一种基本语法，'range(5)'会生成一个从0到4的数字序列。\n",
        "# 'step' 这个变量会依次等于 0, 1, 2, 3, 4。\n",
        "# 所以，这个循环里的代码总共会重复执行5次。\n",
        "for step in range(10):\n",
        "\n",
        "    # --- 接收并编码你的输入 ---\n",
        "    # input() 是一个Python函数，它会在屏幕上打印提示信息 \">> You: \"，然后暂停程序，等待你从键盘输入文字。\n",
        "    # 你输入的内容会被存储在 'user_input' 这个字符串变量里。\n",
        "    user_input = input(\">> You: \")\n",
        "\n",
        "    # 使用翻译官把你的输入文字，编码成数字ID。\n",
        "    # 我们需要给你的输入文字后面，手动加上这个特殊的分隔符，\n",
        "    # 这样模型才知道你说完话了，轮到它了。\n",
        "    # 'tokenizer.eos_token' 的值就是那个特殊分隔符，比如 \"<|endoftext|>\"。\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
        "\n",
        "    # --- 核心步骤：拼接聊天记录 ---\n",
        "    # 'bot_input_ids' 将是我们最终要喂给模型的完整“剧本”。\n",
        "    # 我们用一个 'if...else...' 结构来判断：\n",
        "    # 如果是第一轮对话 (step == 0)，那么“剧本”就是你的第一句话。\n",
        "    # 如果不是第一轮对话，那么“剧本”就是“到目前为止的全部聊天记录” + “你刚刚说的这句话”。\n",
        "    if step == 0:\n",
        "        # 第一轮，聊天记录就是你的输入\n",
        "        bot_input_ids = new_user_input_ids\n",
        "    else:\n",
        "        # 'torch.cat' 是PyTorch里的一个函数，像胶水一样，可以把多个张量(Tensor)粘在一起。\n",
        "        # '[chat_history_ids, new_user_input_ids]' 是一个包含两个张量的列表。\n",
        "        # 'dim=-1' 指示它在最后一个维度上（也就是句子的长度维度上）进行拼接。\n",
        "        # 效果: tensor([1,2,3]) 和 tensor([4,5]) 拼接后变成 tensor([1,2,3,4,5])\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "\n",
        "    # --- 模型生成回复 ---\n",
        "    # 把完整的“剧本”喂给模型，让它生成续写。\n",
        "    # 我们使用上一节课学的采样策略，让对话更有趣！\n",
        "    # 'pad_token_id=tokenizer.eos_token_id' 是一个重要的技术细节。\n",
        "    #   因为我们的句子可能长短不一，模型在生成时可能需要内部填充。\n",
        "    #   我们告诉它，请用“对话结束”这个特殊符号来进行填充，这是对话模型的一个惯例。\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,  # 设置一个足够长的上限，让对话可以延续\n",
        "        do_sample=True,\n",
        "        top_k=2,\n",
        "        top_p=0.3,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # --- 解码并打印回复 ---\n",
        "    # 'chat_history_ids' 现在包含了“到目前为止的全部聊天记录”+“机器人刚刚生成的新回复”。\n",
        "    # 我们只对打印“机器人刚刚生成的新回复”这部分感兴趣。\n",
        "    # 所以，我们用解码器只解码 'chat_history_ids' 中，从 'bot_input_ids' 长度之后开始的部分。\n",
        "    # 'bot_input_ids.shape[-1]' 能获取到输入剧本的长度。\n",
        "    # 'chat_history_ids[:, bot_input_ids.shape[-1]:]' 这种写法是Python中的“切片”，\n",
        "    #   意思是“从所有行(:)中，截取从'输入剧本长度'之后到结尾的列”。\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    # 在屏幕上打印机器人的回复。\n",
        "    print(\"DialoGPT: \" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CdxE3H6yWj7",
        "outputId": "cdef6fef-3f62-4bfd-b386-8b301c9002d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> You: Hello,how are you?\n",
            "DialoGPT: I'm good, how are you?\n",
            ">> You: I'm ill today\n",
            "DialoGPT: I'm sorry to hear that.\n",
            ">> You: Can you help me to buy some food?\n",
            "DialoGPT: I can't, I'm sick\n",
            ">> You: What a pity\n",
            "DialoGPT: I'm sorry\n",
            ">> You: Do you know Saito Asuka?\n",
            "DialoGPT: I don't know her\n",
            ">> You: She is my favorite idol\n",
            "DialoGPT: I'm sorry\n",
            ">> You: She is very cute and Beautiful!\n",
            "DialoGPT: I'm sorry\n",
            ">> You: Can you say something else?\n",
            "DialoGPT: I'm sorry\n",
            ">> You: 666\n",
            "DialoGPT: I'm sorry\n",
            ">> You: Hello?\n",
            "DialoGPT: I'm sorry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vz8O_gLAFmNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 我已完成实践。请对作业和思考题进行讲解。解读top_k 和 top_p的影响。"
      ],
      "metadata": {
        "id": "5q-kQkH9DHwq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3MV9XBLyWhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！很高兴你完成了和AI的第一次亲密对话。现在，我们来深入复盘一下这次的作业和思考题，这会极大加深你对生成式AI行为的理解。\n",
        "\n",
        "---\n",
        "\n",
        "### **作业讲解：`top_k` 和 `top_p` 的影响**\n",
        "\n",
        "在 `model.generate()` 中修改 `top_k` 和 `top_p`，本质上是在调整我们上一节课学的“采样”策略中，那个“随机抽奖”的轮盘大小。\n",
        "\n",
        "#### **情况一：把参数调得很高 (例如 `top_k=0`, `top_p=1.0`)**\n",
        "\n",
        "*   **`top_k=0`**: `0` 是一个特殊值，它意味着**关闭 Top-K 筛选**。\n",
        "*   **`top_p=1.0`**: `1.0` (或者 `0.999`) 意味着**关闭 Top-P 筛选**。因为所有词的概率加起来本来就是1.0，所以轮盘里会包含词汇表里几乎所有的词。\n",
        "\n",
        "*   **发生了什么？**\n",
        "    你等于把我们之前给“艺术家”的两个“紧箍咒”都解除了。现在，抽奖的轮盘变得**巨大无比**，包含了模型认为所有**哪怕只有一丝丝可能**的词，其中也包括大量概率极低、几乎不相关的词。\n",
        "\n",
        "*   **对话风格会变成什么样？**\n",
        "    机器人会变得**极其不稳定、天马行空、甚至胡言乱语**。它的“性格”会变得非常**随机和混乱**。\n",
        "    *   你可能会看到它说出一些语法不通、逻辑不顺的话。\n",
        "    *   它也可能突然从一个话题跳到另一个毫不相干的话题。\n",
        "    *   偶尔，它也可能因为这种极度的随机性，迸发出一些非常有创意的、意想不到的句子（虽然大部分时间是“惊吓”而不是“惊喜”）。\n",
        "\n",
        "#### **情况二：把参数调得很低 (例如 `top_k=2`, `top_p=0.3`)**\n",
        "\n",
        "*   **`top_k=2`**: 意味着每次抽奖的轮盘上**最多只有2个词**。\n",
        "*   **`top_p=0.3`**: 意味着轮盘上的词，它们的概率加起来不会超过30%。通常这也意味着轮盘上只有极少数几个词。\n",
        "\n",
        "*   **发生了什么？**\n",
        "    你给“艺术家”戴上了一个**非常非常紧**的“紧箍咒”。它的创作空间被极大地压缩了。每次选择，都只能从那一两个概率最高的词里进行。\n",
        "\n",
        "*   **对话风格会变成什么样？**\n",
        "    机器人的回答会变得**非常保守、确定、缺乏多样性，甚至有点呆板和重复**。它的“性格”会向我们最开始学的**“绝对理性派”（Greisty Search）**严重趋同。\n",
        "    *   虽然还有一点点随机性（可以在两个选项里抽一个），但因为选项太少，所以每次生成的回答会非常相似。\n",
        "    *   你可能会发现，对于同一个问题，它翻来覆去总是那几句差不多的回答。\n",
        "    *   对话会很快变得无聊，因为它失去了“惊喜感”。\n",
        "\n",
        "#### **总结**\n",
        "\n",
        "`top_k` 和 `top_p` 就像是控制机器人**“想象力”**和**“纪律性”**之间平衡的两个旋钮：\n",
        "\n",
        "*   **参数调高**：想象力↑，纪律性↓ (更像一个疯狂的艺术家)\n",
        "*   **参数调低**：想象力↓，纪律性↑ (更像一个严谨的工程师)\n",
        "*   **合适的中间值 (比如 `top_k=50`, `top_p=0.95`)**：在保持逻辑连贯（纪律性）的同时，又能提供足够的创造力（想象力），达到最佳的对话体验。\n",
        "\n",
        "---\n",
        "\n",
        "### **思考题讲解 (非常重要！)**\n",
        "\n",
        "**问题：** 如果我们忘记了用**新**的 `chat_history_ids` 去覆盖**旧**的 `chat_history_ids` 这一步，对话会变成什么样子？机器人会得什么“病”？\n",
        "\n",
        "**答案：**\n",
        "\n",
        "如果发生了这个错误，我们的代码逻辑会变成这样：\n",
        "1.  **第一轮**：`bot_input_ids` 是你的第一句话。`chat_history_ids` 包含了你的话和机器人的第一句回复。\n",
        "2.  **第二轮**：你输入了第二句话。代码本应把**最新的 `chat_history_ids`** 和你的第二句话拼接起来。但由于我们忘了更新，它拿到的还是**空的、或者说不存在的 `chat_history_ids`**。于是，它只能把**你刚刚输入的第二句话**作为唯一的上下文。\n",
        "3.  **第三轮、第四轮...**：同样的问题重复发生。\n",
        "\n",
        "**结论：**\n",
        "\n",
        "机器人会得一种我们之前提过的病——**“严重短期失忆症” (Amnesia)**！\n",
        "\n",
        "**对话会变成这个样子：**\n",
        "\n",
        "*   **你**: \"你好，你喜欢吃什么水果？\"\n",
        "*   **机器人**: \"我最喜欢吃苹果。\" (它看到了你的问题，正常回答)\n",
        "*   **你**: \"为什么呢？\"\n",
        "*   **机器人**: (它的上下文里只有“为什么呢？”这一句话) 它完全无法理解你在问什么，可能会给出一个完全不相关的、基于“为什么呢”这三个字的通用回答，比如：**\"这是一个好问题。\"** 或者 **\"我不知道为什么。\"**\n",
        "*   **你**: \"你刚才不是说你喜欢苹果吗？\"\n",
        "*   **机器人**: (它的上下文里只有“你刚才不是说你喜欢苹果吗？”) 它可能会回答：**\"哦是吗？我喜欢苹果吗？\"**\n",
        "\n",
        "**机器人将永远无法进行真正意义上的“多轮对话”**。它会变成一个只能“就事论事”的、**单轮问答机**。它说的每一句话，都只基于你刚刚说的那一句，而完全遗忘了你们之间发生过的所有对话历史。\n",
        "\n",
        "这个思考题的核心，就是为了让你深刻地理解：对于对话模型而言，**持续维护和更新一个包含完整上下文的 `chat_history`（聊天记录），是实现连贯、有记忆的对话的生命线。** 我们今天实践的代码中，`torch.cat` 和 `chat_history_ids = model.generate(...)` 这两步，就是这条生命线的具体实现。\n",
        "\n",
        "---\n",
        "\n",
        "你对这些讲解都理解了吗？尤其是这个思考题，如果你能完全想通，那么你就已经掌握了构建多轮对话系统的核心精髓。\n",
        "\n",
        "如果没问题，我们就可以进入今天最后一个，也是非常有用的一个实践环节：**实现一个文本摘要AI**！"
      ],
      "metadata": {
        "id": "hGfoeZeXEJYU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eM1SAIBDyWfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1、按照本方案开始下一步，讲解理论和指导实践。\n",
        "\n",
        "2、要求用学生听得懂的通俗易懂的语言为我进行讲解和指导我进行实践。\n",
        "\n",
        "3、编程环境基于google的colab。由于我对python基础不熟练，注意代码中都要有极尽细致详细的注释，保证我能看懂所有变量，函数，语法结构和每行语句的功能，以便我能进行实践复现。\n",
        "\n",
        "4、按照\"step by step\"原则逐步指导我，并及时根据我的反馈进行调整。"
      ],
      "metadata": {
        "id": "-w6sPzHZF9z7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T_mP_Ud_yWdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！我们已经成功地创造了一个能和我们聊天的伙伴。现在，我们要赋予AI一项非常实用、非常有价值的新技能——**阅读长篇大论并抓住重点**。\n",
        "\n",
        "这项技能，就是**文本摘要（Text Summarization）**。它能把一篇长长的新闻、报告或者文章，浓缩成几句简短的核心要点，为我们节省大量的阅读时间。\n",
        "\n",
        "---\n",
        "\n",
        "### **进阶篇 · 第一步：从“分类”到“生成”**\n",
        "### **Part 3: 实现一个“文本摘要AI” (预计用时: 1-1.5小时)**\n",
        "\n",
        "#### **1. 理论学习：AI如何“阅读理解”并“写摘要”？**\n",
        "\n",
        "想象一下，老师让你读一篇很长的课文，然后写一个段落大意。你会怎么做？\n",
        "\n",
        "1.  **第一步：通读全文（阅读理解）**。你不会只看第一句就开始写。你会从头到尾把整篇文章都读一遍，在脑子里形成一个对文章**整体内容、核心思想**的理解。\n",
        "2.  **第二步：提笔写作（组织语言）**。读懂了之后，你才会合上书，用**你自己的话**，根据你脑子里的那个“整体理解”，重新组织语言，写出简短的摘要。\n",
        "\n",
        "我们今天要用的摘要模型，它的“大脑”结构就完美地模拟了这个过程！它有一个特殊的名字，叫做 **Encoder-Decoder (编码器-解码器) 架构**，也叫 **Sequence-to-Sequence (序列到序列) 模型**。\n",
        "\n",
        "它由两个“半脑”组成：\n",
        "\n",
        "*   **Encoder (编码器/阅读理解专家)**：\n",
        "    *   **它的唯一工作，就是“阅读”**。你把一整篇长文章（一个长序列）喂给它，它会从头到尾、一词不漏地进行处理，把整篇文章的精华和语义，压缩成一个包含核心信息的“思想向量”（一串特殊的数字）。这个“思想向量”就代表了它对文章的**整体理解**。\n",
        "\n",
        "*   **Decoder (解码器/写作专家)**：\n",
        "    *   **它的唯一工作，就是“写作”**。它自己**不会**去读原文。它只接收来自Encoder的那个“思想向量”。\n",
        "    *   然后，它就根据这个“思想向量”里蕴含的整体理解，一个词一个词地生成全新的、简短的摘要文本（另一个短序列）。\n",
        "\n",
        "**这和我们之前的对话模型（如GPT2, DialoGPT）有什么根本不同？**\n",
        "\n",
        "*   **对话模型 (Decoder-only)**：它们是“续写大师”。它们只有一个“半脑”（Decoder），没有专门的“阅读理解专家”。它们擅长根据前面的文字，预测后面的文字，非常适合做续写和对话。但如果你给它一篇长文，它可能会只关注文章的末尾部分，而忘了开头讲了什么。\n",
        "*   **摘要模型 (Encoder-Decoder)**：它们是“阅读理解大师”。它们有两个“半脑”协同工作，确保了在动笔“写”摘要之前，已经对**全部**原文内容有了透彻的理解。这让它们在**翻译、摘要**这类需要“先完整理解，再整体输出”的任务上表现得极其出色。\n",
        "\n",
        "**我们选择的“大脑”：**\n",
        "今天我们请来的专家是 `facebook/bart-large-cnn`。\n",
        "*   `BART` 是一种非常强大的Encoder-Decoder模型。\n",
        "*   `large` 代表它是一个参数量很大的“博士后”版本，能力很强。\n",
        "*   `cnn` 代表它是在一个巨大的新闻文章和摘要数据集（CNN/DailyMail）上专门训练过的，所以它是一位名副其实的“新闻摘要专家”。\n",
        "\n",
        "#### **2. 编程实践：三分钟读完一篇新闻**\n",
        "\n",
        "我们先用最简单快捷的`pipeline`来感受一下它的威力，然后再手动拆解一遍，看看内部的“阅读”和“写作”过程是如何发生的。\n",
        "\n",
        "**实践代码（请在Colab中，将硬件加速器设置为`T4 GPU`后运行）：**\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备一篇需要被“压缩”的长文章\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们虚构了一篇关于人工智能发展的新闻报道作为示例。\n",
        "# 注意这个字符串前面有一个 'f'，并且用了三个双引号 \"\"\"...\"\"\"。\n",
        "# 这是一种Python的语法，叫做 f-string，它允许我们在字符串中方便地嵌入变量。\n",
        "# 三个引号则允许我们创建一个包含换行符的多行字符串。\n",
        "ARTICLE = \"\"\"\n",
        "Artificial intelligence (AI) has been a topic of intense research and public fascination for decades.\n",
        "Initially confined to the realms of science fiction, AI has steadily evolved into a tangible force that is reshaping industries and daily life.\n",
        "The journey began with rule-based systems, where programmers explicitly wrote rules for the AI to follow.\n",
        "While effective for specific tasks, these systems lacked flexibility. The real breakthrough came with the advent of machine learning,\n",
        "particularly deep learning, in the early 2010s. This paradigm shift allowed machines to learn from vast amounts of data,\n",
        "identifying patterns and making predictions without being explicitly programmed. Today, we are witnessing the era of Large Language Models (LLMs)\n",
        "like GPT-4, which can understand and generate human-like text to an unprecedented degree, powering everything from advanced search engines to creative writing assistants.\n",
        "The ethical implications and societal impact of this rapid progress remain a critical area of ongoing discussion.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 体验“一键摘要”的魔力 (使用Pipeline)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import pipeline\n",
        "\n",
        "# 创建一个专门用于“summarization”(摘要)任务的管道。\n",
        "# 我们明确指定使用我们选好的“新闻摘要专家”。\n",
        "summarizer_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# 直接把我们的长文章喂给这个管道。\n",
        "# 管道会自动完成我们之前学的所有步骤：编码 -> 模型推理 -> 解码。\n",
        "pipeline_summary = summarizer_pipeline(ARTICLE, max_length=60, min_length=30, do_sample=False)\n",
        "\n",
        "print(\"--- 1. Pipeline 自动生成的摘要 ---\")\n",
        "# pipeline的输出是一个列表，列表里有一个字典，我们需要提取字典中'summary_text'键对应的值。\n",
        "print(pipeline_summary[0]['summary_text'])\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 手动拆解，亲历“阅读”与“写作”的过程\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# 定义我们要使用的模型名称\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "# 加载专属的翻译官(Tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 关键！这次我们加载的模型类是 'AutoModelForSeq2SeqLM'。\n",
        "# 'Seq2Seq' 就是 'Sequence-to-Sequence' 的缩写，明确告诉程序我们要加载一个Encoder-Decoder架构的模型。\n",
        "# 同样，把它搬到GPU上。\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# --- 4.1 \"阅读理解\"阶段 (Encoding) ---\n",
        "# 使用翻译官把长文章编码成数字ID。\n",
        "# - max_length=1024: BART模型能接受的最大文章长度是1024个token，超过部分会被截断。\n",
        "# - truncation=True: 明确告诉tokenizer，如果文章太长，就截断它。\n",
        "# - return_tensors=\"pt\": 返回PyTorch张量。\n",
        "inputs = tokenizer(ARTICLE, max_length=1024, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# --- 4.2 \"提笔写作\"阶段 (Decoding/Generation) ---\n",
        "# 把编码好的“理解”喂给模型，让它生成摘要。\n",
        "# - inputs['input_ids']: 这就是编码后的文章本体。\n",
        "# - num_beams=4: 对于摘要任务，使用Beam Search通常能得到更连贯、更准确的结果。\n",
        "# - max_length=60: 摘要的最大长度。\n",
        "# - min_length=30: 摘要的最小长度。\n",
        "# - early_stopping=True: 当所有beam都生成了结束符时，提前停止。\n",
        "summary_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    num_beams=4,\n",
        "    max_length=60,\n",
        "    min_length=30,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# --- 4.3 翻译成文 ---\n",
        "# 把模型生成的摘要ID，解码成人类能读的文字。\n",
        "manual_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- 2. 我们手动操作生成的摘要 ---\")\n",
        "print(manual_summary)\n",
        "```\n",
        "\n",
        "#### **3. 结果解读与作业**\n",
        "\n",
        "运行代码后，你会看到 `pipeline` 生成的摘要和我们手动操作生成的摘要几乎一模一样。这证明你已经完全理解并复现了摘要模型的核心工作流程！你成功地指挥一个AI读完了一篇长文，并为你写出了它的“读后感”。\n",
        "\n",
        "**本节总结与作业：**\n",
        "\n",
        "1.  **当一回“编辑”**：去网上（比如BBC, CNN等新闻网站）找一篇你感兴趣的、比较长的英文新闻报道，把它的正文复制下来，替换掉代码中的 `ARTICLE` 变量，看看我们这位“摘要专家”对真实世界的新闻处理得怎么样？它抓到的重点和你自己理解的一致吗？\n",
        "\n",
        "2.  **思考题 (检验你是否理解了模型架构)**：我们今天用了Encoder-Decoder架构的`BART`模型来做摘要。上一节课，我们用了Decoder-only架构的`DialoGPT`模型来做对话。请思考一下，如果我们**不用`BART`**，而是把那篇长文章直接喂给**`DialoGPT`**，让它去“续写”，你觉得会发生什么？它能生成一篇好的摘要吗？为什么？（提示：回想一下“续写大师”和“阅读理解大师”的根本区别。）\n",
        "\n",
        "当你完成了实践和思考后，请告诉我你的答案。这个问题能极好地巩固你对不同LLM架构的理解，这对你未来为不同任务选择合适模型至关重要。完成这一步后，我们进阶篇的第一个大模块就圆满结束了！"
      ],
      "metadata": {
        "id": "69RkdyX7F-WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h44Ihgg2yWbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备\n",
        "# ----------------------------------------------------------------------------------\n",
        "!pip install transformers accelerate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备一篇需要被“压缩”的长文章\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们虚构了一篇关于人工智能发展的新闻报道作为示例。\n",
        "# 注意这个字符串前面有一个 'f'，并且用了三个双引号 \"\"\"...\"\"\"。\n",
        "# 这是一种Python的语法，叫做 f-string，它允许我们在字符串中方便地嵌入变量。\n",
        "# 三个引号则允许我们创建一个包含换行符的多行字符串。\n",
        "ARTICLE = \"\"\"\n",
        "Artificial intelligence (AI) has been a topic of intense research and public fascination for decades.\n",
        "Initially confined to the realms of science fiction, AI has steadily evolved into a tangible force that is reshaping industries and daily life.\n",
        "The journey began with rule-based systems, where programmers explicitly wrote rules for the AI to follow.\n",
        "While effective for specific tasks, these systems lacked flexibility. The real breakthrough came with the advent of machine learning,\n",
        "particularly deep learning, in the early 2010s. This paradigm shift allowed machines to learn from vast amounts of data,\n",
        "identifying patterns and making predictions without being explicitly programmed. Today, we are witnessing the era of Large Language Models (LLMs)\n",
        "like GPT-4, which can understand and generate human-like text to an unprecedented degree, powering everything from advanced search engines to creative writing assistants.\n",
        "The ethical implications and societal impact of this rapid progress remain a critical area of ongoing discussion.\n",
        "\"\"\"\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 体验“一键摘要”的魔力 (使用Pipeline)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import pipeline\n",
        "\n",
        "# 创建一个专门用于“summarization”(摘要)任务的管道。\n",
        "# 我们明确指定使用我们选好的“新闻摘要专家”。\n",
        "summarizer_pipeline = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "# 直接把我们的长文章喂给这个管道。\n",
        "# 管道会自动完成我们之前学的所有步骤：编码 -> 模型推理 -> 解码。\n",
        "pipeline_summary = summarizer_pipeline(ARTICLE, max_length=60, min_length=30, do_sample=False)\n",
        "\n",
        "print(\"--- 1. Pipeline 自动生成的摘要 ---\")\n",
        "# pipeline的输出是一个列表，列表里有一个字典，我们需要提取字典中'summary_text'键对应的值。\n",
        "print(pipeline_summary[0]['summary_text'])\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 手动拆解，亲历“阅读”与“写作”的过程\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# 定义我们要使用的模型名称\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "\n",
        "# 加载专属的翻译官(Tokenizer)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 关键！这次我们加载的模型类是 'AutoModelForSeq2SeqLM'。\n",
        "# 'Seq2Seq' 就是 'Sequence-to-Sequence' 的缩写，明确告诉程序我们要加载一个Encoder-Decoder架构的模型。\n",
        "# 同样，把它搬到GPU上。\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cuda\")\n",
        "\n",
        "# --- 4.1 \"阅读理解\"阶段 (Encoding) ---\n",
        "# 使用翻译官把长文章编码成数字ID。\n",
        "# - max_length=1024: BART模型能接受的最大文章长度是1024个token，超过部分会被截断。\n",
        "# - truncation=True: 明确告诉tokenizer，如果文章太长，就截断它。\n",
        "# - return_tensors=\"pt\": 返回PyTorch张量。\n",
        "inputs = tokenizer(ARTICLE, max_length=1024, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# --- 4.2 \"提笔写作\"阶段 (Decoding/Generation) ---\n",
        "# 把编码好的“理解”喂给模型，让它生成摘要。\n",
        "# - inputs['input_ids']: 这就是编码后的文章本体。\n",
        "# - num_beams=4: 对于摘要任务，使用Beam Search通常能得到更连贯、更准确的结果。\n",
        "# - max_length=60: 摘要的最大长度。\n",
        "# - min_length=30: 摘要的最小长度。\n",
        "# - early_stopping=True: 当所有beam都生成了结束符时，提前停止。\n",
        "summary_ids = model.generate(\n",
        "    inputs['input_ids'],\n",
        "    num_beams=4,\n",
        "    max_length=60,\n",
        "    min_length=30,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# --- 4.3 翻译成文 ---\n",
        "# 把模型生成的摘要ID，解码成人类能读的文字。\n",
        "manual_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n--- 2. 我们手动操作生成的摘要 ---\")\n",
        "print(manual_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456,
          "referenced_widgets": [
            "de926fb4afc2411f9f2a211676f4ec76",
            "046cd073082a4de19a4e3c82f396e131",
            "be2dc1653a204b5a99ad185a674debe4",
            "841458a9a32646909c6d8b6233810545",
            "3ceba4f9d0a440efa6a81b39a5ffe0da",
            "552fad97e8984950964562518e04a893",
            "cb1f19a8fc65411fa76736148ebb2dce",
            "16a8b461c35c486ea592cbb0e3aaf783",
            "cf68cf90f34f417d9a42d00d4aa597da",
            "a9154623c636468cbe77cb14f40c2435",
            "335a94f761244aa1819c5519050a9434",
            "d8ff8aef51c14b0681a9c8ee0a901c4d",
            "0460ded47e8f441ca9f508d8109f93a8",
            "8b100022718f4121be56ed1f84e53183",
            "d285c1eb85e54d068bda1892978c4220",
            "1be1db7bb1d44a449a75fb16edeffb26",
            "850cf8ee69d1484999230327a8900be1",
            "c1bf774641174c3ba62c7399b1868623",
            "35d350a76de543bba4e4ba51e69c5b71",
            "93b846910dc84bb1b605acbc7cb8a855",
            "47815a01b6de4d008130090405e0c94b",
            "911f04274d5b4118bf1d5580feed4ecf",
            "d202a29d68fe4cd2a759ca5f8e06fc1e",
            "c30b1b06c72747f592944c7f57338605",
            "eab3998735dd47b9b521fa37dd872494",
            "69785a37536248b89af8a66e294c0cc6",
            "c57a1e501c604ce8b4fabc13d7a84277",
            "56adbc82b5644fc68ef82a2f1e7085b0",
            "2d5b9c428bcb40809c1cdfd8d5719901",
            "846c0a47287e4af9a33ad0efc1d83682",
            "a1b4e04a3db14534af45b652531c7a37",
            "767be025b6fc475cb584dd4ce3f264db",
            "8304586638314b9b9643cd624770b7cc",
            "955410ddca2040b49107e8eaa68f694e",
            "1b1fc23ad0d84c8ca0429acd97245057",
            "8895abc26a6149d498700937f01f4384",
            "865b97cb029c49f18c3fce2d6af83377",
            "a0207ea2e7e843c284da59685a8e8ab3",
            "f9b81c1d94ee4b6ba7cbb182c0b582f9",
            "d08bd91e48a74b40b6d5929f467943fc",
            "d0b3727c17e945c79140f609970ed9c9",
            "10318d55af3d400cb18a127a13a62507",
            "e95bf1e07ff54d578eec1489e7367b49",
            "f007c787e0d149d2ba76ea62c5f501a0",
            "ed9517c392a44c47bff7ddb1b5597f61",
            "23b3297b8d8b406ea569f853ad6df4df",
            "1ae3467fdd6e446191d17afc9133ed59",
            "ba9d732605ed469a9e8856533dd5c9d2",
            "8ed95ca95c7242e1aa9b8809b8f3247e",
            "3e209ae29623433591c9da3766b54df1",
            "8e24247f9cf14bf8bfe5c7effbad7163",
            "5f6bad780c624030bd2f8b42b0d8e391",
            "d25a045fd9c14df39e2cf9fd7af9ca28",
            "a65afc9187a845cbbc4b580cc5da64ca",
            "ec68720ee7d74faabfe89c93f612529c",
            "ba3ed38ac6b44ef59d4e67b5b8b2215f",
            "38997162f038457a960d89c7f3a4f60c",
            "d017686a1d0b4631a102fa0a891bada8",
            "40ee7249906f46b2a8c3f78cfe205e84",
            "7c92dcc3776a4daa9a98304772ce4f40",
            "2162375061db4b8aa2a4602872e3900f",
            "810139a6f20442be9b6f5f253b0e6dda",
            "4ac32036d6134e9c9f9c6f3d8638b801",
            "dd29f4ded3034b5e96bcd2ce5763cccf",
            "bac06b1750f5432db17c5f132bebdabd",
            "5f690f218ac544b2947b36292569ae83"
          ]
        },
        "id": "sG5a0Qk0yWZT",
        "outputId": "51c8f57b-e3f0-4815-9893-37388d9f7e69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de926fb4afc2411f9f2a211676f4ec76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8ff8aef51c14b0681a9c8ee0a901c4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d202a29d68fe4cd2a759ca5f8e06fc1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "955410ddca2040b49107e8eaa68f694e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed9517c392a44c47bff7ddb1b5597f61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba3ed38ac6b44ef59d4e67b5b8b2215f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Pipeline 自动生成的摘要 ---\n",
            "Artificial intelligence (AI) has been a topic of intense research and public fascination for decades. The journey began with rule-based systems, where programmers explicitly wrote rules for the AI to follow. The real breakthrough came with the advent of machine learning, particularly deep learning.\n",
            "\n",
            "--- 2. 我们手动操作生成的摘要 ---\n",
            "Artificial intelligence (AI) has been a topic of intense research and public fascination for decades. The journey began with rule-based systems, where programmers explicitly wrote rules for the AI to follow. The real breakthrough came with the advent of machine learning, particularly deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9JVGVrMyWXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 我已完成实践。请对作业和思考题进行讲解."
      ],
      "metadata": {
        "id": "CulbqaPsLPln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！很高兴你已经成功训练出了你的第一位“AI阅读助理”。我们现在就来深入分析一下作业和思考题，这能帮你把不同模型架构的“性格”和“特长”记得更牢。\n",
        "\n",
        "---\n",
        "\n",
        "### **作业讲解：当一回“编辑”**\n",
        "\n",
        "当你把一篇真实世界的新闻报道喂给 `facebook/bart-large-cnn` 模型时，你大概率会得到一个**质量相当高**的摘要。\n",
        "\n",
        "*   **它抓重点通常很准**：因为它是在海量的新闻数据上训练的，它已经学会了新闻报道的常见“套路”。比如，它知道文章的开头和结尾部分通常包含核心信息，知道哪些词（如 \"announced\", \"revealed\", \"according to\"）后面往往跟着关键内容。\n",
        "*   **语言流畅自然**：它生成的摘要读起来会非常通顺，就像一个专业编辑写出来的一样，因为它学习了新闻语言的风格。\n",
        "*   **忠于事实**：最重要的是，摘要里的内容基本都能在原文中找到依据，它很少会“自由发挥”或“添油加醋”。\n",
        "\n",
        "**可能遇到的问题或现象：**\n",
        "\n",
        "*   **对于非常专业的领域（比如一篇医学论文），摘要质量可能会下降**：因为它没在那么多专业的医学论文上训练过，可能会不认识某些专业术语，或者抓不住领域内的核心要点。\n",
        "*   **摘要风格固定**：它生成的摘要风格总是很“新闻化”，如果你给它一篇小说让它摘要，它写出来的摘要读起来也可能像一篇新闻报道。\n",
        "\n",
        "这次作业的核心目的，就是让你亲身体会一个**在特定领域数据上微调过**的模型，在处理该领域任务时是多么地**强大和可靠**。\n",
        "\n",
        "---\n",
        "\n",
        "### **思考题讲解 (检验你是否理解了模型架构)**\n",
        "\n",
        "**问题：** 如果我们不用`BART` (Encoder-Decoder)，而是把那篇长文章直接喂给`DialoGPT` (Decoder-only)，让它去“续写”，会发生什么？它能生成一篇好的摘要吗？为什么？\n",
        "\n",
        "**答案：**\n",
        "\n",
        "它**几乎不可能**生成一篇好的摘要。最终的结果可能会非常奇怪，甚至毫无用处。\n",
        "\n",
        "让我们来分析一下“续写大师” (`DialoGPT`) 收到这个任务指令时，它的“内心活动”：\n",
        "\n",
        "1.  **没有“通读全文”的大脑结构**：\n",
        "    *   `DialoGPT` 是一个 Decoder-only 模型，它没有那个专门负责“阅读理解”的 Encoder 半脑。它的工作机制是**顺序地**读入文字，然后根据它读到的**最后一部分内容**，来预测接下来最可能出现的文字。\n",
        "\n",
        "2.  **注意力“短视”问题**：\n",
        "    *   虽然 Transformer 架构有注意力机制，但对于一个很长的序列（比如我们那篇几百个词的文章），一个 Decoder-only 模型在预测下一个词时，它的“注意力”会**极其强烈地集中在序列的末尾部分**。\n",
        "    *   它可能会记住文章最后一句是“...ongoing discussion.”，但对于文章开头讲的“...rule-based systems”，它的记忆已经非常模糊甚至完全忘记了。\n",
        "\n",
        "**所以，会发生什么？**\n",
        "\n",
        "当你把整篇文章喂给它时，它读完了长长的文章，满脑子都是最后那几句话。然后你让它开始生成，它会认为自己的任务是**“续写”**这篇关于AI伦理讨论的文章，而不是**“总结”**它。\n",
        "\n",
        "**你可能会看到以下几种典型的“失败”结果：**\n",
        "\n",
        "*   **结果一：接着写文章**\n",
        "    *   它可能会直接续写下去，比如：“...ongoing discussion. Furthermore, another important aspect to consider is the potential for job displacement...” 它以为自己是在写文章的下一个段落。\n",
        "\n",
        "*   **结果二：重复文章末尾的句子**\n",
        "    *   由于注意力高度集中在末尾，它可能会开始重复最后几句话的模式或内容，陷入一个循环。\n",
        "\n",
        "*   **结果三：生成与“摘要”无关的对话**\n",
        "    *   因为 `DialoGPT` 的“灵魂”是一个对话机器人，它可能会把你的长文章输入误解为一个非常啰嗦的人在对它说话。然后它可能会生成一句对话式的回复，比如：“Wow, that's a lot of information! What do you want me to do with it?”\n",
        "\n",
        "**为什么？—— 根本原因总结**\n",
        "\n",
        "*   **架构决定天职**：\n",
        "    *   `DialoGPT` (Decoder-only) 的天职是 **“延续上下文”**。它的设计目标是“根据已有的，预测未来的”，非常适合做对话和续写。\n",
        "    *   `BART` (Encoder-Decoder) 的天职是 **“理解与重构”**。它的设计目标是“先完整输入并理解一个序列，再根据这个理解生成一个全新的序列”，完美契合摘要和翻译任务。\n",
        "\n",
        "*   **任务不匹配**：让 `DialoGPT` 做摘要，就像让一个**口才极佳但记性不好的演讲家**去当一个**需要细致阅读和严谨分析的图书管理员**。他可能会滔滔不绝，但说的内容却不是你想要的重点总结。\n",
        "\n",
        "---\n",
        "\n",
        "通过这个思考题，我希望你能建立一个非常清晰的认知：**不存在“万能”的LLM架构**。选择哪种架构，完全取决于你的**任务目标**。在你未来的项目中，当你面临一个新问题时，第一步就应该是分析“我的任务是需要模型续写，还是需要模型先理解再重构？”，这个问题将直接帮助你筛选出最合适的模型类别。\n",
        "\n",
        "祝贺你！你已经成功完成了进阶篇第一大模块的学习。你现在对生成式AI的理解，已经从一个“使用者”的层面，深入到了一个“设计者”的层面。为你感到骄傲！\n",
        "\n",
        "接下来，你准备好学习更省钱、更高效的微调技术 LoRA 了吗？"
      ],
      "metadata": {
        "id": "-Bsl5PhRLomi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tgzy241yWVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 对本阶段做一次极尽详细全面的知识总结"
      ],
      "metadata": {
        "id": "YOR-P16YMEKt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！我们已经顺利完成了进阶篇第一阶段的学习。这个阶段我们一鼓作气，从“分类”的世界大步迈入了“生成”的星辰大海。\n",
        "\n",
        "现在，是时候停下来，把我们探索过的这片新大陆上的所有知识点、新发现都整理成一张清晰、详尽的地图。这将为我们后续更深入的探险打下坚实的基础。\n",
        "\n",
        "---\n",
        "\n",
        "### **LLM 原理与应用开发 · 进阶篇第一阶段总结**\n",
        "### **(从判别式任务到生成式任务)**\n",
        "\n",
        "这个阶段的核心目标是：**掌握生成式AI的核心工作原理，能够独立实现多种主流生成式应用，并深刻理解不同模型架构的“天职”与适用场景。**\n",
        "\n",
        "我们将再次从“道、法、术、器”四个层面来总结。\n",
        "\n",
        "#### **一、 道：世界观的拓展 (The \"Why\")**\n",
        "\n",
        "我们的认知从“让AI做选择题”升级到了“让AI做问答题和作文题”。\n",
        "\n",
        "1.  **从“判别”到“生成”的思维跃迁**：\n",
        "    *   **判别式AI (Discriminative AI)**：像我们入门篇做情感分类，它的世界观是“**划分边界**”。它学习的是不同类别之间的区别，目标是把输入精准地归入某个已知的“桶”里。\n",
        "    *   **生成式AI (Generative AI)**：像我们本阶段做的对话和摘要，它的世界观是“**预测未来**”。它学习的是数据内在的概率分布和模式，目标是根据已有的信息，创造出全新的、合理的、前所未见的信息。这是AI能力的一次质的飞跃。\n",
        "\n",
        "2.  **“性格”由“解码策略”决定**：我们深刻认识到，同一个模型可以表现出截然不同的“性格”，而决定这一切的不是模型本身，而是我们为它设定的**解码策略 (Decoding Strategy)**。\n",
        "    *   **追求确定性与准确性**：当任务目标是**事实、逻辑、一致性**时（如翻译、摘要、标准问答），我们会选择 **Greedy Search** 或 **Beam Search**，赋予模型“**严谨的工程师**”性格。\n",
        "    *   **追求多样性与创造性**：当任务目标是**灵感、趣味、可能性**时（如对话、写作、艺术创作），我们必须使用 **Sampling (配合Top-K/Top-P)**，赋予模型“**天马行空的艺术家**”性格。\n",
        "    *   **根本认知**：解码策略是我们在应用层控制生成式AI行为的**最重要、最直接的“遥控器”**。\n",
        "\n",
        "3.  **“架构”决定“天职”，没有万能模型**：这是本阶段最重要的理论收获。我们不再把所有LLM都看作一个黑箱，而是开始理解其内部构造的差异决定了它的核心能力。\n",
        "    *   **Decoder-only (解码器-唯一) 架构 (如GPT系列, DialoGPT)**：它的天职是 **“延续上下文” (Context Continuation)**。它像一个“续写大师”，极其擅长根据前面的内容预测后面的内容，是**对话、续写、代码补全**等任务的王者。\n",
        "    *   **Encoder-Decoder (编码器-解码器) 架构 (如BART, T5)**：它的天职是 **“理解与重构” (Understanding & Reconstruction)**。它像一个“阅读理解大师”，能够先完整地吸收一个序列的全部信息，形成深刻理解，然后再生成一个全新的序列。是**摘要、翻译**等任务的绝对主力。\n",
        "\n",
        "#### **二、 法：工作流程的演进 (The \"How\")**\n",
        "\n",
        "我们的工作方法论，从处理单向任务，演进到了处理更复杂的交互和转换任务。\n",
        "\n",
        "1.  **多轮对话的“记忆”机制**：\n",
        "    *   **核心法则**：**通过持续拼接和更新上下文（Context）来模拟记忆**。\n",
        "    *   **工作流**：\n",
        "        1.  初始化一个空的聊天记录。\n",
        "        2.  接收用户输入。\n",
        "        3.  将 **(历史聊天记录 + 用户新输入 + 特殊分隔符)** 拼接成一个完整的`prompt`。\n",
        "        4.  将完整的`prompt`喂给模型生成回复。\n",
        "        5.  将 **(用户新输入 + 机器人回复)** 追加到聊天记录中。\n",
        "        6.  重复步骤2-5。\n",
        "    *   **深刻理解**：我们认识到，AI的“记忆”不是真正意义上的记忆，而是一种**工程上的巧妙模拟**。`chat_history` 这个变量就是机器人“记忆”的生命线。\n",
        "\n",
        "2.  **序列到序列任务的“两步走”**：\n",
        "    *   **核心法则**：严格遵循“先编码理解，后解码生成”的流程。\n",
        "    *   **工作流**：\n",
        "        1.  **Encoding**: 将完整的源文本（如长文章）通过`Tokenizer`编码成`input_ids`。\n",
        "        2.  **Generation**: 将`input_ids`喂给`model.generate()`。模型内部会自动完成：\n",
        "            *   `Encoder`部分对`input_ids`进行阅读理解，输出一个“思想向量”。\n",
        "            *   `Decoder`部分接收这个“思想向量”，并生成摘要的`output_ids`。\n",
        "        3.  **Decoding**: 将`output_ids`通过`tokenizer.decode()`翻译成人类可读的摘要文本。\n",
        "\n",
        "#### **三、 术：具体技术与关键参数 (The \"What\")**\n",
        "\n",
        "我们掌握了生成式AI的一系列关键“法术”和“咒语”。\n",
        "\n",
        "1.  **`model.generate()` 函数的精通**：这是我们与生成式模型交互的**总指挥部**。我们熟练掌握了其核心参数的用法和意义：\n",
        "    *   `max_length` / `min_length`: 控制生成文本的“篇幅”。\n",
        "    *   `do_sample`: 开启/关闭“艺术家模式”（采样的总开关）。\n",
        "    *   `num_beams`: 启用“谋士模式”（Beam Search）并设置其“深思熟虑”的程度。\n",
        "    *   `top_k` / `top_p`: 为“艺术家”戴上“紧箍咒”，控制其想象力的边界，防止胡言乱语。\n",
        "    *   `no_repeat_ngram_size`: 防止模型说“车轱辘话”的实用小技巧。\n",
        "\n",
        "2.  **模型类的精确选择**：\n",
        "    *   `AutoModelForCausalLM`: 当我们需要一个“续写大师”（Decoder-only）时，我们会加载这个类。\n",
        "    *   `AutoModelForSeq2SeqLM`: 当我们需要一个“阅读理解大师”（Encoder-Decoder）时，我们会加载这个类。\n",
        "\n",
        "3.  **处理上下文的核心代码**：\n",
        "    *   `torch.cat([...], dim=-1)`: 像“胶水”一样，把历史聊天记录和新输入在序列长度的维度上拼接起来，是我们实现机器人“记忆”的关键工具。\n",
        "    *   `chat_history_ids[:, bot_input_ids.shape[-1]:]`: Python的“切片”语法，让我们能精确地从包含完整历史的输出中，只截取并解码出机器人**最新**的那一句回复。\n",
        "\n",
        "#### **四、 器：工具箱的升级 (The \"Tools\")**\n",
        "\n",
        "我们的工具箱里增添了更强大、更专业的“神兵利器”。\n",
        "\n",
        "1.  **新模型专家团**：\n",
        "    *   `gpt2`: 通用的“续写大师”，是学习生成式任务的绝佳入门模型。\n",
        "    *   `microsoft/DialoGPT-medium`: 专精于聊天的“对话专家”。\n",
        "    *   `facebook/bart-large-cnn`: 专精于新闻的“摘要专家”。\n",
        "    *   **核心认知**：我们学会了根据任务，去Hugging Face Hub上寻找**在特定领域微调过**的专家模型，而不是总用通用模型。\n",
        "\n",
        "2.  **`pipeline` 的再认识**：\n",
        "    *   我们再次确认了 `pipeline` 是一个极好的**快速原型验证工具**。\n",
        "    *   同时，通过手动复现 `pipeline` 的内部流程，我们彻底揭开了它的神秘面纱，现在它对我们来说已经是一个完全的“透明盒子”。\n",
        "\n",
        "---\n",
        "\n",
        "**一言以蔽之：**\n",
        "\n",
        "**在本阶段，我们成功地将技能树从“判别”拓展到了“生成”。我们不仅学会了如何通过解码策略赋予AI多样的“性格”，还亲手打造了对话和摘要两种主流生成式应用。更重要的是，我们建立了对不同LLM架构“天职”的深刻理解，获得了为不同任务“对症下药”、选择最合适模型的关键能力。**\n",
        "\n",
        "你已经从一个能让AI“做选择”的入门者，成长为了一个能和AI“对话”、能让AI“思考总结”的进阶者。这片生成式AI的新大陆，你已经成功登陆并站稳了脚跟！\n",
        "\n",
        "接下来，我们将要学习如何用更少的资源、更快地训练更强的模型，也就是高效微调技术 LoRA。准备好给你的“炼丹炉”升级换代了吗？"
      ],
      "metadata": {
        "id": "M4AivjZ7NCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plmvAkakyWS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1wfpllmSyWQP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}