{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIkihkhIj360"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，我们现在开启一个全新的 Colab 笔记本，踏上旅程中最激动人心的一步！\n",
        "\n",
        "在第一、二步中，我们学会了如何“使用”一个别人已经训练好的模型，就像我们直接聘请了一位已经功成名就的专家。\n",
        "\n",
        "但在第三步，我们的角色将发生转变：**我们要做“老师”，亲自“训练”和“教育”一个模型，把它从一个“通才”培养成我们特定任务的“专才”**。这个过程，就叫做 **Fine-tuning（微调）**。\n",
        "\n",
        "---\n",
        "\n",
        "### **第三步：动手“再教育”模型 —— Fine-tuning 微调入门 (预计用时: 1-2 小时，大部分为机器运行时间)**\n",
        "\n",
        "#### **1. 理论学习：什么是微调？为什么要微调？**\n",
        "\n",
        "想象一下，`distilbert-base-uncased` 这个模型是一位刚从名牌大学毕业的通才博士。\n",
        "\n",
        "*   **他是“通才” (Pre-trained Model)**：他读了成千上万的书（在海量互联网数据上做过预训练），所以他对英语的语法、词汇、常识都有了深刻的理解。\n",
        "*   **但他不是“专才”**：你现在想让他去做“电影评论情感分析”这份非常具体的工作。虽然他凭借通用知识也能做个大概，但可能不够专业。比如，他可能不知道 “plot twist”（情节反转）或者 “cinematography”（摄影技巧）这些词在电影评论里通常意味着什么。\n",
        "\n",
        "**微调（Fine-tuning）就是要做的“岗前培训”**：\n",
        "\n",
        "我们拿一本专门的教材——**“电影评论数据集”**，里面有成千上万条电影评论和它们对应的情感标签（好评/差评）。我们让这位“通才博士”把这本教材从头到尾学习一遍（甚至好几遍）。\n",
        "\n",
        "在这个过程中，他会把他脑子里已有的庞大通用知识，**“微调”和“适配”**到我们这个特定的任务上。学习完成后，他就从一个“通才博士”变成了“电影评论情感分析专家”，再去做这个工作时，就会更精准、更专业。\n",
        "\n",
        "**微调的优势：**\n",
        "\n",
        "*   **效果好**：在特定任务上，通常比直接用通用模型效果更好。\n",
        "*   **成本低**：相比于从零开始训练一个模型（需要海量数据和数百万美元的计算资源），微调只需要在特定的小数据集上进行，时间和金钱成本都大大降低。我们用 Colab 的免费 GPU 就能完成！\n",
        "\n",
        "#### **2. 编程实践：微调一个模型用于电影评论情感分析**\n",
        "\n",
        "现在，让我们开始扮演“老师”的角色。请在你的新 Colab 笔记本中，把硬件加速器设置为 `T4 GPU`，然后运行下面的代码。\n",
        "\n",
        "**注意：** 第一次运行会下载数据集和模型，根据网络情况可能需要几分钟。之后的训练过程也会持续一段时间（大约10-20分钟），你可以看到一个进度条，这是完全正常的。正好可以利用这段时间去泡杯茶，并仔细阅读代码注释，理解每一步的作用。\n",
        "\n",
        "**实践代码（请完整复制到 Colab 单元格中运行）：**\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备 - 安装所有需要的库\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 除了 'transformers' 和 'datasets'，我们还需要安装 'accelerate'。\n",
        "# 'accelerate' 是Hugging Face开发的另一个库，它可以帮助我们更轻松地在GPU上进行训练，\n",
        "# 即使我们只有一块GPU，它也能帮助优化代码。\n",
        "# 'evaluate' 库包含了各种评估指标的实现，比如准确率。\n",
        "!pip install transformers datasets accelerate evaluate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备“教材” - 加载数据集\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 从 'datasets' 库中导入 'load_dataset' 函数。\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 我们使用 'load_dataset' 函数来从Hugging Face Hub上下载数据集。\n",
        "# 'imdb' 是一个非常经典的电影评论情感分析数据集。\n",
        "# 它包含两个部分：'train' (训练集，用来给模型学习) 和 'test' (测试集，用来在模型学完后考查它)。\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 我们来看一下数据集的结构。\n",
        "print(\"数据集结构预览:\")\n",
        "print(imdb_dataset)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备“翻译官”和一位“通才博士” - 加载Tokenizer和模型\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 这次我们选择的模型是 \"distilbert-base-uncased\"。\n",
        "# 注意！这次的模型名字后面没有 \"-finetuned-...\" 之类的后缀。\n",
        "# 这意味着我们加载的是一个只经过“预训练”的通用基础模型，它还没有在任何情感分析任务上被微调过。\n",
        "# 我们就是要亲自来完成微调这个步骤。\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 根据模型名字，加载对应的“翻译官”。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 根据模型名字，加载对应的模型。\n",
        "# - AutoModelForSequenceClassification: 我们仍然是做分类任务。\n",
        "# - num_labels=2: 我们需要明确告诉模型，我们的任务最终要分成几类。对于IMDB情感分析，就是2类（消极/积极）。\n",
        "# - id2label 和 label2id: 为了方便后续解读，我们创建一个ID到标签和标签到ID的映射字典。\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 批量处理“教材” - 对整个数据集进行Tokenization\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们需要写一个函数，这个函数的作用是接收一条数据，然后用tokenizer把它处理成模型需要的格式。\n",
        "def preprocess_function(examples):\n",
        "    # 'examples[\"text\"]' 就是数据集中的每一条电影评论文本。\n",
        "    # 'truncation=True' 表示如果文本太长，就把它截断到模型能接受的最大长度。\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# '.map()' 是 'datasets' 库里一个非常强大的功能。\n",
        "# 它可以把我们刚刚定义的 'preprocess_function' 函数，应用到数据集的每一条数据上。\n",
        "# 'batched=True' 表示一次性给函数一批数据进行处理，这样速度会快很多。\n",
        "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 设置“教学大纲” - 定义训练参数\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 'TrainingArguments' 类可以让我们详细地设置训练过程中的各种参数。\n",
        "# 就像老师制定教学计划一样。\n",
        "training_args = TrainingArguments(\n",
        "    # 'output_dir' : 训练过程中产生的所有文件（比如模型的存档）都将保存在这个文件夹里。\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    # 'learning_rate' : 学习率，可以理解为模型在学习时每一步“迈出去”的步子大小。\n",
        "    learning_rate=2e-5,\n",
        "    # 'per_device_train_batch_size' : 训练时，每次给GPU多少条数据进行学习。\n",
        "    per_device_train_batch_size=16,\n",
        "    # 'per_device_eval_batch_size' : 评估时，每次给GPU多少条数据。\n",
        "    per_device_eval_batch_size=16,\n",
        "    # 'num_train_epochs' : 把整个训练数据集从头到尾学习几遍。\n",
        "    num_train_epochs=2,\n",
        "    # 'weight_decay' : 一种防止模型“死记硬背”（过拟合）的技术。\n",
        "    weight_decay=0.01,\n",
        "    # 'evaluation_strategy' : 设置评估策略。'epoch'表示每学完一遍数据集，就进行一次“期末考试”。\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    # 'save_strategy' : 设置模型保存策略，'epoch'表示每考完一次试，就保存一次模型。\n",
        "    save_strategy=\"epoch\",\n",
        "    # 'load_best_model_at_end' : 训练结束后，自动把考试成绩最好的那次保存的模型加载回来。\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 聘请“金牌私教” - 创建Trainer对象\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import Trainer\n",
        "\n",
        "# 'Trainer' 是Hugging Face提供的一个超级方便的类，它把复杂的训练循环都封装好了。\n",
        "# 我们只需要把“学生”、“教学大纲”、“训练教材”、“考试试卷”都告诉它就行。\n",
        "trainer = Trainer(\n",
        "    model=model,                         # 我们要训练的模型 (学生)\n",
        "    args=training_args,                  # 训练参数 (教学大纲)\n",
        "    train_dataset=tokenized_imdb[\"train\"], # 训练数据集 (训练教材)\n",
        "    eval_dataset=tokenized_imdb[\"test\"],   # 评估数据集 (考试试卷)\n",
        "    tokenizer=tokenizer,                 # 我们使用的翻译官 (方便处理数据)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤7: 开始上课！- 启动训练\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 一切准备就绪，我们只需要调用 .train() 方法，训练就会自动开始！\n",
        "# Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\n",
        "print(\"训练即将开始...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤8: 检验学习成果 - 使用微调后的模型进行预测\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 训练完成后，我们的 'trainer' 对象里就包含了微调好的、最优秀的模型。\n",
        "# 我们可以直接用它来做一个预测。\n",
        "# 准备一条新的电影评论，这个评论模型在训练时从未见过。\n",
        "my_review = \"This movie was fantastic! The acting was superb and the plot was gripping.\"\n",
        "\n",
        "# 导入pipeline，用我们刚刚训练好的模型来快速创建一个情感分析管道。\n",
        "from transformers import pipeline\n",
        "\n",
        "# 'model=trainer.model' 表示我们用trainer里那个训练好的模型。\n",
        "# 'tokenizer=trainer.tokenizer' 同样使用配套的tokenizer。\n",
        "# 'device=0' 表示把这个管道放到GPU的0号卡上运行，速度会更快。\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)\n",
        "\n",
        "# 见证奇迹的时刻！\n",
        "result = sentiment_pipeline(my_review)\n",
        "print(\"\\n用我们自己微调的模型进行预测:\")\n",
        "print(f\"评论: '{my_review}'\")\n",
        "print(f\"预测结果: {result}\")\n",
        "\n",
        "# 再试一条负面的\n",
        "my_review_neg = \"It was a complete waste of time. The story was predictable and boring.\"\n",
        "result_neg = sentiment_pipeline(my_review_neg)\n",
        "print(f\"\\n评论: '{my_review_neg}'\")\n",
        "print(f\"预测结果: {result_neg}\")\n",
        "```\n",
        "\n",
        "#### **3. 结果解读与总结**\n",
        "\n",
        "当训练开始后，你会看到类似这样的输出日志：\n",
        "\n",
        "```\n",
        "Epoch | Training Loss | Validation Loss | ...\n",
        "...\n",
        "1     | 0.3...        | 0.2...          | ...\n",
        "...\n",
        "2     | 0.2...        | 0.2...          | ...\n",
        "```\n",
        "\n",
        "*   **Epoch**: 代表第几遍学习整个数据集。\n",
        "*   **Training Loss**: “随堂测验”的错误率。这个值在训练过程中应该会不断下降，代表模型正在有效地从数据中学习。\n",
        "*   **Validation Loss**: “期末考试”的错误率。这个值也在下降，说明我们的模型不仅是记住了训练题，还具备了解决新问题的“举一反三”的能力。\n",
        "\n",
        "训练全部结束后，最后几行会打印出用我们**亲手微调好**的模型进行预测的结果，你会发现它已经能非常准确地判断新评论的情感了！\n",
        "\n",
        "---\n",
        "\n",
        "**第三步总结与作业：**\n",
        "\n",
        "1.  **成功运行！**：你已经成功地完成了从数据准备、模型加载、参数设置到启动训练和最终验证的全过程。你不再只是一个模型的使用者，你已经是一位模型的“训练师”了！\n",
        "2.  **修改输入：** 在最后一步的预测中，尝试输入你自己写的、更复杂的电影评论（可以说一些反话，或者带有复杂情绪的话），看看你训练的这个模型能否准确判断。\n",
        "3.  **思考题 (调参侠入门)：** 在 `TrainingArguments` 中，有一个参数 `num_train_epochs=2`，我们设置的是学习2遍。你觉得如果把它改成 `1` 会发生什么？如果改成 `3` 呢？（提示：可以思考一下学习时间、模型效果可能会有什么变化？这其实就是AI领域最核心的工作之一：“调参”的初步体验）。\n",
        "\n",
        "你已经掌握了LLM应用开发中“皇冠上的明珠”——Fine-tuning技术。这让你具备了解决各种定制化文本任务的能力。\n",
        "\n",
        "当你成功运行完代码，并对整个流程有了自己的理解后，请随时告诉我。我们可以讨论一下你的思考题，或者进入下一个阶段：如何将你训练好的模型保存、分享，并在其他地方加载使用。加油！"
      ],
      "metadata": {
        "id": "HiHSDTL4mr8i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EV5ZSHrqlEb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 2.9 MB/s eta 0:00:00\n",
        "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:\n",
        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
        "You will be able to reuse this secret in all of your notebooks.\n",
        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
        "  warnings.warn(\n",
        "README.md:\n",
        " 7.81k/? [00:00<00:00, 161kB/s]\n",
        "plain_text/train-00000-of-00001.parquet: 100%\n",
        " 21.0M/21.0M [00:00<00:00, 26.3MB/s]\n",
        "plain_text/test-00000-of-00001.parquet: 100%\n",
        " 20.5M/20.5M [00:00<00:00, 51.2MB/s]\n",
        "plain_text/unsupervised-00000-of-00001.p(…): 100%\n",
        " 42.0M/42.0M [00:00<00:00, 26.2MB/s]\n",
        "Generating train split: 100%\n",
        " 25000/25000 [00:00<00:00, 74089.16 examples/s]\n",
        "Generating test split: 100%\n",
        " 25000/25000 [00:00<00:00, 74630.15 examples/s]\n",
        "Generating unsupervised split: 100%\n",
        " 50000/50000 [00:00<00:00, 72477.06 examples/s]\n",
        "数据集结构预览:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    unsupervised: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 50000\n",
        "    })\n",
        "})\n",
        "tokenizer_config.json: 100%\n",
        " 48.0/48.0 [00:00<00:00, 821B/s]\n",
        "config.json: 100%\n",
        " 483/483 [00:00<00:00, 11.6kB/s]\n",
        "vocab.txt: 100%\n",
        " 232k/232k [00:00<00:00, 2.26MB/s]\n",
        "tokenizer.json: 100%\n",
        " 466k/466k [00:00<00:00, 3.71MB/s]\n",
        "model.safetensors: 100%\n",
        " 268M/268M [00:02<00:00, 131MB/s]\n",
        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Map: 100%\n",
        " 25000/25000 [00:25<00:00, 983.23 examples/s]\n",
        "Map: 100%\n",
        " 25000/25000 [00:25<00:00, 1028.41 examples/s]\n",
        "Map: 100%\n",
        " 50000/50000 [00:52<00:00, 996.55 examples/s]\n",
        "---------------------------------------------------------------------------\n",
        "TypeError                                 Traceback (most recent call last)\n",
        "/tmp/ipython-input-2284890269.py in <cell line: 0>()\n",
        "     68 # 'TrainingArguments' 类可以让我们详细地设置训练过程中的各种参数。\n",
        "     69 # 就像老师制定教学计划一样。\n",
        "---> 70 training_args = TrainingArguments(\n",
        "     71     # 'output_dir' : 训练过程中产生的所有文件（比如模型的存档）都将保存在这个文件夹里。\n",
        "     72     output_dir=\"my_awesome_model\",\n",
        "\n",
        "TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n",
        "\n",
        "\n",
        "分析报错信息"
      ],
      "metadata": {
        "id": "2kklUcriJOZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there's a TypeError because the TrainingArguments class in the installed version of the transformers library doesn't recognize the evaluation_strategy argument. This might be due to a version mismatch or a change in the API. I will change evaluation_strategy to eval_strategy to fix this issue.\n",
        "\n",
        "I see you have a !pip install --upgrade command in the cell to update the libraries, but it seems the runtime wasn't restarted after the installation. It's important to restart the runtime after installing or upgrading packages to ensure the new versions are loaded correctly.\n",
        "\n",
        "I'll go ahead and fix the argument name in the code. Please remember to restart the session after running the pip install command and then re-run the code cell.\n",
        "\n",
        "I've corrected the argument name in the TrainingArguments based on the error message. It seems the new error is related to wandb. If you encounter issues with wandb logging interrupting the training process, you can disable it by adding report_to=\"none\" to the TrainingArguments.\n",
        "\n",
        "As for the next steps in your tutorial, you can now focus on:\n",
        "\n",
        "Successfully running the cell: Ensure you restart the Colab session after the pip install --upgrade command and then re-run the code cell to allow the training process to complete without the TypeError.\n",
        "Modifying the input: Experiment with your own movie reviews in the final prediction step.\n",
        "Thinking about hyperparameters: Consider the effect of changing num_train_epochs as suggested in your markdown.\n",
        "Let me know if you'd like me to add the report_to=\"none\" argument to the code cell.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备 - 强制升级所有需要的库到最新版本\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 使用 --upgrade 参数来确保我们使用的是最新版本的库，这样就能识别所有新的参数。\n",
        "!pip install --upgrade transformers datasets accelerate evaluate\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# !! 重要提示 !!\n",
        "# !! 运行完上面的pip install后，如果看到提示需要\"RESTART SESSION\"，请务必手动重启。!!\n",
        "# !! 操作：点击菜单栏的 [代码执行程序] -> [重启会话]                               !!\n",
        "# !! 重启后，再从头运行一遍这个代码单元格。                                     !!\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备“教材” - 加载数据集\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 从 'datasets' 库中导入 'load_dataset' 函数。\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 我们使用 'load_dataset' 函数来从Hugging Face Hub上下载数据集。\n",
        "# 'imdb' 是一个非常经典的电影评论情感分析数据集。\n",
        "# 它包含两个部分：'train' (训练集，用来给模型学习) 和 'test' (用来在模型学完后考查它)。\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 我们来看一下数据集的结构。\n",
        "print(\"数据集结构预览:\")\n",
        "print(imdb_dataset)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备“翻译官”和一位“通才博士” - 加载Tokenizer和模型\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 这次我们选择的模型是 \"distilbert-base-uncased\"。\n",
        "# 注意！这次的模型名字后面没有 \"-finetuned-...\" 之类的后缀。\n",
        "# 这意味着我们加载的是一个只经过“预训练”的通用基础模型，它还没有在任何情感分析任务上被微调过。\n",
        "# 我们就是要亲自来完成微调这个步骤。\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 根据模型名字，加载对应的“翻译官”。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 根据模型名字，加载对应的模型。\n",
        "# - AutoModelForSequenceClassification: 我们仍然是做分类任务。\n",
        "# - num_labels=2: 我们需要明确告诉模型，我们的任务最终要分成几类。对于IMDB情感分析，就是2类（消极/积极）。\n",
        "# - id2label 和 label2id: 为了方便后续解读，我们创建一个ID到标签和标签到ID的映射字典。\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 批量处理“教材” - 对整个数据集进行Tokenization\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们需要写一个函数，这个函数的作用是接收一条数据，然后用tokenizer把它处理成模型需要的格式。\n",
        "def preprocess_function(examples):\n",
        "    # 'examples[\"text\"]' 就是数据集中的每一条电影评论文本。\n",
        "    # 'truncation=True' 表示如果文本太长，就把它截断到模型能接受的最大长度。\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# '.map()' 是 'datasets' 库里一个非常强大的功能。\n",
        "# 它可以把我们刚刚定义的 'preprocess_function' 函数，应用到数据集的每一条数据上。\n",
        "# 'batched=True' 表示一次性给函数一批数据进行处理，这样速度会快很多。\n",
        "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 设置“教学大纲” - 定义训练参数\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 'TrainingArguments' 类可以让我们详细地设置训练过程中的各种参数。\n",
        "# 就像老师制定教学计划一样。\n",
        "training_args = TrainingArguments(\n",
        "    # 'output_dir' : 训练过程中产生的所有文件（比如模型的存档）都将保存在这个文件夹里。\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    # 'learning_rate' : 学习率，可以理解为模型在学习时每一步“迈出去”的步子大小。\n",
        "    learning_rate=2e-5,\n",
        "    # 'per_device_train_batch_size' : 训练时，每次给GPU多少条数据进行学习。\n",
        "    per_device_train_batch_size=16,\n",
        "    # 'per_device_eval_batch_size' : 评估时，每次给GPU多少条数据。\n",
        "    per_device_eval_batch_size=16,\n",
        "    # 'num_train_epochs' : 把整个训练数据集从头到尾学习几遍。\n",
        "    num_train_epochs=2,\n",
        "    # 'weight_decay' : 一种防止模型“死记硬背”（过拟合）的技术。\n",
        "    weight_decay=0.01,\n",
        "    # 'eval_strategy' : 设置评估策略。'epoch'表示每学完一遍数据集，就进行一次“期末考试”。\n",
        "    eval_strategy=\"epoch\",\n",
        "    # 'save_strategy' : 设置模型保存策略，'epoch'表示每考完一次试，就保存一次模型。\n",
        "    save_strategy=\"epoch\",\n",
        "    # 'load_best_model_at_end' : 训练结束后，自动把考试成绩最好的那次保存的模型加载回来。\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 聘请“金牌私教” - 创建Trainer对象\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import Trainer\n",
        "\n",
        "# 'Trainer' 是Hugging Face提供的一个超级方便的类，它把复杂的训练循环都封装好了。\n",
        "# 我们只需要把“学生”、“教学大纲”、“训练教材”、“考试试卷”都告诉它就行。\n",
        "trainer = Trainer(\n",
        "    model=model,                         # 我们要训练的模型 (学生)\n",
        "    args=training_args,                  # 训练参数 (教学大纲)\n",
        "    train_dataset=tokenized_imdb[\"train\"], # 训练数据集 (训练教材)\n",
        "    eval_dataset=tokenized_imdb[\"test\"],   # 评估数据集 (考试试卷)\n",
        "    tokenizer=tokenizer,                 # 我们使用的翻译官 (方便处理数据)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤7: 开始上课！- 启动训练\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 一切准备就绪，我们只需要调用 .train() 方法，训练就会自动开始！\n",
        "# Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\n",
        "print(\"训练即将开始...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤8: 检验学习成果 - 使用微调后的模型进行预测\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 训练完成后，我们的 'trainer' 对象里就包含了微调好的、最优秀的模型。\n",
        "# 我们可以直接用它来做一个预测。\n",
        "# 准备一条新的电影评论，这个评论模型在训练时从未见过。\n",
        "my_review = \"This movie was fantastic! The acting was superb and the plot was gripping.\"\n",
        "\n",
        "# 导入pipeline，用我们刚刚训练好的模型来快速创建一个情感分析管道。\n",
        "from transformers import pipeline\n",
        "\n",
        "# 'model=trainer.model' 表示我们用trainer里那个训练好的模型。\n",
        "# 'tokenizer=trainer.tokenizer' 同样使用配套的tokenizer。\n",
        "# 'device=0' 表示把这个管道放到GPU的0号卡上运行，速度会更快。\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)\n",
        "\n",
        "# 见证奇迹的时刻！\n",
        "result = sentiment_pipeline(my_review)\n",
        "print(\"\\n用我们自己微调的模型进行预测:\")\n",
        "print(f\"评论: '{my_review}'\")\n",
        "print(f\"预测结果: {result}\")\n",
        "\n",
        "# 再试一条负面的\n",
        "my_review_neg = \"It was a complete waste of time. The story was predictable and boring.\"\n",
        "result_neg = sentiment_pipeline(my_review_neg)\n",
        "print(f\"\\n评论: '{my_review_neg}'\")\n",
        "print(f\"预测结果: {result_neg}\")\n",
        "```\n",
        "\n",
        "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
        "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
        "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
        "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
        "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
        "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
        "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
        "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
        "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
        "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
        "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
        "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
        "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
        "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
        "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
        "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
        "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
        "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
        "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
        "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
        "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
        "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
        "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
        "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
        "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
        "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
        "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
        "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
        "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
        "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
        "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
        "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
        "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
        "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
        "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
        "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
        "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
        "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
        "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
        "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
        "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
        "数据集结构预览:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    unsupervised: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 50000\n",
        "    })\n",
        "})\n",
        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Map: 100%\n",
        " 25000/25000 [00:32<00:00, 432.66 examples/s]\n",
        "Map: 100%\n",
        " 25000/25000 [00:26<00:00, 971.14 examples/s]\n",
        "Map: 100%\n",
        " 50000/50000 [00:52<00:00, 981.27 examples/s]\n",
        "/tmp/ipython-input-2282751338.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
        "  trainer = Trainer(\n",
        "训练即将开始...\n",
        "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
        "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
        "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
        "wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
        "wandb: Paste an API key from your profile and hit enter:\n",
        "---------------------------------------------------------------------------\n",
        "Abort                                     Traceback (most recent call last)\n",
        "/tmp/ipython-input-2282751338.py in <cell line: 0>()\n",
        "    114 # Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\n",
        "    115 print(\"训练即将开始...\")\n",
        "--> 116 trainer.train()\n",
        "    117 print(\"训练完成！\")\n",
        "    118\n",
        "\n",
        "15 frames\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
        "   2326                 hf_hub_utils.enable_progress_bars()\n",
        "   2327         else:\n",
        "-> 2328             return inner_training_loop(\n",
        "   2329                 args=args,\n",
        "   2330                 resume_from_checkpoint=resume_from_checkpoint,\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/trainer.py in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
        "   2576         grad_norm: Optional[float] = None\n",
        "   2577         learning_rate = None\n",
        "-> 2578         self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n",
        "   2579\n",
        "   2580         if args.eval_on_start:\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py in on_train_begin(self, args, state, control)\n",
        "    504     def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n",
        "    505         control.should_training_stop = False\n",
        "--> 506         return self.call_event(\"on_train_begin\", args, state, control)\n",
        "    507\n",
        "    508     def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py in call_event(self, event, args, state, control, **kwargs)\n",
        "    554     def call_event(self, event, args, state, control, **kwargs):\n",
        "    555         for callback in self.callbacks:\n",
        "--> 556             result = getattr(callback, event)(\n",
        "    557                 args,\n",
        "    558                 state,\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py in on_train_begin(self, args, state, control, model, **kwargs)\n",
        "    966             args.run_name = None\n",
        "    967         if not self._initialized:\n",
        "--> 968             self.setup(args, state, model, **kwargs)\n",
        "    969\n",
        "    970     def on_train_end(self, args: TrainingArguments, state, control, model=None, processing_class=None, **kwargs):\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py in setup(self, args, state, model, **kwargs)\n",
        "    893\n",
        "    894             if self._wandb.run is None:\n",
        "--> 895                 self._wandb.init(\n",
        "    896                     project=os.getenv(\"WANDB_PROJECT\", \"huggingface\"),\n",
        "    897                     **init_args,\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py in init(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\n",
        "   1579         # Need to build delay into this sentry capture because our exit hooks\n",
        "   1580         # mess with sentry's ability to send out errors before the program ends.\n",
        "-> 1581         wandb._sentry.reraise(e)\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/analytics/sentry.py in reraise(self, exc)\n",
        "    160         # this will messily add this \"reraise\" function to the stack trace,\n",
        "    161         # but hopefully it's not too bad\n",
        "--> 162         raise exc.with_traceback(sys.exc_info()[2])\n",
        "    163\n",
        "    164     @_safe_noop\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py in init(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\n",
        "   1502         wi = _WandbInit(wl, init_telemetry)\n",
        "   1503\n",
        "-> 1504         wi.maybe_login(init_settings)\n",
        "   1505         run_settings, show_warnings = wi.make_run_settings(init_settings)\n",
        "   1506\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py in maybe_login(self, init_settings)\n",
        "    188             return\n",
        "    189\n",
        "--> 190         wandb_login._login(\n",
        "    191             anonymous=run_settings.anonymous,\n",
        "    192             host=run_settings.base_url,\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py in _login(anonymous, key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent, _disable_warning)\n",
        "    316             key_is_pre_configured = True\n",
        "    317         else:\n",
        "--> 318             key, key_status = wlogin.prompt_api_key(referrer=referrer)\n",
        "    319\n",
        "    320     if verify:\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py in prompt_api_key(self, referrer)\n",
        "    235     ) -> Tuple[Optional[str], ApiKeyStatus]:\n",
        "    236         \"\"\"Updates the global API key by prompting the user.\"\"\"\n",
        "--> 237         key, status = self._prompt_api_key(referrer)\n",
        "    238         if status == ApiKeyStatus.NOTTY:\n",
        "    239             directive = (\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py in _prompt_api_key(self, referrer)\n",
        "    211         while True:\n",
        "    212             try:\n",
        "--> 213                 key = apikey.prompt_api_key(\n",
        "    214                     self._settings,\n",
        "    215                     api=api,\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/apikey.py in prompt_api_key(settings, api, input_callback, browser_callback, no_offline, no_create, local, referrer)\n",
        "    190                 )\n",
        "    191             wandb.termlog(_api_key_prompt_str(app_url, referrer))\n",
        "--> 192             key = input_callback(api_ask).strip()\n",
        "    193     elif result == LOGIN_CHOICE_NOTTY:\n",
        "    194         # TODO: Needs refactor as this needs to be handled by caller\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/click/termui.py in prompt(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\n",
        "    166     while True:\n",
        "    167         while True:\n",
        "--> 168             value = prompt_func(prompt)\n",
        "    169             if value:\n",
        "    170                 break\n",
        "\n",
        "/usr/local/lib/python3.12/dist-packages/click/termui.py in prompt_func(text)\n",
        "    149             if hide_input:\n",
        "    150                 echo(None, err=err)\n",
        "--> 151             raise Abort() from None\n",
        "    152\n",
        "    153     if value_proc is None:\n",
        "\n",
        "Abort:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "分析本次修改后遇到的问题\n"
      ],
      "metadata": {
        "id": "4lvSJy7wJOQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的同学，我们再次化身“侦探”，来分析这次遇到的新情况。你做得非常好，不仅运行了代码，还把完整的输出日志都贴了出来，这为我们解决问题提供了最关键的线索。\n",
        "\n",
        "这次的情况比上次稍微复杂一点，我们一步步来拆解。\n",
        "\n",
        "### **1. 第一次报错分析与你的正确诊断（`TypeError`）**\n",
        "\n",
        "*   **原始问题**：`TypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'`\n",
        "*   **你的分析**：你准确地判断出这是因为 `transformers` 库的版本问题，导致 `TrainingArguments` 类不认识 `evaluation_strategy` 这个参数名，并推断出可能的新名称是 `eval_strategy`。**这个判断是完全正确的！** 早期版本的 `transformers` 库使用的就是 `evaluation_strategy`，后来为了统一和简化API，改成了 `eval_strategy`。\n",
        "*   **你的解决方案**：你提供了修正后的代码，将参数名修改正确。这是一个非常标准的debug流程，堪称典范！\n",
        "\n",
        "### **2. 第二次报错分析（`Abort` 和 `wandb`）**\n",
        "\n",
        "在你修正了第一个问题并重新运行后，我们遇到了一个新的、看起来完全不同的问题。\n",
        "\n",
        "#### **定位关键线索**\n",
        "\n",
        "这次的报错日志很长，但我们往上翻，找到**问题开始的地方**：\n",
        "\n",
        "```\n",
        "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
        "wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
        "wandb: Paste an API key from your profile and hit enter:\n",
        "---------------------------------------------------------------------------\n",
        "Abort                                     Traceback (most recent call last)\n",
        "...\n",
        "```\n",
        "\n",
        "#### **解读线索**\n",
        "\n",
        "1.  **`wandb` 是什么？**\n",
        "    *   `wandb` 是 \"Weights & Biases\" 的缩写。它是一个第三方工具，专门用来**记录和可视化**机器学习实验的过程。你可以把它想象成一个非常高级的实验记录本，能帮你画出 `Loss` 下降的曲线图，记录你每次实验用的参数等等。\n",
        "    *   新版本的 `transformers` 的 `Trainer` 默认集成了这个功能。当它检测到 `wandb` 库被安装时，就会**自动尝试启动它**。\n",
        "\n",
        "2.  **它想做什么？**\n",
        "    *   日志清晰地显示，`wandb` 正在尝试**登录（Logging into wandb.ai）**。\n",
        "    *   它甚至给出了一个网址，并提示你 **`Paste an API key ... and hit enter:`** (请粘贴你的API密钥并按回车)。\n",
        "\n",
        "3.  **为什么会 `Abort` (中断)？**\n",
        "    *   在 Google Colab 这种非交互式的笔记本环境中，当一个程序暂停下来，等待用户从键盘输入信息（比如让你粘贴API key）时，如果长时间没有得到输入，或者环境本身就不支持这种“命令行式”的交互，它就会自动**中断（Abort）**这个操作，以防止程序卡死。\n",
        "    *   所以，根本原因不是训练代码本身有错，而是 `Trainer` 启动了一个我们没预料到的“附属程序”(`wandb`)，而这个附属程序需要我们进行一个交互式的登录操作，但在Colab里这个操作失败了，导致整个程序被中断。\n",
        "\n",
        "### **3. 解决问题**\n",
        "\n",
        "既然我们知道了问题是 `wandb` 引起的，而且我们当前的学习目标只是微调模型本身，并不需要一个花哨的实验记录工具，那么最直接的解决方案就是：**明确告诉 `Trainer`，请不要启动 `wandb`。**\n",
        "\n",
        "怎么告诉它呢？这就要用到你之前分析中提到的另一个关键参数了！\n",
        "\n",
        "在 `TrainingArguments` 中，有一个参数专门负责控制把训练日志报告给谁。\n",
        "\n",
        "**解决方案**：在 `TrainingArguments` 的参数列表里，添加一行 `report_to=\"none\"`。\n",
        "\n",
        "### **完整修正后的代码**\n",
        "\n",
        "下面是经过两次修正后的最终版本。我只修改了 `TrainingArguments` 部分，并在关键位置增加了注释。请用这个版本替换你 Colab 中的代码，然后**从头完整运行一次**。这次应该就能顺利跑通了！\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备 - 安装所有需要的库\n",
        "# ----------------------------------------------------------------------------------\n",
        "!pip install transformers datasets accelerate evaluate -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2 到 步骤4: (这部分代码完全不变，保持原样即可)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 设置“教学大纲” - 定义训练参数 (包含两次修正)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # --- 第一次修正 ---\n",
        "    # 旧的参数名是 evaluation_strategy，在新版库中已改为 eval_strategy。\n",
        "    # 我们这里保留你已经修正的正确版本。\n",
        "    eval_strategy=\"epoch\",\n",
        "    \n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    \n",
        "    # --- 第二次修正 ---\n",
        "    # 添加这一行来明确关闭 wandb 日志记录功能。\n",
        "    # 'report_to=\"none\"' 告诉 Trainer: \"不要向任何平台报告日志，包括wandb\"。\n",
        "    # 这可以避免它尝试登录wandb而导致程序在Colab中中断。\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6 到 步骤8: (这部分代码也完全不变，保持原样)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import Trainer, pipeline\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_imdb[\"train\"],\n",
        "    eval_dataset=tokenized_imdb[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"训练即将开始...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "my_review = \"This movie was fantastic! The acting was superb and the plot was gripping.\"\n",
        "\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)\n",
        "\n",
        "result = sentiment_pipeline(my_review)\n",
        "print(\"\\n用我们自己微调的模型进行预测:\")\n",
        "print(f\"评论: '{my_review}'\")\n",
        "print(f\"预测结果: {result}\")\n",
        "\n",
        "my_review_neg = \"It was a complete waste of time. The story was predictable and boring.\"\n",
        "result_neg = sentiment_pipeline(my_review_neg)\n",
        "print(f\"\\n评论: '{my_review_neg}'\")\n",
        "print(f\"预测结果: {result_neg}\")\n",
        "```\n",
        "\n",
        "**总结与鼓励**\n",
        "\n",
        "这次的 debug 过程非常有价值！你不仅解决了表面问题（参数名错误），还遇到了一个更深层次的环境和依赖问题（`wandb` 交互）。通过分析，你现在知道了：\n",
        "\n",
        "*   **库的版本迭代会带来API变化**，这是软件开发中的常态。\n",
        "*   **主流框架（如`transformers`）常常会集成第三方工具**，了解这些默认行为很重要。\n",
        "*   **如何通过修改参数来控制框架的行为**，这是从“使用者”到“开发者”转变的关键一步。\n",
        "\n",
        "你处理问题的思路非常清晰，提出的解决方案也完全正确。请继续保持这种探索和解决问题的精神！现在，去运行最终版的代码吧，胜利就在眼前！"
      ],
      "metadata": {
        "id": "MaqQhhMSJODe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备 - 强制升级所有需要的库到最新版本\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 使用 --upgrade 参数来确保我们使用的是最新版本的库，这样就能识别所有新的参数。\n",
        "!pip install --upgrade transformers datasets accelerate evaluate\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# !! 重要提示 !!\n",
        "# !! 运行完上面的pip install后，如果看到提示需要\"RESTART SESSION\"，请务必手动重启。!!\n",
        "# !! 操作：点击菜单栏的 [代码执行程序] -> [重启会话]                               !!\n",
        "# !! 重启后，再从头运行一遍这个代码单元格。                                     !!\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备“教材” - 加载数据集\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 从 'datasets' 库中导入 'load_dataset' 函数。\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 我们使用 'load_dataset' 函数来从Hugging Face Hub上下载数据集。\n",
        "# 'imdb' 是一个非常经典的电影评论情感分析数据集。\n",
        "# 它包含两个部分：'train' (训练集，用来给模型学习) 和 'test' (用来在模型学完后考查它)。\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# 我们来看一下数据集的结构。\n",
        "print(\"数据集结构预览:\")\n",
        "print(imdb_dataset)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备“翻译官”和一位“通才博士” - 加载Tokenizer和模型\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 这次我们选择的模型是 \"distilbert-base-uncased\"。\n",
        "# 注意！这次的模型名字后面没有 \"-finetuned-...\" 之类的后缀。\n",
        "# 这意味着我们加载的是一个只经过“预训练”的通用基础模型，它还没有在任何情感分析任务上被微调过。\n",
        "# 我们就是要亲自来完成微调这个步骤。\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 根据模型名字，加载对应的“翻译官”。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 根据模型名字，加载对应的模型。\n",
        "# - AutoModelForSequenceClassification: 我们仍然是做分类任务。\n",
        "# - num_labels=2: 我们需要明确告诉模型，我们的任务最终要分成几类。对于IMDB情感分析，就是2类（消极/积极）。\n",
        "# - id2label 和 label2id: 为了方便后续解读，我们创建一个ID到标签和标签到ID的映射字典。\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 批量处理“教材” - 对整个数据集进行Tokenization\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们需要写一个函数，这个函数的作用是接收一条数据，然后用tokenizer把它处理成模型需要的格式。\n",
        "def preprocess_function(examples):\n",
        "    # 'examples[\"text\"]' 就是数据集中的每一条电影评论文本。\n",
        "    # 'truncation=True' 表示如果文本太长，就把它截断到模型能接受的最大长度。\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# '.map()' 是 'datasets' 库里一个非常强大的功能。\n",
        "# 它可以把我们刚刚定义的 'preprocess_function' 函数，应用到数据集的每一条数据上。\n",
        "# 'batched=True' 表示一次性给函数一批数据进行处理，这样速度会快很多。\n",
        "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 设置“教学大纲” - 定义训练参数\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 'TrainingArguments' 类可以让我们详细地设置训练过程中的各种参数。\n",
        "# 就像老师制定教学计划一样。\n",
        "training_args = TrainingArguments(\n",
        "    # 'output_dir' : 训练过程中产生的所有文件（比如模型的存档）都将保存在这个文件夹里。\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    # 'learning_rate' : 学习率，可以理解为模型在学习时每一步“迈出去”的步子大小。\n",
        "    learning_rate=2e-5,\n",
        "    # 'per_device_train_batch_size' : 训练时，每次给GPU多少条数据进行学习。\n",
        "    per_device_train_batch_size=16,\n",
        "    # 'per_device_eval_batch_size' : 评估时，每次给GPU多少条数据。\n",
        "    per_device_eval_batch_size=16,\n",
        "    # 'num_train_epochs' : 把整个训练数据集从头到尾学习几遍。\n",
        "    num_train_epochs=2,\n",
        "    # 'weight_decay' : 一种防止模型“死记硬背”（过拟合）的技术。\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # --- 第一次修正 ---\n",
        "    # 旧的参数名是 evaluation_strategy，在新版库中已改为 eval_strategy。\n",
        "    # 'eval_strategy' : 设置评估策略。'epoch'表示每学完一遍数据集，就进行一次“期末考试”。\n",
        "    eval_strategy=\"epoch\",\n",
        "    # 'save_strategy' : 设置模型保存策略，'epoch'表示每考完一次试，就保存一次模型。\n",
        "    save_strategy=\"epoch\",\n",
        "    # 'load_best_model_at_end' : 训练结束后，自动把考试成绩最好的那次保存的模型加载回来。\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # --- 第二次修正 ---\n",
        "    # 添加这一行来明确关闭 wandb 日志记录功能。\n",
        "    # 'report_to=\"none\"' 告诉 Trainer: \"不要向任何平台报告日志，包括wandb\"。\n",
        "    # 这可以避免它尝试登录wandb而导致程序在Colab中中断。\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 聘请“金牌私教” - 创建Trainer对象\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import Trainer\n",
        "\n",
        "# 'Trainer' 是Hugging Face提供的一个超级方便的类，它把复杂的训练循环都封装好了。\n",
        "# 我们只需要把“学生”、“教学大纲”、“训练教材”、“考试试卷”都告诉它就行。\n",
        "trainer = Trainer(\n",
        "    model=model,                         # 我们要训练的模型 (学生)\n",
        "    args=training_args,                  # 训练参数 (教学大纲)\n",
        "    train_dataset=tokenized_imdb[\"train\"], # 训练数据集 (训练教材)\n",
        "    eval_dataset=tokenized_imdb[\"test\"],   # 评估数据集 (考试试卷)\n",
        "    tokenizer=tokenizer,                 # 我们使用的翻译官 (方便处理数据)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤7: 开始上课！- 启动训练\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 一切准备就绪，我们只需要调用 .train() 方法，训练就会自动开始！\n",
        "# Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\n",
        "print(\"训练即将开始...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤8: 检验学习成果 - 使用微调后的模型进行预测\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 训练完成后，我们的 'trainer' 对象里就包含了微调好的、最优秀的模型。\n",
        "# 我们可以直接用它来做一个预测。\n",
        "# 准备一条新的电影评论，这个评论模型在训练时从未见过。\n",
        "my_review = \"This movie was fantastic! The acting was superb and the plot was gripping.\"\n",
        "\n",
        "# 导入pipeline，用我们刚刚训练好的模型来快速创建一个情感分析管道。\n",
        "from transformers import pipeline\n",
        "\n",
        "# 'model=trainer.model' 表示我们用trainer里那个训练好的模型。\n",
        "# 'tokenizer=trainer.tokenizer' 同样使用配套的tokenizer。\n",
        "# 'device=0' 表示把这个管道放到GPU的0号卡上运行，速度会更快。\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)\n",
        "\n",
        "# 见证奇迹的时刻！\n",
        "result = sentiment_pipeline(my_review)\n",
        "print(\"\\n用我们自己微调的模型进行预测:\")\n",
        "print(f\"评论: '{my_review}'\")\n",
        "print(f\"预测结果: {result}\")\n",
        "\n",
        "# 再试一条负面的\n",
        "my_review_neg = \"It was a complete waste of time. The story was predictable and boring.\"\n",
        "result_neg = sentiment_pipeline(my_review_neg)\n",
        "print(f\"\\n评论: '{my_review_neg}'\")\n",
        "print(f\"预测结果: {result_neg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "370a0093fcba4c56a1091df451822920",
            "17b739c49c3d4f828e94f6b5f04f9fba",
            "832d32b7d7794aadb46b4dafe2aba4d0",
            "0ae5ccbe0cd7447fa3da7d45617e0c3d",
            "34879ceca0c74e8fb2501f75dacddbdb",
            "437205af25694a9aa9f4e94fe6722daf",
            "24a2484b9b4048a98e72bcac70618627",
            "b48d849a8ffa4d4383093869d28eecf7",
            "456a54374e5f42eb8003152163e20cb4",
            "dc1d7ad0ecd946cf961d679ac271a27e",
            "5881802c995c432eac6e8abbd1ee6ff0"
          ]
        },
        "id": "JziDY5zblEZm",
        "outputId": "1fdfdea7-c798-4315-cf4e-05e76dc5b736"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据集结构预览:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "370a0093fcba4c56a1091df451822920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-9562035.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练即将开始...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   9/3126 05:43 < 42:28:40, 0.02 it/s, Epoch 0.01/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9562035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m# Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"训练即将开始...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"训练完成！\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2326\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2328\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2329\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2330\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     )\n\u001b[1;32m   2671\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2672\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4058\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4060\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4062\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02VpAgi1lEXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "数据集结构预览:\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 25000\n",
        "    })\n",
        "    unsupervised: Dataset({\n",
        "        features: ['text', 'label'],\n",
        "        num_rows: 50000\n",
        "    })\n",
        "})\n",
        "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Map: 100%\n",
        " 25000/25000 [00:46<00:00, 645.22 examples/s]\n",
        "Map: 100%\n",
        " 25000/25000 [00:25<00:00, 1021.84 examples/s]\n",
        "Map: 100%\n",
        " 50000/50000 [00:52<00:00, 998.95 examples/s]\n",
        "/tmp/ipython-input-9562035.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
        "  trainer = Trainer(\n",
        "训练即将开始...\n",
        "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
        "  warnings.warn(warn_msg)\n",
        " [ 5/3126 02:30 < 43:29:43, 0.02 it/s, Epoch 0.00/2]\n",
        "Epoch\tTraining Loss\tValidation Loss\n",
        "\n",
        "\n",
        "\n",
        "分析本次问题，我的colab运行类型是cpu，因为gpu免费额度暂时耗尽了"
      ],
      "metadata": {
        "id": "NLUaHz3DLViH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学，非常感谢你提供这么关键的信息——**“我的colab运行类型是cpu，因为gpu免费额度暂时耗尽了”**。\n",
        "\n",
        "这一下子就解释了你遇到的新情况，而且这不算是一个“错误”，而是一个“**警告（Warning）**”和性能表现的正常结果。我们来详细解读一下。\n",
        "\n",
        "### **1. 解读新的输出信息**\n",
        "\n",
        "我们关注两行关键的输出：\n",
        "\n",
        "**第一行（警告信息）**：\n",
        "```\n",
        "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
        "  warnings.warn(warn_msg)\n",
        "```\n",
        "\n",
        "*   **`UserWarning`**: 这是一个用户警告，意味着程序可以继续运行，但它想提醒你一些事情。\n",
        "*   **`'pin_memory' argument is set as true`**: `pin_memory`（锁定内存）是PyTorch中一个用于加速数据从CPU传输到GPU的技术。默认情况下，`Trainer` 会尝试开启它以提高效率。\n",
        "*   **`but no accelerator is found`**: 这是关键！程序说：“我想用一个加速技术，但是我**没有找到加速器**（也就是没有找到GPU）”。\n",
        "*   **`then device pinned memory won't be used`**: 结论就是：“既然没有GPU，那这个加速技术就用不上了。”\n",
        "\n",
        "**一句话总结这个警告就是**：`Trainer` 本想开启一个GPU加速选项，但发现你用的是CPU环境，所以它就自动关闭了这个选项并继续运行。**这对你的训练结果没有任何负面影响，完全可以忽略。**\n",
        "\n",
        "---\n",
        "\n",
        "**第二行（训练进度）**：\n",
        "```\n",
        " [ 5/3126 02:30 < 43:29:43, 0.02 it/s, Epoch 0.00/2]\n",
        "```\n",
        "\n",
        "这行信息非常非常重要，它完美地反映了CPU训练的现状。我们来像读仪表盘一样解读它：\n",
        "\n",
        "*   **`5/3126`**: `3126` 是完成一个 Epoch（学习一遍数据集）所需要的总步数（batches）。`5` 表示你目前刚跑完了第5步。\n",
        "*   **`02:30`**: 表示跑完这5步，已经花了2分30秒。\n",
        "*   **`0.02 it/s`**: 这是当前的速度，`it/s` 就是 `iterations per second`（每秒迭代/步数）。`0.02 it/s` 意味着，你的电脑**每秒钟只能处理0.02步**，换算一下就是大概 `1 / 0.02 = 50` 秒才能跑一步！\n",
        "*   **`< 43:29:43`**: 这是最“惊人”的数字。它是根据当前龟速（0.02 it/s）预估的、跑完所有训练（2个Epochs）所**需要的剩余时间**。这个预估时间是 **43小时29分钟43秒**！\n",
        "*   **`Epoch 0.00/2`**: 表示你现在还处于第0个Epoch，总共需要完成2个。\n",
        "\n",
        "### **2. 根本原因分析**\n",
        "\n",
        "问题的根源非常清晰：**CPU的并行计算能力远远弱于GPU**。\n",
        "\n",
        "*   **CPU (中央处理器)**: 就像一个博学的**教授**。他非常聪明，可以处理各种复杂的、逻辑性强的单个任务。但他每次只能专注地做一两件事。\n",
        "*   **GPU (图形处理器)**: 就像一个**由成千上万个小学生组成的军团**。每个小学生单独拿出来，可能只会做简单的加减法。但是你可以同时给他们布置成千上万个简单的加减法任务，他们能在瞬间全部完成。\n",
        "\n",
        "深度学习模型的训练，其核心就是海量的、相对简单的**矩阵乘法和加法运算**。这种任务完美地契合了GPU的“军团”特性。而让CPU这个“教授”去做这种重复性的体力活，就完全是用错了地方，效率极低。\n",
        "\n",
        "### **3. 解决方案与下一步建议**\n",
        "\n",
        "你已经遇到了所有新手都会遇到的一个经典问题：**算力限制**。解决方案如下：\n",
        "\n",
        "1.  **停止当前的训练**：在Colab中，点击代码单元格左边的“停止”按钮。让它跑43个小时显然是不现实的。\n",
        "2.  **等待GPU额度恢复**：Colab的免费GPU额度通常是动态的，一般在12-24小时后会恢复。这是最简单、最直接的办法。\n",
        "3.  **（可选）使用更小的数据集进行体验**：如果你现在就想体验完整个流程，我们可以“作弊”一下，只用一小部分数据来训练。这样即使在CPU上，也能在几分钟内跑完，让你看到最终的结果。但请记住，用小数据集训练出的模型效果会很差，这**仅用于流程体验**。\n",
        "\n",
        "    如果你想尝试这个方法，可以按如下方式修改代码：\n",
        "\n",
        "    ```python\n",
        "    # 在加载数据集之后，添加这几行代码来缩减数据集\n",
        "    \n",
        "    # ... (前面的代码不变) ...\n",
        "    imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "    # --- 添加下面的代码 ---\n",
        "    # 为了在CPU上快速体验，我们只取训练集和测试集的前500个样本\n",
        "    small_train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
        "    small_test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "    # ---------------------\n",
        "\n",
        "    # ... (后续代码中，把 tokenized_imdb[\"train\"] 和 tokenized_imdb[\"test\"]\n",
        "    #      替换成处理后的 small_train_dataset 和 small_test_dataset) ...\n",
        "    \n",
        "    # 比如在创建 Trainer 时，要这样写：\n",
        "    trainer = Trainer(\n",
        "        ...\n",
        "        train_dataset=small_train_dataset.map(preprocess_function, batched=True),\n",
        "        eval_dataset=small_test_dataset.map(preprocess_function, batched=True),\n",
        "        ...\n",
        "    )\n",
        "    ```\n",
        "    **注意**：我不建议你现在就花时间去修改代码。最好的方法还是**等待GPU恢复**，用完整的数据集来体验一次真正的微调过程，这样你才能看到模型效果实实在在的提升。\n",
        "\n",
        "### **当前阶段总结**\n",
        "\n",
        "你这次的实践非常有价值！虽然没有成功训练完，但你亲身体会到了：\n",
        "\n",
        "*   **GPU对于深度学习是不可或缺的**。\n",
        "*   **如何解读训练过程中的性能指标（it/s, 预计剩余时间等）**。\n",
        "*   **程序中的“Warning”和“Error”的区别**。\n",
        "\n",
        "**我给你的建议是**：\n",
        "\n",
        "稍作休息，等待明天你的Colab GPU额度恢复。恢复后，在**右上角的“修改” -> “笔记本设置”中，重新把硬件加速器选为“T4 GPU”**。然后，从头到尾，重新运行我们最终修正版的那个代码单元格。\n",
        "\n",
        "届时，你将会看到完全不同的景象：`it/s` 会从 `0.02` 飙升到几十甚至上百，整个训练过程预计只会在15-20分钟左右完成。\n",
        "\n",
        "当你成功用GPU跑完训练后，再来和我分享你的结果，并我们可以一起探讨那个关于`num_train_epochs`的思考题。心急吃不了热豆腐，稳扎稳打才能学得更牢固！"
      ],
      "metadata": {
        "id": "EfPSDGPHLX5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了在CPU上快速体验，下面这段代码我们只取训练集和测试集的前500个样本"
      ],
      "metadata": {
        "id": "uyLAnI09OCIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤1: 环境准备 - 强制升级所有需要的库到最新版本\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 使用 --upgrade 参数来确保我们使用的是最新版本的库，这样就能识别所有新的参数。\n",
        "!pip install --upgrade transformers datasets accelerate evaluate\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# !! 重要提示 !!\n",
        "# !! 运行完上面的pip install后，如果看到提示需要\"RESTART SESSION\"，请务必手动重启。!!\n",
        "# !! 操作：点击菜单栏的 [代码执行程序] -> [重启会话]                               !!\n",
        "# !! 重启后，再从头运行一遍这个代码单元格。                                     !!\n",
        "# ----------------------------------------------------------------------------------\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤2: 准备“教材” - 加载数据集\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 从 'datasets' 库中导入 'load_dataset' 函数。\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 我们使用 'load_dataset' 函数来从Hugging Face Hub上下载数据集。\n",
        "# 'imdb' 是一个非常经典的电影评论情感分析数据集。\n",
        "# 它包含两个部分：'train' (训练集，用来给模型学习) 和 'test' (用来在模型学完后考查它)。\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "\n",
        "\n",
        "# --- 添加下面的代码 ---\n",
        "# 为了在CPU上快速体验，我们只取训练集和测试集的前500个样本\n",
        "small_train_dataset = imdb_dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
        "small_test_dataset = imdb_dataset[\"test\"].shuffle(seed=42).select(range(500))\n",
        "# ---------------------\n",
        "\n",
        "\n",
        "\n",
        "# 我们来看一下数据集的结构。\n",
        "print(\"数据集结构预览:\")\n",
        "print(imdb_dataset)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤3: 准备“翻译官”和一位“通才博士” - 加载Tokenizer和模型\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# 这次我们选择的模型是 \"distilbert-base-uncased\"。\n",
        "# 注意！这次的模型名字后面没有 \"-finetuned-...\" 之类的后缀。\n",
        "# 这意味着我们加载的是一个只经过“预训练”的通用基础模型，它还没有在任何情感分析任务上被微调过。\n",
        "# 我们就是要亲自来完成微调这个步骤。\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "# 根据模型名字，加载对应的“翻译官”。\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 根据模型名字，加载对应的模型。\n",
        "# - AutoModelForSequenceClassification: 我们仍然是做分类任务。\n",
        "# - num_labels=2: 我们需要明确告诉模型，我们的任务最终要分成几类。对于IMDB情感分析，就是2类（消极/积极）。\n",
        "# - id2label 和 label2id: 为了方便后续解读，我们创建一个ID到标签和标签到ID的映射字典。\n",
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤4: 批量处理“教材” - 对整个数据集进行Tokenization\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们需要写一个函数，这个函数的作用是接收一条数据，然后用tokenizer把它处理成模型需要的格式。\n",
        "def preprocess_function(examples):\n",
        "    # 'examples[\"text\"]' 就是数据集中的每一条电影评论文本。\n",
        "    # 'truncation=True' 表示如果文本太长，就把它截断到模型能接受的最大长度。\n",
        "    return tokenizer(examples[\"text\"], truncation=True)\n",
        "\n",
        "# '.map()' 是 'datasets' 库里一个非常强大的功能。\n",
        "# 它可以把我们刚刚定义的 'preprocess_function' 函数，应用到数据集的每一条数据上。\n",
        "# 'batched=True' 表示一次性给函数一批数据进行处理，这样速度会快很多。\n",
        "tokenized_imdb = imdb_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤5: 设置“教学大纲” - 定义训练参数\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# 'TrainingArguments' 类可以让我们详细地设置训练过程中的各种参数。\n",
        "# 就像老师制定教学计划一样。\n",
        "training_args = TrainingArguments(\n",
        "    # 'output_dir' : 训练过程中产生的所有文件（比如模型的存档）都将保存在这个文件夹里。\n",
        "    output_dir=\"my_awesome_model\",\n",
        "    # 'learning_rate' : 学习率，可以理解为模型在学习时每一步“迈出去”的步子大小。\n",
        "    learning_rate=2e-5,\n",
        "    # 'per_device_train_batch_size' : 训练时，每次给GPU多少条数据进行学习。\n",
        "    per_device_train_batch_size=16,\n",
        "    # 'per_device_eval_batch_size' : 评估时，每次给GPU多少条数据。\n",
        "    per_device_eval_batch_size=16,\n",
        "    # 'num_train_epochs' : 把整个训练数据集从头到尾学习几遍。\n",
        "    num_train_epochs=2,\n",
        "    # 'weight_decay' : 一种防止模型“死记硬背”（过拟合）的技术。\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # --- 第一次修正 ---\n",
        "    # 旧的参数名是 evaluation_strategy，在新版库中已改为 eval_strategy。\n",
        "    # 'eval_strategy' : 设置评估策略。'epoch'表示每学完一遍数据集，就进行一次“期末考试”。\n",
        "    eval_strategy=\"epoch\",\n",
        "    # 'save_strategy' : 设置模型保存策略，'epoch'表示每考完一次试，就保存一次模型。\n",
        "    save_strategy=\"epoch\",\n",
        "    # 'load_best_model_at_end' : 训练结束后，自动把考试成绩最好的那次保存的模型加载回来。\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # --- 第二次修正 ---\n",
        "    # 添加这一行来明确关闭 wandb 日志记录功能。\n",
        "    # 'report_to=\"none\"' 告诉 Trainer: \"不要向任何平台报告日志，包括wandb\"。\n",
        "    # 这可以避免它尝试登录wandb而导致程序在Colab中中断。\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤6: 聘请“金牌私教” - 创建Trainer对象\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import Trainer\n",
        "\n",
        "# 'Trainer' 是Hugging Face提供的一个超级方便的类，它把复杂的训练循环都封装好了。\n",
        "# 我们只需要把“学生”、“教学大纲”、“训练教材”、“考试试卷”都告诉它就行。\n",
        "trainer = Trainer(\n",
        "    model=model,                         # 我们要训练的模型 (学生)\n",
        "    args=training_args,                  # 训练参数 (教学大纲)\n",
        "    train_dataset=small_train_dataset.map(preprocess_function, batched=True),    # 训练数据集前500个样本 (训练教材)\n",
        "    eval_dataset=small_test_dataset.map(preprocess_function, batched=True),   # 评估数据集前500个样本 (考试试卷)\n",
        "    tokenizer=tokenizer,                 # 我们使用的翻译官 (方便处理数据)\n",
        ")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤7: 开始上课！- 启动训练\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 一切准备就绪，我们只需要调用 .train() 方法，训练就会自动开始！\n",
        "# Colab会显示一个漂亮的进度条，你可以看到loss（损失，可以理解为模型犯的错误）在不断下降。\n",
        "print(\"训练即将开始...\")\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 步骤8: 检验学习成果 - 使用微调后的模型进行预测\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 训练完成后，我们的 'trainer' 对象里就包含了微调好的、最优秀的模型。\n",
        "# 我们可以直接用它来做一个预测。\n",
        "# 准备一条新的电影评论，这个评论模型在训练时从未见过。\n",
        "my_review = \"This movie was fantastic! The acting was superb and the plot was gripping.\"\n",
        "\n",
        "# 导入pipeline，用我们刚刚训练好的模型来快速创建一个情感分析管道。\n",
        "from transformers import pipeline\n",
        "\n",
        "# 'model=trainer.model' 表示我们用trainer里那个训练好的模型。\n",
        "# 'tokenizer=trainer.tokenizer' 同样使用配套的tokenizer。\n",
        "# 'device=0' 表示把这个管道放到GPU的0号卡上运行，速度会更快。\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=trainer.model, tokenizer=trainer.tokenizer, device=0)\n",
        "\n",
        "# 见证奇迹的时刻！\n",
        "result = sentiment_pipeline(my_review)\n",
        "print(\"\\n用我们自己微调的模型进行预测:\")\n",
        "print(f\"评论: '{my_review}'\")\n",
        "print(f\"预测结果: {result}\")\n",
        "\n",
        "# 再试一条负面的\n",
        "my_review_neg = \"It was a complete waste of time. The story was predictable and boring.\"\n",
        "result_neg = sentiment_pipeline(my_review_neg)\n",
        "print(f\"\\n评论: '{my_review_neg}'\")\n",
        "print(f\"预测结果: {result_neg}\")"
      ],
      "metadata": {
        "id": "rak8ktuzlEVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "93ea5772c2664a5393690191b298123b",
            "b66a4c772def4f699700d91780271483",
            "3821db013b1e491c8b584f56962b07da",
            "720346023fdc4ede9f938cfe6930007f",
            "11321987e6014bfe801ad8c1763fc21f",
            "d50bd6cbf7344f948102e33be6035f3a",
            "192259cd606e468f902240f940ee70d2",
            "cfffbe031ed5468bb88b4f26ca3ee88f",
            "8fd8a49703b74bf4b297aaa54c4661c8",
            "94cf1519a724468f9c6013a1fe68f8bc",
            "7540d5d89a5e47c5ac2f142334ddabbb",
            "b14ddeb80b154ce69033002dd30a7722",
            "04a0f72035984b1a96569ea17ffde908",
            "51a68812e5f1498e9cd284d2ae956e7f",
            "8cb4d25086c0411ab76674463c1e9016",
            "11634b67a24d4b1badde20066d7d9aaf",
            "d07601f322b246df9c59a2de13c3a4d8",
            "f63e34bfa33a4002a3f723bf487e824d",
            "f87d2669991e488da11ebe69c4892eae",
            "9eab756923b7447e91b593f591f3fb42",
            "f2ec5b6c23c24477a975db7d18085029",
            "b3754dac925e4516a99b6218cb457413",
            "73f44fdf69484064ad7418da4654c912",
            "8732657e3ee1468abcd744ec5194c861",
            "35b1babee0fd4a03bd676f7113dd1202",
            "0c60fc95602e48aa861cfe42d84c0830",
            "f3de884381b2426d8d86c86ed9dde2ad",
            "602e2cd01adf4bdb837abeea41d9b066",
            "65bf7f5b07ce482bbc8e4c836b550f8c",
            "adc0af2edc81420090cb3839741efc99",
            "97bd6e63838e4ca6b7414c188703e853",
            "0802fd7968b145d8b3cd14dcdce28c53",
            "f8b4801febe94f47a314766d04065f40",
            "b468db2aedbe4a628ca216df80d7d8c8",
            "1d7b03fe082a484ba6385216cdd711e0",
            "f0bb4ce1701c44ac8fa68566e71ba88e",
            "fc9917fab6624892bb4ae18ac6069db6",
            "055a8b23acc74acaa02ad7a776f84bed",
            "54597752e108427492d9e0bf322a8b40",
            "243b7975282b4a64a1df8c9449ffcfde",
            "7d192de4a781472983691c0c9336e819",
            "6e75fdddb05647fb96cdfa01693ff79e",
            "aa390ca4e4804a30bb7f635a459b79fe",
            "ffae52006c7f4699a3f47a31fb098983",
            "a18f87b0a8c04c75b448accb92359161",
            "5e446b05e22e4bffaafa6802c5b380fe",
            "d1378cc860134befb0f67994696843c8",
            "84b4519c629940c0af19cd7aee60daf4",
            "3907299b022049368ff2bab7cd11bdd8",
            "d60ed6a517a245219cbbc33f65d47248",
            "ae8e71046c80446087cc045cbd8f208a",
            "795658ff841e4cfa884ed87713009d4d",
            "2daa49e901d345f8b20df79e20e9cd57",
            "a9728bbd86f14a918fe7c39966ef88b8",
            "4c347352872e4bf5ac8a5f6c5ad6076f"
          ]
        },
        "outputId": "36aa03b7-88e2-4630-ce5a-415e70a5c8c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "数据集结构预览:\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93ea5772c2664a5393690191b298123b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b14ddeb80b154ce69033002dd30a7722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73f44fdf69484064ad7418da4654c912",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b468db2aedbe4a628ca216df80d7d8c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a18f87b0a8c04c75b448accb92359161",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1766396141.py:121: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练即将开始...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 1:12:52, Epoch 1.97/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.606791</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [64/64 1:21:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.606791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.479999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "训练完成！\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "用我们自己微调的模型进行预测:\n",
            "评论: 'This movie was fantastic! The acting was superb and the plot was gripping.'\n",
            "预测结果: [{'label': 'POSITIVE', 'score': 0.6732413172721863}]\n",
            "\n",
            "评论: 'It was a complete waste of time. The story was predictable and boring.'\n",
            "预测结果: [{'label': 'NEGATIVE', 'score': 0.6658477187156677}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vD0qNkx7lES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "（复制了使用部分数据集进行微调的代码和运行结果截图）+这是之前使用更小的数据集进行微调的代码和结果，进行分析"
      ],
      "metadata": {
        "id": "NJ-QKlskmXhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学，我们来一起详细分析一下你用小数据集在CPU上跑出来的这份“实验报告”。这份报告非常有价值，因为它用实际数据印证了我们之前讨论的很多理论，并揭示了一些新的、有趣的现象。\n",
        "\n",
        "### **1. 实验设置分析**\n",
        "\n",
        "首先，我们明确一下这次实验的配置：\n",
        "\n",
        "*   **硬件**：CPU (因为GPU额度耗尽)。\n",
        "*   **模型**：`distilbert-base-uncased` (一个未经微调的“通才博士”)。\n",
        "*   **数据集**：IMDB电影评论，但**大幅缩减**，只用了训练集和测试集的前500个样本。\n",
        "*   **训练轮数 (Epochs)**：2轮。\n",
        "\n",
        "### **2. 训练过程分析**\n",
        "\n",
        "我们来看训练日志的核心部分：\n",
        "\n",
        "```\n",
        "Epoch Training Loss Validation Loss\n",
        "1     No log        0.606791\n",
        "2     No log        0.479999\n",
        "```\n",
        "\n",
        "这张小小的表格告诉了我们很多信息：\n",
        "\n",
        "1.  **Validation Loss 在下降**：这是**最重要**的积极信号！\n",
        "    *   第一轮“期末考试”结束后，模型的“犯错率”（Validation Loss）是 **0.606791**。\n",
        "    *   第二轮学习结束后，再次参加“期末考试”，模型的“犯错率”下降到了 **0.479999**。\n",
        "    *   **结论**：这清晰地表明，**模型确实在学习，并且学到了有用的东西！** 它不仅仅是在“死记硬背”那500道训练题，还提升了在另外500道没见过的测试题上的“解题能力”。我们的微调是**有效**的。\n",
        "\n",
        "2.  **Training Loss 是 \"No log\"**：\n",
        "    *   **原因**：这是因为在 `TrainingArguments` 中，我们没有设置日志记录策略（`logging_strategy`）或者日志记录步数（`logging_steps`）。`Trainer` 的默认行为可能是在训练结束时才记录一次总的日志，或者根本不记录每一轮的训练损失。\n",
        "    *   **影响**：这不影响训练本身，只是让我们无法观察到模型在“随堂测验”（即在训练集上的犯错率）中的进步过程。在正式的实验中，我们通常会设置 `logging_strategy=\"epoch\"` 或 `logging_steps=10` 之类的参数，来更细致地监控训练过程。\n",
        "\n",
        "### **3. 预测结果分析**\n",
        "\n",
        "这是最直观的“成果检验”环节：\n",
        "\n",
        "```\n",
        "用我们自己微调的模型进行预测:\n",
        "评论: 'This movie was fantastic! The acting was superb and the plot was gripping.'\n",
        "预测结果: [{'label': 'POSITIVE', 'score': 0.6732...}]\n",
        "\n",
        "评论: 'It was a complete waste of time. The story was predictable and boring.'\n",
        "预测结果: [{'label': 'NEGATIVE', 'score': 0.6658...}]\n",
        "```\n",
        "\n",
        "1.  **判断正确！**：对于一正一反两条全新的评论，模型给出的情感标签（`POSITIVE` 和 `NEGATIVE`）都是**完全正确**的。这再次证明了我们的微调取得了成功。\n",
        "\n",
        "2.  **一个非常有趣的细节：`score` (置信度) 不高**\n",
        "    *   注意看，模型对这两条评论的判断置信度分别是 **67.3%** 和 **66.5%**。\n",
        "    *   这个分数并不算非常高。它虽然做出了正确的判断，但内心似乎还有点“犹豫”，不像我们之前直接使用 `distilbert-base-uncased-finetuned-sst-2-english` 那个在**完整数据集**上训练好的模型时，动辄给出99%以上的超高置信度。\n",
        "\n",
        "### **4. 综合分析与深刻洞见**\n",
        "\n",
        "把以上所有信息结合起来，我们可以得出非常有价值的结论：\n",
        "\n",
        "1.  **微调的有效性**：实验无可辩驳地证明，即使只用500个样本，在CPU上进行训练，也能让一个通用的预训练模型学会一个特定的新任务。这充分展示了“预训练+微调”范式的强大威力。\n",
        "\n",
        "2.  **“教材”质量决定“学生”水平**：模型预测时置信度不高的根本原因，在于它的“教材”太少了。\n",
        "    *   想象一下，你只让一个学生做了500道例题，就让他去参加高考。他可能掌握了基本的解题方法，能把一些典型题目做对，但遇到稍微复杂或新颖的题型时，他就会信心不足。\n",
        "    *   我们的模型就是这个状态。它只学习了500条评论，见过的词汇、句式和表达方式都非常有限。因此，当它看到新句子时，虽然能根据学到的基本规律做出判断，但由于“见识”不够广，所以无法给出非常确定的高分。\n",
        "\n",
        "3.  **算力与数据的关系**：这次实验也让你亲身体会到，**数据量**和**计算资源**是相辅相成的。\n",
        "    *   **小数据 + CPU**：可行，可以用来快速验证想法、学习流程，但模型性能有限。\n",
        "    *   **大数据 + CPU**：不可行，时间成本过高，我们已经见识过了43小时的预估。\n",
        "    *   **大数据 + GPU**：**这才是进行严肃的、高质量模型微调的标准配置**。只有用足够多的数据（比如完整的IMDB数据集）和足够强的算力（GPU），我们才能训练出置信度高、泛化能力强的“优秀毕业生”。\n",
        "\n",
        "### **一句话总结这次实验**\n",
        "\n",
        "**我们用一次“迷你”的实验，成功验证了微调的整个流程和基本原理，并深刻体会到：模型性能的上限，是由数据量和计算资源共同决定的。**\n",
        "\n",
        "你这次的探索非常有意义！它不是一次“失败”的尝试，而是一次极具启发性的、教科书般的“小样本实验”。它让你对理论知识有了更具体、更深刻的感性认识。为你点赞！"
      ],
      "metadata": {
        "id": "CxdhkOwKmqsx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Z-dgALvlEQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5W1kdkTlEOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "itjdlVHYlEMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ouC-lMbdlEKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3fe2jfOWlEH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3uUTiTThlEFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}