{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej8wqugW8oab"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1、按照方案开始下一步，讲解理论和指导实践。\n",
        "\n",
        "2、要求用学生听得懂的通俗易懂的语言为我进行讲解和指导我进行实践。\n",
        "\n",
        "3、编程环境基于google的colab。由于我对python基础不熟练，注意代码中都要有极尽细致详细的注释，保证我能看懂所有变量，函数，语法结构和每行语句的功能，以便我能进行实践复现。\n",
        "\n",
        "4、按照\"step by step\"原则逐步指导我，并及时根据我的反馈进行调整。"
      ],
      "metadata": {
        "id": "g7bU0MXX8vUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lx22whjKeSSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！你已经成功地炼制出了属于你自己的“丹药”——Chat-嬛嬛模型。现在，我们要进入进阶篇的最后一个、也是非常关键的一个环节了。\n",
        "\n",
        "之前我们所有的工作，都是在我们自己的“炼丹炉”（Colab笔记本）里完成的。但如果我们想让朋友也来体验一下和“嬛嬛”聊天，或者想在别的程序里调用我们训练好的模型，该怎么办呢？难道每次都要重新加载基础模型，再把LoRA插件“插”上去吗？这太麻烦了！\n",
        "\n",
        "我们需要学习如何把我们训练好的**“LoRA插件”**和**“老教授”（基础模型）**完美地**“焊接”**在一起，形成一个全新的、独立的、可以直接使用的**“甄嬛分身”**模型。然后，把这个完整的“分身”保存下来，甚至分享到Hugging Face Hub，让所有人都能一键使用！\n",
        "\n",
        "这就是我们今天要学习的核心内容：**LoRA模型的合并、保存与加载**。\n",
        "\n",
        "---\n",
        "\n",
        "### **进阶篇 · 第三步（收官）：模型的合并、保存与分享**\n",
        "### **(预计用-时: 1-1.5小时)**\n",
        "\n",
        "#### **1. 理论学习：从“外挂插件”到“内化于心”**\n",
        "\n",
        "我们再用一次“老教授”和“智能笔记本”的比喻：\n",
        "\n",
        "*   **训练时 (PEFT模型)**：我们是“**教授 + 笔记本**”的组合。教授的大脑（基础模型）是冰冻的，所有的知识都记在笔记本（LoRA适配器）上。这个组合很灵活，但每次“上课”（推理）时，都需要同时带着教授和笔记本。\n",
        "\n",
        "*   **合并与保存 (Merging)**：现在，我们要进行一个“**知识内化**”的过程。我们让老教授在下课后，花点时间把他那个小小的“智能笔记本”上所有的精华内容，都**亲手誊抄、吸收、并融入**到他自己庞大的知识体系里去。\n",
        "    *   在数学上，这个过程就是 `W_merged = W_base + BA`。即，把原始权重 `W_base` 和我们学到的那个低秩“改变” `BA` **直接加起来**，形成一个新的、完整的权重矩阵 `W_merged`。\n",
        "    *   这个“内化”完成后，那个“智能笔记本”就可以扔掉了。老教授自己，就已经**永久地学会了**“宋朝奶茶史”这门新课。他变成了一位全新的、知识更丰富的教授。\n",
        "\n",
        "**这么做的好处是什么？**\n",
        "\n",
        "1.  **推理速度更快**：\n",
        "    *   在“教授+笔记本”模式下，每次计算都需要先查一下教授的大脑，再查一下笔记本，然后把两个结果结合起来。\n",
        "    *   在“知识内化”后，我们只有一个全新的“超级教授”。每次计算只需要查一次他的大脑就行了，没有了额外的“插件”计算开销，所以**推理速度会显著提升**。\n",
        "\n",
        "2.  **部署和分享更简单**：\n",
        "    *   我们不再需要管理一个基础模型和一堆LoRA插件文件。我们直接拥有一个**单一的、完整的、开箱即用**的新模型。\n",
        "    *   你可以把它直接上传到Hugging Face Hub，你的朋友只需要用一行 `AutoModel.from_pretrained(\"你的用户名/你的甄嬛模型名\")` 就可以直接使用，完全不需要知道任何关于LoRA的细节。\n",
        "\n",
        "#### **2. 编程实践：打造并分享你的“甄嬛”完全体**\n",
        "\n",
        "我们将接着“Chat-嬛嬛”的项目，把训练好的LoRA适配器，与Llama-3-8B基础模型进行合并，然后把它推送到你的Hugging Face主页。\n",
        "\n",
        "**重要准备工作**：\n",
        "1.  **确保你的“Chat-嬛嬛”训练代码已经成功运行完毕**，并且在 `output/huanhuan` 目录下生成了类似 `checkpoint-xxx` 的文件夹。这些文件夹里就装着我们训练好的LoRA“智能笔记本”。\n",
        "2.  **准备好你的Hugging Face账号和Token**，就像我们在入门篇第四步做的那样。\n",
        "\n",
        "**实践代码（请在你训练“Chat-嬛嬛”的Colab笔记本最后，新建一个代码单元格并运行）：**\n",
        "\n",
        "```python\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景一: 准备工作 - 加载我们的“老教授”和“智能笔记本”\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# --- 定义路径 ---\n",
        "# 基础模型的路径，也就是我们之前加载的那个量化版的Llama3\n",
        "base_model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "# LoRA适配器(智能笔记本)的路径。\n",
        "# !! 重要 !! 请确保这里的路径指向你训练完成后的checkpoint文件夹。\n",
        "# 通常是output_dir里最后一个checkpoint，比如 \"output/huanhuan/checkpoint-700\"。\n",
        "# 你需要根据你自己的训练结果，修改下面这个路径！\n",
        "lora_path = \"./output/huanhuan/checkpoint-933\" # <--- !! 修改这里 !!\n",
        "\n",
        "# --- 加载基础模型和翻译官 ---\n",
        "# 我们需要先加载原始的、未经微调的基础模型。\n",
        "# 注意！这次我们不需要加载量化配置了，因为合并后的模型将是一个新的、全精度的模型。\n",
        "# 但是为了能在Colab里加载，我们仍然需要量化它。\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"基础模型和Tokenizer加载完成！\")\n",
        "\n",
        "# --- 加载LoRA适配器并应用到基础模型上 ---\n",
        "# PeftModel.from_pretrained() 这个函数，就像一个“安装插件”的工人。\n",
        "# 它接收一个基础模型(base_model)和LoRA插件的路径(lora_path)，\n",
        "# 然后返回一个“教授+笔记本”模式的、随时可以用来推理的PEFT模型。\n",
        "peft_model = PeftModel.from_pretrained(base_model, lora_path)\n",
        "\n",
        "print(f\"成功将LoRA适配器从 {lora_path} 加载到基础模型上！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景二: 知识内化 - “焊接”模型\n",
        "# ----------------------------------------------------------------------------------\n",
        "# .merge_and_unload() 这就是我们期待已久的“知识内化”魔法！\n",
        "# 执行这行代码后：\n",
        "# 1. 'merge': LoRA的权重会被数学上地“加”到基础模型的权重上去。\n",
        "# 2. 'unload': LoRA插件本身会被从模型中卸载掉。\n",
        "# 执行完毕后，'peft_model' 这个变量，其内容已经变成了一个全新的、完整的、\n",
        "# 知识已经融合进去的“超级教授”模型。\n",
        "merged_model = peft_model.merge_and_unload()\n",
        "\n",
        "print(\"LoRA层已成功合并到基础模型中，并已卸载PEFT插件！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景三: 毕业存档 - 保存与分享\n",
        "# ----------------------------------------------------------------------------------\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# 登录到你的Hugging Face Hub账号。\n",
        "# 会弹出一个框，你需要粘贴你的有'write'权限的Access Token。\n",
        "notebook_login()\n",
        "\n",
        "# --- 保存并上传模型 ---\n",
        "# 给你的新模型在Hugging Face Hub上起一个响亮的名字。\n",
        "# !! 务必替换成 '你的用户名/你的模型名' 格式 !!\n",
        "hub_model_id = \"YourUsername/huanhuan-llama3-8b-lora-merged\" # <--- !! 修改这里 !!\n",
        "\n",
        "# 使用 .push_to_hub() 方法，把我们这个全新的、“知识内化”后的模型推送到云端Hub。\n",
        "# 这会自动创建一个新的模型仓库。\n",
        "# private=True 参数表示将这个模型仓库设置为私有，只有你自己能看到。\n",
        "# 如果你想让所有人都能用，可以去掉这个参数或者设置为 False。\n",
        "merged_model.push_to_hub(hub_model_id, private=True)\n",
        "print(f\"合并后的模型已成功推送到Hub仓库: {hub_model_id}\")\n",
        "\n",
        "# 翻译官也需要一起上传，这样别人用的时候才能正确地编码解码。\n",
        "tokenizer.push_to_hub(hub_model_id, private=True)\n",
        "print(f\"Tokenizer已成功推送到Hub仓库: {hub_model_id}\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景四: 终极检验 - 从你的Hub仓库“一键唤醒”甄嬛\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 现在，最激动人心的时刻到了。\n",
        "# 我们可以像使用任何官方模型一样，只用我们刚刚创建的那个hub_model_id，\n",
        "# 就能一键加载我们自己的、完整的“Chat-嬛嬛”模型了！\n",
        "\n",
        "print(\"\\n--- 正在从你自己的Hub仓库进行最终验证 ---\")\n",
        "\n",
        "# 为了确保是从网络加载，我们先删除内存里的旧模型（可选，但有助于验证）\n",
        "del base_model\n",
        "del peft_model\n",
        "del merged_model\n",
        "\n",
        "# 只用一行代码，从你的个人主页加载完整的“甄嬛”模型。\n",
        "# 注意：因为我们上传的是量化模型合并后的版本，加载时也需要量化。\n",
        "final_model = AutoModelForCausalLM.from_pretrained(\n",
        "    hub_model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "final_tokenizer = AutoTokenizer.from_pretrained(hub_model_id, trust_remote_code=True)\n",
        "\n",
        "print(\"\\n从Hub一键加载'Chat-嬛嬛'成功！\")\n",
        "\n",
        "# --- 进行一次对话测试 ---\n",
        "prompt = \"今日阳光正好，私心想着若是能和你一起走走，那定是极好的。\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一位精通古代宫廷言辞的女子，名叫甄嬛。\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "text = final_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "model_inputs = final_tokenizer([text], return_tensors=\"pt\").to(final_model.device)\n",
        "\n",
        "generated_ids = final_model.generate(model_inputs.input_ids, max_new_tokens=256)\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "response = final_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print('你的提问：', prompt)\n",
        "print('Chat-嬛嬛：', response)\n",
        "```\n",
        "\n",
        "#### **3. 结果解读与作业**\n",
        "\n",
        "当你成功运行完所有代码后：\n",
        "1.  你会看到模型和Tokenizer被成功上传的日志。\n",
        "2.  你可以去 `https://huggingface.co/你的用户名` 这个地址，在你的个人主页上看到一个崭新的、名为 `huanhuan-llama3-8b-lora-merged` 的模型仓库！\n",
        "3.  最终的对话测试会证明，这个从云端一键加载的模型，确实拥有了“甄嬛”的灵魂。\n",
        "\n",
        "**本节总结与作业：**\n",
        "\n",
        "1.  **拥有你的第一个专属大模型**：请务必完成所有步骤，将你的“Chat-嬛嬛”上传到你的Hub主页。这是一个值得在你的简历和个人项目里大书特书的成就！\n",
        "\n",
        "2.  **分享与交流**：尝试把你的模型设置为公开（去掉 `private=True`），然后把模型链接分享给你的朋友或者学习小组的同学，让他们也来和“嬛嬛”聊聊天，看看他们的反馈。\n",
        "\n",
        "3.  **思考题**：我们今天学习了合并LoRA。但有时，我们可能**不想**合并。比如，我有一个基础模型，同时微调了10个不同的LoRA插件（一个用于写诗，一个用于写代码，一个用于扮演甄嬛...）。在这种情况下，你觉得是“为每个插件都合并并保存一个8B的完整模型”好，还是“只保存一个8B的基础模型和10个几MB的LoRA插件，在使用时动态加载”好？为什么？这两种策略分别适用于什么样的场景？\n",
        "\n",
        "恭喜你！你已经完成了我们整个进阶篇的所有核心内容！你已经是一位能够独立完成从数据准备、高效微调、到最终模型部署分享全链路的LLM开发者了！"
      ],
      "metadata": {
        "id": "m6KhQMkk8x34"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6u8qitBf8ySI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mJ4jZ0HtSccm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------------------\n",
        "# 场景一: 环境准备 (已加入新魔法)\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 我们在原来的基础上，额外安装 'bitsandbytes' 库，这是实现模型4位量化的核心魔法工具。\n",
        "# 同时，为了确保版本兼容性，我们最好也把'trl'库装上，它包含了一些有用的训练工具。\n",
        "!pip install transformers datasets accelerate evaluate peft bitsandbytes trl -q\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景二: 数据准备 (下载并处理“甄嬛”的台词)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# 从网络上直接下载处理好的甄嬛传JSON数据集\n",
        "# 这是一个包含了 instruction, input, output 的标准指令数据集\n",
        "!wget https://raw.githubusercontent.com/datawhalechina/self-llm/master/dataset/huanhuan.json\n",
        "\n",
        "# 使用pandas库来读取JSON文件，这是一种处理表格数据非常强大的工具\n",
        "df = pd.read_json('huanhuan.json')\n",
        "\n",
        "# 将pandas的DataFrame对象转换成Hugging Face的Dataset对象，以便后续使用.map()等功能\n",
        "ds = load_dataset(\"json\", data_files=\"huanhuan.json\", split=\"train\")\n",
        "\n",
        "# 打印前3条数据，检查一下我们的“教材”内容是否正确\n",
        "print(\"数据集预览:\")\n",
        "print(ds[:3])\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景三: 加载模型和翻译官 (极限挑战版)\n",
        "# ----------------------------------------------------------------------------------\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 核心修正 ---\n",
        "# 我们不再使用 \"auto\"，而是用 {\"\": 0} 强制告诉加载器：\n",
        "# “我知道有风险，但请把模型的所有部分都加载到0号GPU上！”\n",
        "# 这绕过了加载器的自动保护机制，让我们来直面T4的显存极限。\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map={\"\": 0}, # <--- 关键修改！\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "print(\"\\n模型加载成功，并已强制加载到GPU！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景四: 格式化数据 (为“甄嬛”定制专属“教材”)\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 这个函数用来把我们的 \"instruction, input, output\" 数据，\n",
        "# 转换成Llama3模型认识的、带有特殊标记的对话格式。\n",
        "def process_func(example):\n",
        "    # Llama3的对话模板格式\n",
        "    # <|begin_of_text|> <|start_header_id|> system <|end_header_id|>\n",
        "    # {system_prompt} <|eot_id|>\n",
        "    # <|start_header_id|> user <|end_header_id|>\n",
        "    # {user_prompt} <|eot_id|>\n",
        "    # <|start_header_id|> assistant <|end_header_id|>\n",
        "    # {assistant_response} <|eot_id|>\n",
        "\n",
        "    # 我们将 instruction 和 input 拼接到 user prompt 部分\n",
        "    # output 放到 assistant response 部分\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"user\", \"content\": example['instruction'] + example['input']},\n",
        "            {\"role\": \"assistant\", \"content\": example['output']}\n",
        "        ],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=False  # 我们是训练，不是生成，所以不需要加 assistant 的开头\n",
        "    )\n",
        "    # 编码后的结果会包含 'input_ids', 'attention_mask'\n",
        "    result = tokenizer(text, truncation=True, max_length=512)\n",
        "    # 我们不再手动创建 'labels' 字段，把这个工作完全交给 DataCollator。\n",
        "\n",
        "    return result\n",
        "\n",
        "# 使用.map()函数，将处理函数应用到数据集的每一条数据上\n",
        "tokenized_ds = ds.map(process_func, remove_columns=ds.column_names)\n",
        "\n",
        "print(\"\\n数据处理完成，一条处理后的数据示例:\")\n",
        "print(tokenizer.decode(tokenized_ds[0]['input_ids']))\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景五: 安装并配置LoRA“学习插件”\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 开启梯度检查点，这是一种用时间换空间的技术，能进一步节省显存\n",
        "model.gradient_checkpointing_enable()\n",
        "# 对于PEFT模型，需要执行此方法以确保兼容性\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "# 创建LoRA配置\n",
        "config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    # Llama3的注意力层和全连接层\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    inference_mode=False, # 设置为训练模式\n",
        "    r=8,                  # LoRA的秩\n",
        "    lora_alpha=32,        # LoRA的alpha\n",
        "    lora_dropout=0.1      # Dropout比例\n",
        ")\n",
        "\n",
        "# 将LoRA插件“安装”到我们的量化模型上\n",
        "peft_model = get_peft_model(model, config)\n",
        "\n",
        "# 打印可训练参数，亲眼见证LoRA的威力\n",
        "peft_model.print_trainable_parameters()\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景六: 设置训练计划并启动！\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 设置训练参数\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./output/huanhuan\",\n",
        "\n",
        "    # --- 核心加速配置 ---\n",
        "    per_device_train_batch_size=2,   # 减小批次大小，从4减到2，降低单次计算的显存压力\n",
        "    gradient_accumulation_steps=2,   # 减小梯度累积，从4减到2，让参数更新更频繁\n",
        "    gradient_checkpointing=False,    # !! 关键：关闭梯度检查点 !! 这是最大的提速手段，但会增加显存占用\n",
        "                                     # 我们寄希望于减小batch_size后，显存依然够用\n",
        "\n",
        "    # --- 训练轮数与日志 ---\n",
        "    num_train_epochs=1,              # 为了快速看到结果，我们先只训练1个轮次\n",
        "    logging_steps=5,                 # 每5步就打印一次日志，方便我们观察\n",
        "\n",
        "    # --- 学习率与其他 ---\n",
        "    learning_rate=1e-4,\n",
        "    save_steps=100,\n",
        "    save_on_each_node=True,\n",
        "    report_to=\"none\",\n",
        "\n",
        "    # --- 混合精度训练 ---\n",
        "    # bf16=True,                       # 开启bf16混合精度训练，可以进一步提速并节省显存\n",
        "                                     # (注意: T4 GPU 对 bf16 的支持不是原生的，但通常也能运行并提速)\n",
        ")\n",
        "\n",
        "# 创建Trainer\n",
        "trainer = Trainer(\n",
        "    model=peft_model,                # 我们的学生是：安装了LoRA并已量化的模型\n",
        "    args=args,                       # 教学大纲\n",
        "    train_dataset=tokenized_ds,      # 教材\n",
        "    # 修正！使用更适合Decoder-only模型的DataCollatorForLanguageModeling\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "print(\"\\nLoRA微调即将开始...\")\n",
        "# 启动训练！\n",
        "trainer.train()\n",
        "print(\"训练完成！\")\n",
        "\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 场景七: 与“嬛嬛”对话\n",
        "# ----------------------------------------------------------------------------------\n",
        "# 训练完成后，我们可以用训练好的模型进行对话\n",
        "prompt = \"嬛嬛，我听说御花园的枫叶都红了，我们一起去看看吧。\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一位精通古代宫廷言辞的女子，名叫甄嬛。\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "# 使用apply_chat_template来格式化输入\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "# 生成回复\n",
        "generated_ids = peft_model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=256\n",
        ")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print('你的提问：', prompt)\n",
        "print('Chat-嬛嬛：', response)"
      ],
      "metadata": {
        "id": "ioY9ehFy8zCj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d7234301e7b7459b967d49e6f78172d6",
            "ef6e73be750e48cdaad7b71a0fc48ee7",
            "b304ae95a1c245239e2aee6b93ed9369",
            "183c3f34d54f448281c152c5a07fba91",
            "b1b20c5b859d4387a3c42b789dba92c3",
            "a932f8c5f16e49bc880e2eb08b02893f",
            "3e4d3465875a437a974472d2815ea668",
            "2a4c1db5db5444358b3023bf6236019a",
            "0e0891bfbc734252871d72372fd4a3bc",
            "a4df5e1df55c4606ab4bd07fbbfeb01c",
            "e28a88a39fdc4c6aa2808a5be0a82802",
            "057b2eaa69144273b9f0bf93d5b3689c",
            "883129fa8ce144478fccf04a1ac3ab73",
            "a49261bb90f7431f8a6bcc36008e5af2",
            "eb4abe0cdc5a43ac99011a3e10c23145",
            "0001c1d4bb7e411ea3d73a1e9df4f603",
            "935661e2f74b419fbe72a6b6d774fbc2",
            "1a8aed019fe845d28757e5c5bffd1c59",
            "84099e0c343b4771ab8cffae493268c9",
            "7897014444454ac19983fb0ad0efcd23",
            "d086ed97a36a4b92afc560b923c12baa",
            "db1f595a8f8745dcbb4f63761d5606ef",
            "352e2911aef64cf3b98612e0f6f4b4db",
            "dff204dfb6764b37ab0feaebe1fdd42c",
            "2b8090250164402ca88f2a6dcb3d09d3",
            "5b810a085bad46218843ae2c700d9603",
            "41e8737698f144329204e306b153e2bb",
            "b0fb165e9dad4c7583e9291457dc8ad7",
            "e059be6339604e028d4958bcfcf972b3",
            "17792c709894405c9d7cc6b7201bb0aa",
            "af314cadbe444b22a444448493ee4b4d",
            "bba4f27e7cc446539043f128ba53b107",
            "eaf06a91444049e1b41398cbba1236e6",
            "9ff68eac6ded4fa2a8d00f8f2b8faa91",
            "409d82c7f736452f85eb47a358d021cb",
            "0e1e380318cc46f08e0b987da0de1437",
            "ac1ae4c760d8401a9b5d772087efbd69",
            "4ba96c5195b34afbbfae22dcbe9a7d16",
            "63a08d6224924ae28ad487d9c587fdcc",
            "28c3cdfdfcb34b3caf55cd518a154b88",
            "69af345a6f404afb93abc18c7b1dcd88",
            "fa5b54eee1194d028cbc3ddfbb8e1ffa",
            "912d1a114efb4d80befff0fdc0746611",
            "84367c3a1c2f46c7a66fab46cbd0bcba",
            "17c104423e4541b49f685dcc052f34c0",
            "dd698944d5a44502b2edd02b4f50b718",
            "2ae6190c3d834ce0ab5b02410bcbad12",
            "1cd0674b9cfb40f1821350292ef2000d",
            "a7c275f7ca824003be10bb407071b501",
            "6264dd2018e045ce90be2b6c6a780a24",
            "a39a07d6c2ea4f6a98d14ad964705e67",
            "577ff76c6bb4445998cbc2a29c1168d1",
            "5e9b6e0fb1724010b9b407c5ba46decd",
            "86dadb769b6349f9bff272f7389d68bb",
            "bcfee14d8beb44368d9621bd1aa9b1c6",
            "2169cf43d7db4ea185115a078c113496",
            "9a3dcd96d1984e74a813bb534132ad2c",
            "ce356abfa73b4e759e6b1a66a42f0ad5",
            "932c4d6505904ac7883df4e6ee05bbbe",
            "13f58f03090a4bfca34f4b3d9a66682c",
            "c54e572104454cc1bd6aa2bb96238c79",
            "f13d1b31589d4bf0a72222c9d080d522",
            "ae9da68b8ba74f9eae8c8d26e36ad730",
            "ca4bbd4a00d1471aadca5524201ad393",
            "fef7da1c196e46feb72535ee4332b04f",
            "ae579e0cb314428bb7c65145193d378b",
            "6a058876d2df4d57b27b370f5710f2d6",
            "e633828be9de4d849bde08abb8c2fcfd",
            "e41ebe4e84974a468cf64dc7fd77d530",
            "518215d7047347fb867179e8f1fc00f8",
            "c82fe9f84a6a46128a8e605a082bdb7a",
            "1f68e3e67a174ab8acec0c8f567f0b27",
            "d6ff9a72a8d94ba8a832e8939077995a",
            "5b35c5d054a843a9a0902197494036cd",
            "287567ca2ad348718a595d1c286f27f7",
            "76c5fbda017f4abd988eaf9837742b91",
            "3233f1bf86c7422b8ec54d9a6eb817c8",
            "5674ed86fe4d4acfa58f38dce4988148",
            "3ea2ff1b89d54693ada2624ff0790031",
            "2d6cc21c685c4f74b59a17478a26149d",
            "e221e92982d049ff92ae533bec434fed",
            "659235343a42429ebb83c2b6a0920bd4",
            "e9a84403896f4552bce216374f728140",
            "31ae80ec17504acba4ad83367e8d9d9c",
            "b707c4d23cbb42c78a8933398906f7de",
            "b07c602022df4c5d860ea8f8df493187",
            "2d5b093ca60a4d96bc882cbd6b374b13",
            "5e910626039a4543866fcd0717ddc94e"
          ]
        },
        "outputId": "00d79712-78dd-4a2e-8812-a60c49ee2223"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2025-09-15 12:12:20--  https://raw.githubusercontent.com/datawhalechina/self-llm/master/dataset/huanhuan.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 936793 (915K) [text/plain]\n",
            "Saving to: ‘huanhuan.json’\n",
            "\n",
            "huanhuan.json       100%[===================>] 914.84K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-15 12:12:20 (23.3 MB/s) - ‘huanhuan.json’ saved [936793/936793]\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7234301e7b7459b967d49e6f78172d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据集预览:\n",
            "{'instruction': ['小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——', '这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱们府里跑。', '嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。'], 'input': ['', '', ''], 'output': ['嘘——都说许愿说破是不灵的。', '你们俩话太多了，我该和温太医要一剂药，好好治治你们。', '出来走走，也是散心。']}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "057b2eaa69144273b9f0bf93d5b3689c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "352e2911aef64cf3b98612e0f6f4b4db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ff68eac6ded4fa2a8d00f8f2b8faa91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17c104423e4541b49f685dcc052f34c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2169cf43d7db4ea185115a078c113496",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a058876d2df4d57b27b370f5710f2d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "模型加载成功，并已强制加载到GPU！\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5674ed86fe4d4acfa58f38dce4988148",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3729 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "数据处理完成，一条处理后的数据示例:\n",
            "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "嘘——都说许愿说破是不灵的。<|eot_id|>\n",
            "trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
            "\n",
            "LoRA微调即将开始...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='933' max='933' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [933/933 2:07:57, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.575700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.693400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.403600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.245100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.062300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.144300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>3.378100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.111600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.851100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.936500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.978600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.028200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.974400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.831100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.754200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.712800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.815200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.734100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.768800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.739400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.030100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.730600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>2.762300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.517300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.554500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>2.724900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>2.756800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>2.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.535900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.783600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>2.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.532900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.638200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>2.511300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>2.752900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.671700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.600800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>2.697800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>2.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.443200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>2.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>2.411900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.701100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>2.659600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>2.737400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.633300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>2.520900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.839400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>2.729000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.605700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>2.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.645800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>2.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.849800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>2.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.585600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>2.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.760700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>2.693000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>2.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.573300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>2.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.515200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>2.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.556000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>2.628000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.316400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>2.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>2.181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>2.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>2.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>2.540600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>2.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>2.622800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>2.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.682000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>2.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>2.596600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>2.542000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>2.509600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>2.434400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>2.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>2.376300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>2.767000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>2.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>2.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>2.689300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>2.657800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>2.738900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>2.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>2.485300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>2.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.527100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>2.515900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>2.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>2.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>2.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>2.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>2.498100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>2.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>2.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>2.588700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>2.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>2.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>2.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>2.524200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>2.261200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>2.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>2.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>2.733200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>2.464100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.698500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>2.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>2.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>2.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>2.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>2.224800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>2.601400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>2.407100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>2.412400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>2.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.419200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>2.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>2.495400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>2.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>2.592600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>2.630100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>2.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>2.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>2.140500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>2.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>2.163900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>2.562100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>2.466300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>2.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>2.593100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>2.451400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>2.588400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>2.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>795</td>\n",
              "      <td>2.648200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.121500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>2.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>2.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>815</td>\n",
              "      <td>2.250600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>2.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>2.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>2.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>835</td>\n",
              "      <td>2.331200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>2.329700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>845</td>\n",
              "      <td>2.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.203600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>2.508100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>2.599400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>865</td>\n",
              "      <td>2.610300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>2.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>2.548100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>2.547700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>2.703200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>2.394200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>895</td>\n",
              "      <td>2.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.380600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>905</td>\n",
              "      <td>2.695200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>2.307800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>915</td>\n",
              "      <td>2.597800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>2.469700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>2.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>2.440700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练完成！\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "你的提问： 嬛嬛，我听说御花园的枫叶都红了，我们一起去看看吧。\n",
            "Chat-嬛嬛： 臣\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SiiLdIt8y_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 详细讲解作业和思考题"
      ],
      "metadata": {
        "id": "rhpA54MM9HgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "好的，同学！我们来一起深入探讨一下最后的作业和思考题。这部分内容将帮助你从一个“技术实现者”的角度，上升到“方案设计者”的角度，让你在未来的项目中，能够根据实际需求，做出最明智、最高效的选择。\n",
        "\n",
        "---\n",
        "\n",
        "### **作业讲解：分享与交流**\n",
        "\n",
        "这个作业看似简单，但其意义深远。\n",
        "\n",
        "**1. 为什么要分享？**\n",
        "\n",
        "*   **获得反馈，持续改进**：你自己和“嬛嬛”聊天，可能会陷入思维定式。但你的朋友们会从各种意想不到的角度提出各种刁钻的问题。\n",
        "    *   他们可能会发现，你的“嬛嬛”在回答某些类型的问题时，会“人设崩塌”，突然变回一个普通的AI助手。\n",
        "    *   他们可能会发现，模型在某些话题上会产生重复的回答。\n",
        "    *   他们可能会发现一些有趣的、你没预料到的“涌现”能力。\n",
        "    *   **这些反馈，是比任何自动评估指标都宝贵的“金矿”**，它们能直接告诉你，你的模型在哪些方面做得好，哪些方面还需要用更多、更有针对性的数据去进行第二轮、第三轮的微调。\n",
        "\n",
        "*   **建立个人技术品牌**：在Hugging Face Hub上拥有一个受欢迎的、有特色的模型，就像一个程序员在GitHub上有一个高星(star)项目一样。这是你技术能力最直观、最硬核的证明。它可以帮你吸引同好、找到合作机会，甚至在求职时成为一个巨大的加分项。\n",
        "\n",
        "*   **推动开源社区发展**：你分享的模型，可能会启发其他开发者。有人可能会在你的“Chat-嬛嬛”基础上，继续微调出一个更强大的版本；有人可能会借鉴你的方法，去创造“Chat-孙悟空”或“Chat-林黛玉”。你的工作成为了社区生态的一部分，这就是开源精神的魅力。\n",
        "\n",
        "**2. 交流什么？**\n",
        "\n",
        "当你分享模型时，除了模型本身，你还应该分享：\n",
        "*   **你的训练方法**：你用了什么基础模型？LoRA的配置（`r`值等）是什么？训练了多久？\n",
        "*   **你的数据故事**：你的数据集是怎么来的？你做了哪些清洗和格式化的工作？\n",
        "*   **你的发现和思考**：你遇到了哪些坑？你是如何解决的？你觉得模型的优点和局限性在哪里？\n",
        "*   **一个好的`Model Card`（模型卡片）**：在Hugging Face模型主页上，认真填写`README.md`文件。一个清晰、详尽的模型卡片，是模型能否被社区发现和信任的关键。\n",
        "\n",
        "---\n",
        "\n",
        "### **思考题讲解：合并 vs. 动态加载 —— 两种策略的智慧**\n",
        "\n",
        "**问题：** 我有一个基础模型，同时微调了10个不同的LoRA插件。是“为每个插件都合并并保存10个8B的完整模型”好，还是“只保存一个8B的基础模型和10个几MB的LoRA插件，在使用时动态加载”好？为什么？\n",
        "\n",
        "这是一个典型的**系统设计**问题，没有绝对的对错，只有在特定场景下的优劣。我们来分析这两种策略。\n",
        "\n",
        "#### **策略一：“合并为王” (Merge and Deploy)**\n",
        "\n",
        "*   **做法**：执行10次 `merge_and_unload()`，得到10个独立的、完整的、可以直接部署的模型（比如 `Chat-甄嬛-8B`, `Chat-悟空-8B`, `Code-Helper-8B` ...）。\n",
        "*   **磁盘占用**：`1个基础模型(16G) + 10个LoRA(几十MB) ≈ 16G` (训练时) -> **`10个合并模型(160G)`** (部署时)。磁盘空间占用**暴增10倍**。\n",
        "\n",
        "*   **优点**：\n",
        "    1.  **推理性能最优**：这是最大的优点。每个模型都是一个独立的整体，没有任何额外的计算开销。当你的应用需要**极致的响应速度**时（比如一个高并发的在线聊天服务），这种方式是最好的。\n",
        "    2.  **部署简单、解耦**：每个模型都是一个独立的单元，互不依赖。你可以把“Chat-甄嬛”服务部署在一台机器上，“Code-Helper”服务部署在另一台上，管理起来非常清晰，互不影响。\n",
        "\n",
        "*   **缺点**：\n",
        "    1.  **存储成本极高**：磁盘空间占用是巨大的，这直接转化为存储成本。\n",
        "    2.  **管理和更新困难**：如果有一天，基础模型`Llama-3`升级到了`Llama-4`，你需要重新对10个任务进行微调，然后再次合并、保存、部署10个新的巨大模型，整个过程非常笨重。\n",
        "\n",
        "*   **适用场景**：\n",
        "    *   **生产环境中的高性能、高并发应用**：当你的AI服务已经非常成熟，需要为大量用户提供低延迟服务时，性能是第一位的，存储成本可以被业务价值覆盖。\n",
        "    *   **任务数量少且固定**：如果你只有一两个核心任务，那么为它们分别维护一个合并后的模型是完全可以接受的。\n",
        "    *   **模型分享与分发**：当你希望把你的模型作为一个“最终成品”分享给社区，让用户可以“一键使用”而不需要了解LoRA细节时，合并是最好的方式。\n",
        "\n",
        "#### **策略二：“插件模式” (Dynamic Loading)**\n",
        "\n",
        "*   **做法**：只保存1个基础模型和10个轻巧的LoRA适配器文件。在提供服务时，根据用户的请求，动态地将对应的LoRA插件“插”到基础模型上。\n",
        "*   **磁盘占用**：**`1个基础模型(16G) + 10个LoRA(几十MB) ≈ 16G`**。磁盘空间占用**几乎没有增加**。\n",
        "\n",
        "*   **优点**：\n",
        "    1.  **存储效率极高**：这是最大的优点。极大地节省了存储成本。\n",
        "    2.  **管理和扩展极其灵活**：\n",
        "        *   新增一个任务？只需要再训练一个几MB的LoRA插件即可。\n",
        "        *   基础模型升级？只需要把所有插件在新的基础模型上重新训练一遍（或者如果兼容，直接换），而不需要动10个庞大的部署单元。\n",
        "    3.  **支持大规模、多任务系统**：你可以轻松地支持成百上千个不同的个性化AI（比如为每个VIP用户定制一个专属的LoRA插件），这在合并模式下是不可想象的。\n",
        "\n",
        "*   **缺点**：\n",
        "    1.  **推理性能略有损失**：每次计算时，都需要额外进行LoRA矩阵的计算，会带来一点点的延迟。虽然这个延迟通常很小，但在极端追求性能的场景下可能会被关注。\n",
        "    2.  **增加了系统复杂度**：你的服务需要有一个“适配器管理层”，负责根据请求，动态地加载、卸载、切换LoRA插件。这比直接调用一个固定模型要复杂一些。\n",
        "    3.  **显存管理**：如果多个不同的LoRA插件需要同时在一个GPU上为不同用户服务，你需要更精细地管理显存（虽然现在已经有技术可以支持在同一个基础模型上同时加载多个LoRA适配器）。\n",
        "\n",
        "*   **适用场景**：\n",
        "    *   **需要支持大量不同任务或用户的平台**：比如一个“AI角色扮演”平台，上面有成百上千个不同的角色，每个角色都是一个LoRA插件。\n",
        "    *   **研发和实验阶段**：可以快速地训练和测试各种不同的LoRA模型，而不需要每次都等待漫长的模型合并和保存过程。\n",
        "    *   **资源受限的环境**：在磁盘空间非常宝贵的情况下，这是唯一的选择。\n",
        "\n",
        "---\n",
        "\n",
        "**最终结论**\n",
        "\n",
        "| 特性 | 合并为王 (Merge and Deploy) | 插件模式 (Dynamic Loading) |\n",
        "| :--- | :--- | :--- |\n",
        "| **核心思想** | 性能优先，简单部署 | 效率优先，灵活扩展 |\n",
        "| **推理速度** | **最快** | 略慢 (有微小开销) |\n",
        "| **存储成本** | **极高** | **极低** |\n",
        "| **管理复杂度**| 简单 | 较高 |\n",
        "| **适用场景** | **生产环境、高并发、任务少** | **多任务平台、研发实验、资源受限** |\n",
        "\n",
        "所以，你的选择完全取决于你的**需求**和**场景**。这两种策略都是LoRA生态中非常重要的、需要掌握的工作流。你能够思考到这一层，说明你已经具备了成为一名优秀AI系统架构师的潜力！"
      ],
      "metadata": {
        "id": "yRGVW1pe9KhX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1OyGbhx8y9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dxt982Pj8y6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljcLVsy08y4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSCAPnW18y2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y1vGBiDR8y0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnFz9p8b8yyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcS28O2d8yw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDBG8C1E8yuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NvTF3Av8ysg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qU7YTlKd8yqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1KoUHuy8yon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q8wpQzaq8yml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eQLihmJ8ykm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoCMJcfu8yiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZCB94RT78yf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5lfJQ1aL8ydf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0JHSYru8ybB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t4q4iv2h8yXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zve8GhM58yUs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}