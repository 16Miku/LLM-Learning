{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qTliUCPn1yz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BjLDh3d4oR4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入 torch 库，这是 PyTorch 的核心库，我们将用它来构建神经网络的所有部分。\n",
        "import torch\n",
        "# 导入 torch.nn 模块，它包含了所有构建神经网络所需的类和函数，比如线性层、Dropout层等。\n",
        "# 我们给它一个别名 nn，这是 PyTorch 的一个常用约定。\n",
        "import torch.nn as nn\n",
        "# 导入 math 库，这是一个 Python 的标准数学库，我们将用它来进行一些数学计算，比如 sin, cos, log 等。\n",
        "import math\n",
        "\n",
        "# 这行代码是用来设置 PyTorch 在哪个设备上运行。\n",
        "# torch.cuda.is_available() 会检查你的环境是否支持 NVIDIA 的 CUDA，也就是是否能用 GPU。\n",
        "# 如果可以用 GPU，device 就被设置为 \"cuda\"；否则，就用 \"cpu\"。\n",
        "# 在我们刚刚设置好的 Colab 环境中，这里会选择 \"cuda\"。\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 打印一下我们当前使用的设备，确认一下 GPU 是否设置成功。\n",
        "print(f\"当前使用的设备是: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FH-nMiFboR62",
        "outputId": "3a767db1-055c-4bd8-eb0b-b884630148a3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "当前使用的设备是: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIM3SlSYoR8z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 【修正后的 PositionalEncoding 类】\n",
        "# 定义一个名为 PositionalEncoding 的类。\n",
        "# 它继承自 nn.Module，这是 PyTorch 中所有神经网络模块的基类。\n",
        "# 继承它意味着我们的 PositionalEncoding 类会自动获得很多有用的功能（比如参数管理）。\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    # 这是类的构造函数（initializer），当我们创建一个 PositionalEncoding 的实例时，这个函数会被自动调用。\n",
        "    # 它接收三个参数：\n",
        "    # d_model: 词嵌入的维度（比如 512）。位置编码的维度需要和词嵌入维度相同，这样它们才能相加。\n",
        "    # dropout: 一个介于 0 和 1 之间的浮点数，代表在训练时随机“丢弃”一部分神经元的比例，用于防止过拟合。默认为 0.1。\n",
        "    # max_len: 模型能处理的句子的最大长度（比如 5000）。我们会预先计算好这么长所有位置的位置编码。\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        # super().__init__() 是一个必须的步骤，它调用了父类 nn.Module 的构造函数，以正确地初始化基类。\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义一个 Dropout 层。nn.Dropout 是 PyTorch 提供的现成的层。\n",
        "        # 我们将把它应用在位置编码与词嵌入相加之后的结果上。\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # 创建一个形状为 (max_len, d_model) 的全零张量（tensor），用来存放我们的位置编码。\n",
        "        # 张量是 PyTorch 中最基本的数据结构，可以看作是多维数组。\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # 创建一个形状为 (max_len, 1) 的张量，代表句子的位置索引，即 [0, 1, 2, ..., max_len-1]。\n",
        "        # torch.arange(0, max_len, dtype=torch.float) 会生成一个一维张量 [0., 1., ..., max_len-1.]。\n",
        "        # .unsqueeze(1) 会在第 1 维（从0开始数）增加一个维度，将形状从 [max_len] 变为 [max_len, 1]。\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # 这是计算位置编码公式中分母的部分：10000^(2i / d_model)。\n",
        "        # torch.arange(0, d_model, 2).float() 生成 [0, 2, 4, ..., d_model-2]，代表公式中的 2i。\n",
        "        # 整个表达式计算出了一个包含 d_model/2 个值的张量，用于后续和 position 相乘。\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # 使用切片操作和广播机制，同时计算所有偶数维度（0, 2, 4, ...）的位置编码值。\n",
        "        # pe[:, 0::2] 表示选取所有行，但只选取从第 0 列开始，步长为 2 的列（即偶数列）。\n",
        "        # position * div_term 会利用广播（broadcasting）机制，(max_len, 1) 的 position 会和 (d_model/2) 的 div_term 相乘，\n",
        "        # 得到一个 (max_len, d_model/2) 的结果。\n",
        "        # 最后用 torch.sin 计算正弦值。\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # 同理，计算所有奇数维度（1, 3, 5, ...）的位置编码值。\n",
        "        # pe[:, 1::2] 表示选取所有行，但只选取从第 1 列开始，步长为 2 的列（即奇数列）。\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # 【修正1】: 调整 pe 的形状以匹配 (batch_size, seq_len, d_model) 的输入格式。\n",
        "        # 我们之前的代码对 pe 进行了复杂且错误的变形。\n",
        "        # 正确的做法是，我们只需要在最前面增加一个维度，作为“批次”维度。\n",
        "        # 这样 pe 的形状从 (max_len, d_model) 变为 (1, max_len, d_model)。\n",
        "        # 这个形状可以通过广播（broadcasting）机制，轻松地与 (batch_size, seq_len, d_model) 的输入相加。\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # self.register_buffer('pe', pe) 是一个重要的方法。\n",
        "        # 它将 pe 这个张量注册为模型的“缓冲区”(buffer)。\n",
        "        # 这意味着 pe 是模型状态的一部分（会和模型一起保存、加载，一起被移动到 GPU），\n",
        "        # 但它不是模型的参数（parameters），所以在模型训练时，它的值不会被梯度下降更新。\n",
        "        # 这正是我们想要的，因为位置编码是固定的，不需要学习。\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # forward 方法定义了当数据通过这个模块时，需要执行的计算。\n",
        "    # x: 输入的张量，代表词嵌入。它的形状是 (batch_size, seq_len, d_model)。\n",
        "    def forward(self, x):\n",
        "        # 【修正2】: 修正切片操作，并正确地加上位置编码。\n",
        "        # x.size(1) 现在正确地代表了序列长度 seq_len。\n",
        "        # self.pe[:, :x.size(1), :] 的意思是：\n",
        "        # - 第一个 ':' 表示取批次维度的所有数据（我们只有一个，就是那个 1）。\n",
        "        # - ':x.size(1)' 表示取序列长度维度的前 seq_len 个位置编码。\n",
        "        # - 第二个 ':' 表示取 d_model 维度的所有数据。\n",
        "        # 最终切片出来的 pe 形状是 (1, seq_len, d_model)。\n",
        "        # 当它和形状为 (batch_size, seq_len, d_model) 的 x 相加时，PyTorch 会自动将 pe 的第一维复制 batch_size 次，完成相加。\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "        # 将相加后的结果通过 dropout 层，然后返回。\n",
        "        return self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "yQz8GDycoR_D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1n-3xF1oSBC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 MultiHeadAttention 的类，它同样继承自 nn.Module。\n",
        "# 这是我们 Transformer 模型的核心引擎。\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # d_model: 词嵌入的维度，它必须能被头的数量整除。\n",
        "    # num_heads: 注意力“头”的数量。多头允许模型从不同角度关注信息。\n",
        "    # dropout: Dropout 的比例，默认为 0.1。\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        # 必须的步骤：调用父类 nn.Module 的构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 使用 assert 语句进行一个健全性检查。\n",
        "        # assert 是一个断言，如果后面的条件为 False，程序就会在这里报错。\n",
        "        # 这里我们确保 d_model 必须能够被 num_heads 整除。\n",
        "        # 比如，如果 d_model=512, num_heads=8，那么 512 % 8 == 0，条件为 True，程序继续。\n",
        "        # 如果 d_model=512, num_heads=7，条件为 False，程序会报错，提示我们参数设置有问题。\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        # 初始化实例变量。\n",
        "        self.d_model = d_model      # 模型的总维度\n",
        "        self.num_heads = num_heads  # 注意力头的数量\n",
        "        self.d_k = d_model // num_heads # 每个头的维度。// 是整数除法。例如 512 // 8 = 64。\n",
        "\n",
        "        # 定义四个线性层（Linear Layer），它们本质上就是全连接层，用来进行线性变换（矩阵乘法）。\n",
        "        # nn.Linear(input_features, output_features)\n",
        "        # 这里的四个线性层就对应我们理论中学到的 Wq, Wk, Wv, Wo 权重矩阵。\n",
        "        self.query = nn.Linear(d_model, d_model) # 用于生成 Query 向量\n",
        "        self.key = nn.Linear(d_model, d_model)   # 用于生成 Key 向量\n",
        "        self.value = nn.Linear(d_model, d_model) # 用于生成 Value 向量\n",
        "\n",
        "        # 这是最后一个线性层，对应 Wo 矩阵。它接收拼接后的多头注意力结果，并输出最终的 d_model 维度向量。\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # 定义一个 Dropout 层。\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # query, key, value: 这三个是输入的 Q, K, V 向量。\n",
        "    #   - 在 Encoder 的自注意力层中，这三个输入是完全相同的（都等于上一层的输出）。\n",
        "    #   - 在 Decoder 的 Encoder-Decoder 注意力层中，Query 来自 Decoder，而 Key 和 Value 来自 Encoder 的输出。\n",
        "    # mask: 掩码，用于告诉模型哪些部分是填充（padding）的，不需要关注。或者在 Decoder 中用于防止看到未来的词。\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        # 获取 batch_size（批次大小），也就是一次性处理多少个句子。\n",
        "        # query.shape 是一个元组，例如 (32, 100, 512)，代表 (batch_size, sequence_length, d_model)。\n",
        "        # query.shape[0] 就是获取第一个维度的大小，即 32。\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # 1. 将输入的 query, key, value 通过我们定义的线性层，进行线性变换。\n",
        "        #    这步相当于乘以 Wq, Wk, Wv 矩阵。\n",
        "        #    输入形状: (batch_size, seq_len, d_model)\n",
        "        #    输出形状: (batch_size, seq_len, d_model)\n",
        "        Q = self.query(query)\n",
        "        K = self.key(key)\n",
        "        V = self.value(value)\n",
        "\n",
        "        # 2. 将变换后的 Q, K, V 进行形状重塑（reshape），以便进行多头注意力的计算。\n",
        "        #    我们需要把 d_model 这个维度拆分成 num_heads 和 d_k 两个维度。\n",
        "        #    .view() 函数用于改变张量的形状。\n",
        "        #    原始形状: (batch_size, seq_len, d_model)\n",
        "        #    目标形状: (batch_size, seq_len, num_heads, d_k)\n",
        "        #    .transpose(1, 2) 用于交换维度 1 和 2。\n",
        "        #    最终形状: (batch_size, num_heads, seq_len, d_k)\n",
        "        #    这么做是为了让每个头都能独立地处理整个序列。\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # 3. 计算注意力得分。这对应于我们理论中的公式 (Q * K^T) / sqrt(d_k)\n",
        "        #    torch.matmul() 执行矩阵乘法。\n",
        "        #    K.transpose(-2, -1) 将 K 的最后两个维度进行转置。\n",
        "        #    K 的形状是 (batch_size, num_heads, seq_len_k, d_k)\n",
        "        #    转置后 K^T 的形状是 (batch_size, num_heads, d_k, seq_len_k)\n",
        "        #    Q * K^T 的结果 `energy` 的形状是 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        energy = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # 4. 应用掩码（mask）。\n",
        "        #    如果传入了 mask，就需要将 mask 中值为 0 的位置（通常是 padding 的位置）在 energy 中对应的值设为一个非常小的负数。\n",
        "        #    这样，在下一步进行 softmax 时，这些位置的概率就会趋近于 0，相当于模型忽略了它们。\n",
        "        #    mask == 0 会创建一个布尔张量，padding 的位置是 True，其他位置是 False。\n",
        "        #    .masked_fill_() 是一个原地操作，将 energy 中对应 True 的位置填充为 -1e9 (一个非常小的数)。\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e9\"))\n",
        "\n",
        "        # 5. 对得分进行 softmax 归一化，得到注意力权重。\n",
        "        #    dim=-1 表示在最后一个维度上进行 softmax 操作，确保每一行的权重加起来等于 1。\n",
        "        #    `attention` 的形状和 `energy` 相同: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # 应用 dropout。\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        # 6. 将注意力权重与 V 相乘，得到加权的 Value。\n",
        "        #    torch.matmul(attention, V) 的结果 `x` 的形状是 (batch_size, num_heads, seq_len_q, d_k)\n",
        "        #    这代表了每个头计算出的上下文向量。\n",
        "        x = torch.matmul(attention, V)\n",
        "\n",
        "        # 7. 拼接多头的结果。\n",
        "        #    我们需要把多头计算的结果重新组合成一个 d_model 维度的向量。\n",
        "        #    .transpose(1, 2) 将形状变回 (batch_size, seq_len_q, num_heads, d_k)。\n",
        "        #    .contiguous() 是一个 PyTorch 的操作，它确保张量在内存中是连续存储的，这是 .view() 操作所必需的。\n",
        "        #    .view() 将最后两个维度 (num_heads, d_k) 重新合并为 d_model。\n",
        "        #    最终形状: (batch_size, seq_len_q, d_model)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 8. 将拼接后的结果通过最后一个线性层（对应 Wo 矩阵），得到最终的输出。\n",
        "        #    输入形状: (batch_size, seq_len_q, d_model)\n",
        "        #    输出形状: (batch_size, seq_len_q, d_model)\n",
        "        x = self.fc_out(x)\n",
        "\n",
        "        # 返回最终的输出张量。\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "cL1N1HN8oSC1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzTcV5CdoSEy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 PositionwiseFeedforward 的类，它继承自 nn.Module。\n",
        "# \"Positionwise\" 意味着这个网络会独立地、相同地应用于输入序列中的每一个位置（每一个词）。\n",
        "class PositionwiseFeedforward(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # d_model: 输入和输出的维度。\n",
        "    # d_ff: 内部隐藏层的维度。在原版 Transformer 论文中，这个值通常是 d_model 的 4 倍（例如, d_model=512, d_ff=2048）。\n",
        "    # dropout: Dropout 的比例，默认为 0.1。\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        # 调用父类 nn.Module 的构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义第一个线性层。它将输入从 d_model 维度扩展到 d_ff 维度。\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        # 定义第二个线性层。它将维度从 d_ff 压缩回 d_model。\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        # 定义 Dropout 层。\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 定义 ReLU 激活函数。它为模型引入了非线性，使得模型能学习更复杂的关系。\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # x: 输入张量，形状为 (batch_size, seq_len, d_model)。\n",
        "    def forward(self, x):\n",
        "        # 1. 将输入 x 通过第一个线性层 (fc1)。\n",
        "        #    形状变化: (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        # 2. 将结果通过 ReLU 激活函数。\n",
        "        #    形状不变: (batch_size, seq_len, d_ff)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # 3. 将结果通过 Dropout 层。\n",
        "        #    形状不变: (batch_size, seq_len, d_ff)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # 4. 将结果通过第二个线性层 (fc2)。\n",
        "        #    形状变化: (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # 返回最终的输出张量。\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "lrwssas-oSHA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "llTGrQmQoSOD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 EncoderLayer 的类，它继承自 nn.Module。\n",
        "# 这是构成整个 Encoder 的基本单元。\n",
        "class EncoderLayer(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # d_model: 模型的维度。\n",
        "    # num_heads: 多头注意力的头数。\n",
        "    # d_ff: 前馈网络内部的维度。\n",
        "    # dropout: Dropout 的比例。\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        # 调用父类构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 实例化一个多头自注意力模块。我们直接使用上面定义的 MultiHeadAttention 类。\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # 实例化一个前馈神经网络模块。我们直接使用上面定义的 PositionwiseFeedforward 类。\n",
        "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
        "\n",
        "        # 定义两个层归一化（Layer Normalization）模块。\n",
        "        # nn.LayerNorm 会对输入的最后一个维度（这里是 d_model）进行归一化。\n",
        "        # 这有助于稳定训练过程，加速收敛。\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 定义两个 Dropout 层。\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # x: 输入张量，形状为 (batch_size, seq_len, d_model)。\n",
        "    # mask: 掩码，用于在自注意力计算中忽略 padding 的部分。\n",
        "    def forward(self, x, mask):\n",
        "        # 1. --- 第一个子层：多头自注意力 ---\n",
        "\n",
        "        #    a. 计算多头自注意力的输出。\n",
        "        #       注意，对于自注意力，query, key, value 都是相同的，都等于输入 x。\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "\n",
        "        #    b. 残差连接 (Add) 和层归一化 (Norm)。\n",
        "        #       首先，将注意力层的输出通过一个 dropout 层。\n",
        "        #       然后，将 dropout 后的结果与原始输入 x 相加（这就是残差连接）。\n",
        "        #       最后，将相加的结果通过第一个层归一化模块 (norm1)。\n",
        "        #       这个 `x + ...` 的操作就是 Add & Norm 中的 \"Add\"。\n",
        "        #       `self.norm1(...)` 就是 \"Norm\"。\n",
        "        x = self.norm1(x + self.dropout1(attn_output))\n",
        "\n",
        "        # 2. --- 第二个子层：前馈神经网络 ---\n",
        "\n",
        "        #    a. 计算前馈神经网络的输出。\n",
        "        #       输入是上一个子层归一化后的结果 x。\n",
        "        forward_output = self.feed_forward(x)\n",
        "\n",
        "        #    b. 再次进行残差连接 (Add) 和层归一化 (Norm)。\n",
        "        #       首先，将前馈网络的输出通过第二个 dropout 层。\n",
        "        #       然后，将 dropout 后的结果与该子层的输入 x 相加。\n",
        "        #       最后，将相加的结果通过第二个层归一化模块 (norm2)。\n",
        "        x = self.norm2(x + self.dropout2(forward_output))\n",
        "\n",
        "        # 返回编码器层的最终输出。\n",
        "        # 输出的形状与输入相同: (batch_size, seq_len, d_model)。\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "eb8TBHFIoSQD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMoMKFpqoSSB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 DecoderLayer 的类，它继承自 nn.Module。\n",
        "# 这是构成整个 Decoder 的基本单元。\n",
        "class DecoderLayer(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # d_model: 模型的维度。\n",
        "    # num_heads: 多头注意力的头数。\n",
        "    # d_ff: 前馈网络内部的维度。\n",
        "    # dropout: Dropout 的比例。\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        # 调用父类构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 实例化第一个多头注意力模块，用于解码器自身的“带掩码自注意力”。\n",
        "        # 我们仍然使用 MultiHeadAttention 类，之后在前向传播时传入一个特殊的掩码即可。\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # 实例化第二个多头注意力模块，用于“编码器-解码器注意力”。\n",
        "        # 它关注编码器的输出。\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # 实例化一个前馈神经网络模块。\n",
        "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
        "\n",
        "        # 定义三个层归一化模块，因为我们有三个子层。\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # 定义三个 Dropout 层。\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # x: 解码器的输入张量，形状为 (batch_size, target_seq_len, d_model)。\n",
        "    # enc_output: 编码器的输出张量，形状为 (batch_size, source_seq_len, d_model)。这是解码器需要关注的上下文。\n",
        "    # target_mask: 目标语言的掩码。这既包含了对 padding 的掩码，也包含了防止看到未来的“顺序掩码”(look-ahead mask)。\n",
        "    # source_mask: 源语言的掩码。这只包含了对 padding 的掩码。\n",
        "    def forward(self, x, enc_output, source_mask, target_mask):\n",
        "        # 1. --- 第一个子层：带掩码的多头自注意力 ---\n",
        "\n",
        "        #    a. 计算多头自注意力的输出。\n",
        "        #       这里的 query, key, value 都是解码器的输入 x。\n",
        "        #       我们传入 target_mask 来确保每个位置只能关注到它自己和它前面的位置。\n",
        "        self_attn_output = self.self_attn(x, x, x, target_mask)\n",
        "\n",
        "        #    b. 残差连接和层归一化。\n",
        "        x = self.norm1(x + self.dropout1(self_attn_output))\n",
        "\n",
        "        # 2. --- 第二个子层：编码器-解码器注意力 ---\n",
        "\n",
        "        #    a. 计算编码器-解码器注意力的输出。\n",
        "        #       这里的关键是：\n",
        "        #       - Query (Q) 来自于上一个子层的输出 x (解码器自身的信息)。\n",
        "        #       - Key (K) 和 Value (V) 都来自于编码器的输出 enc_output (源语言句子的信息)。\n",
        "        #       - 我们传入 source_mask，因为它作用于 K 和 V，需要屏蔽掉源语言句子中的 padding 部分。\n",
        "        enc_dec_attn_output = self.enc_dec_attn(x, enc_output, enc_output, source_mask)\n",
        "\n",
        "        #    b. 残差连接和层归一化。\n",
        "        x = self.norm2(x + self.dropout2(enc_dec_attn_output))\n",
        "\n",
        "        # 3. --- 第三个子层：前馈神经网络 ---\n",
        "\n",
        "        #    a. 计算前馈神经网络的输出。\n",
        "        forward_output = self.feed_forward(x)\n",
        "\n",
        "        #    b. 残差连接和层归一化。\n",
        "        x = self.norm3(x + self.dropout3(forward_output))\n",
        "\n",
        "        # 返回解码器层的最终输出。\n",
        "        # 输出的形状与输入 x 相同: (batch_size, target_seq_len, d_model)。\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "YKRk_wA2oST0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5GZ6hjXgoSVy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 Encoder 的类，它继承自 nn.Module。\n",
        "# 整个编码器部分由 N 个相同的 EncoderLayer 堆叠而成。\n",
        "class Encoder(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # input_dim: 输入词典的大小（比如源语言有 30000 个不同的词）。\n",
        "    # d_model: 模型的维度。\n",
        "    # num_layers: 编码器层 (EncoderLayer) 的数量 (原论文中是 6)。\n",
        "    # num_heads: 多头注意力的头数。\n",
        "    # d_ff: 前馈网络内部的维度。\n",
        "    # dropout: Dropout 的比例。\n",
        "    def __init__(self, input_dim, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        # 调用父类构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义词嵌入层 (Embedding Layer)。\n",
        "        # nn.Embedding 会创建一个查找表（lookup table），将每个词的索引（一个整数）映射到一个 d_model 维的向量。\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "\n",
        "        # 实例化我们之前创建的位置编码模块。\n",
        "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # 创建一个模块列表 (ModuleList) 来存放所有的编码器层。\n",
        "        # nn.ModuleList 是一个特殊的列表，它可以正确地注册它包含的所有模块，让 PyTorch 知道它们是模型的一部分。\n",
        "        # 我们使用一个 for 循环来创建 num_layers 个 EncoderLayer 实例。\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # 定义一个 Dropout 层。\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # src: 源语言句子的输入张量，形状为 (batch_size, src_len)，内容是词的索引。\n",
        "    # mask: 源语言的掩码。\n",
        "    def forward(self, src, mask):\n",
        "        # 1. 将输入的词索引通过嵌入层，转换为词向量。\n",
        "        #    形状变化: (batch_size, src_len) -> (batch_size, src_len, d_model)\n",
        "        src = self.embedding(src)\n",
        "\n",
        "        # 2. 将位置编码添加到词向量上。\n",
        "        #    形状不变: (batch_size, src_len, d_model)\n",
        "        src = self.pos_encoding(src)\n",
        "\n",
        "        # 3. 让数据依次通过 ModuleList 中的每一个编码器层。\n",
        "        #    我们使用一个 for 循环来遍历 self.layers。\n",
        "        #    在每一层，输入是上一层的输出。\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, mask)\n",
        "\n",
        "        # 返回编码器最终的输出。\n",
        "        # 形状: (batch_size, src_len, d_model)\n",
        "        return src\n"
      ],
      "metadata": {
        "id": "T99yTFkUoSYB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kl8WsBxKoSaI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个名为 Decoder 的类，它继承自 nn.Module。\n",
        "# 整个解码器部分由 N 个相同的 DecoderLayer 堆叠而成。\n",
        "class Decoder(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # output_dim: 输出词典的大小（比如目标语言有 32000 个不同的词）。\n",
        "    # d_model, num_layers, num_heads, d_ff, dropout: 参数含义与 Encoder 相同。\n",
        "    def __init__(self, output_dim, d_model, num_layers, num_heads, d_ff, dropout=0.1):\n",
        "        # 调用父类构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        # 定义目标语言的词嵌入层。\n",
        "        self.embedding = nn.Embedding(output_dim, d_model)\n",
        "\n",
        "        # 实例化位置编码模块。\n",
        "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        # 创建一个 ModuleList 来存放所有的解码器层。\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # 定义一个最终的线性层。\n",
        "        # 这个层的作用是将解码器最后一层的输出（d_model 维度）映射到整个目标词典的大小（output_dim 维度）。\n",
        "        # 这样，对于每个位置，我们都能得到一个代表每个词得分的向量。\n",
        "        self.fc_out = nn.Linear(d_model, output_dim)\n",
        "\n",
        "        # 定义 Dropout 层。\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # forward 方法定义了前向传播的逻辑。\n",
        "    # trg: 目标语言句子的输入张量，形状为 (batch_size, trg_len)。\n",
        "    # enc_src: 编码器的输出，形状为 (batch_size, src_len, d_model)。\n",
        "    # trg_mask: 目标语言的掩码。\n",
        "    # src_mask: 源语言的掩码。\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "        # 1. 将目标语言的词索引通过嵌入层，转换为词向量。\n",
        "        #    形状变化: (batch_size, trg_len) -> (batch_size, trg_len, d_model)\n",
        "        trg = self.embedding(trg)\n",
        "\n",
        "        # 2. 将位置编码添加到词向量上。\n",
        "        #    形状不变: (batch_size, trg_len, d_model)\n",
        "        trg = self.pos_encoding(trg)\n",
        "\n",
        "        # 3. 让数据依次通过 ModuleList 中的每一个解码器层。\n",
        "        #    每一层都需要接收上一步的输出 trg, 编码器的输出 enc_src, 以及两种掩码。\n",
        "        for layer in self.layers:\n",
        "            trg = layer(trg, enc_src, src_mask, trg_mask)\n",
        "\n",
        "        # 4. 将解码器最后一层的输出通过最终的线性层 fc_out。\n",
        "        #    形状变化: (batch_size, trg_len, d_model) -> (batch_size, trg_len, output_dim)\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        # 返回最终的输出（也称为 logits）。\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "0ygJj1_YoScY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x40Jlr8GoSec"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义最终的 Transformer 模型类。\n",
        "class Transformer(nn.Module):\n",
        "    # 类的构造函数。\n",
        "    # encoder: 一个 Encoder 类的实例。\n",
        "    # decoder: 一个 Decoder 类的实例。\n",
        "    # src_pad_idx: 源语言中填充符号 <pad> 的索引。\n",
        "    # trg_pad_idx: 目标语言中填充符号 <pad> 的索引。\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx):\n",
        "        # 调用父类构造函数。\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "\n",
        "    # 定义一个方法来创建源语言的掩码。\n",
        "    # src: 源语言输入张量，形状为 (batch_size, src_len)。\n",
        "    def make_src_mask(self, src):\n",
        "        # 1. 检查 src 张量中哪些元素等于填充索引。\n",
        "        #    (src != self.src_pad_idx) 会生成一个布尔张量，padding 的位置是 False，非 padding 是 True。\n",
        "        #    形状: (batch_size, src_len)\n",
        "        # 2. 在最后两个维度上增加一个维度，以匹配多头注意力的期望形状。\n",
        "        #    .unsqueeze(1).unsqueeze(2) 会将形状变为 (batch_size, 1, 1, src_len)。\n",
        "        #    这个形状可以和注意力得分矩阵 (batch_size, num_heads, seq_len, seq_len) 进行广播。\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    # 定义一个方法来创建目标语言的掩码。\n",
        "    # trg: 目标语言输入张量，形状为 (batch_size, trg_len)。\n",
        "    def make_trg_mask(self, trg):\n",
        "        # 1. 创建 padding 掩码，逻辑与 src_mask 相同。\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        # 形状: (batch_size, 1, 1, trg_len)\n",
        "\n",
        "        # 2. 创建“顺序”掩码 (look-ahead mask)，防止看到未来的词。\n",
        "        trg_len = trg.shape[1]\n",
        "        #    torch.tril() 会创建一个下三角矩阵。\n",
        "        #    torch.ones((trg_len, trg_len)) 创建一个全 1 的方阵。\n",
        "        #    torch.tril(...) 会将这个方阵的右上部分变为 0。\n",
        "        #    例如 trg_len=3, 结果是:\n",
        "        #    [[1, 0, 0],\n",
        "        #     [1, 1, 0],\n",
        "        #     [1, 1, 1]]\n",
        "        #    .to(device) 确保掩码和数据在同一个设备上（CPU 或 GPU）。\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=device)).bool()\n",
        "        # 形状: (trg_len, trg_len)\n",
        "\n",
        "        # 3. 将 padding 掩码和顺序掩码结合起来。\n",
        "        #    使用逻辑与 (&) 操作。只有当两个掩码在某个位置都为 True 时，最终结果才为 True。\n",
        "        #    trg_pad_mask 的形状是 (batch_size, 1, 1, trg_len)\n",
        "        #    trg_sub_mask 的形状是 (trg_len, trg_len)\n",
        "        #    PyTorch 的广播机制会自动处理这两个不同形状的张量。\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    # forward 方法定义了整个模型的前向传播。\n",
        "    # src: 源语言输入。\n",
        "    # trg: 目标语言输入。\n",
        "    def forward(self, src, trg):\n",
        "        # 1. 创建源语言和目标语言的掩码。\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # 2. 将源语言和掩码输入编码器，得到编码器的输出。\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # 3. 将目标语言、编码器输出以及两种掩码输入解码器，得到最终的输出。\n",
        "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # 返回输出。\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "3M915Tt7oShA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wHuMyV2oSjM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XpZ1qXFoSlK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 模型超参数定义 ---\n",
        "\n",
        "# INPUT_DIM: 输入词典的大小。\n",
        "# 假设我们的源语言（例如中文）词典中有 5000 个独特的词。\n",
        "INPUT_DIM = 5000\n",
        "\n",
        "# OUTPUT_DIM: 输出词典的大小。\n",
        "# 假设我们的目标语言（例如英文）词典中有 5000 个独特的词。\n",
        "OUTPUT_DIM = 5000\n",
        "\n",
        "# D_MODEL: 模型的维度，也就是词嵌入向量的维度。\n",
        "# 这是 Transformer 模型中的一个核心维度，它贯穿整个模型。\n",
        "# 论文中设置为 512。\n",
        "D_MODEL = 512\n",
        "\n",
        "# NUM_LAYERS: 编码器和解码器中堆叠的层数。\n",
        "# 论文中设置为 6。\n",
        "NUM_LAYERS = 6\n",
        "\n",
        "# NUM_HEADS: 多头注意力机制中的“头”数。\n",
        "# 注意：D_MODEL 必须能够被 NUM_HEADS 整除 (512 % 8 == 0)。\n",
        "# 论文中设置为 8。\n",
        "NUM_HEADS = 8\n",
        "\n",
        "# D_FF: 前馈神经网络内部的隐藏层维度。\n",
        "# 论文中建议设置为 D_MODEL 的 4 倍。\n",
        "D_FF = 2048\n",
        "\n",
        "# DROPOUT: Dropout 的比例，用于防止过拟合。\n",
        "# 论文中设置为 0.1。\n",
        "DROPOUT = 0.1\n",
        "\n",
        "# SRC_PAD_IDX: 源语言中，用于填充（padding）的特殊符号 <pad> 在词典中的索引。\n",
        "# 我们假设它的索引是 0。这个值在创建掩码时至关重要。\n",
        "SRC_PAD_IDX = 0\n",
        "\n",
        "# TRG_PAD_IDX: 目标语言中，用于填充的特殊符号 <pad> 在词典中的索引。\n",
        "# 我们也假设它的索引是 0。\n",
        "TRG_PAD_IDX = 0\n"
      ],
      "metadata": {
        "id": "5TsAS1vfoSnW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8mzHTjlAoSpY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 模型实例化 ---\n",
        "\n",
        "# 1. 实例化编码器 (Encoder)。\n",
        "#    我们将上面定义的所有相关超参数作为参数传入。\n",
        "enc = Encoder(INPUT_DIM,\n",
        "              D_MODEL,\n",
        "              NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              D_FF,\n",
        "              DROPOUT)\n",
        "\n",
        "# 2. 实例化解码器 (Decoder)。\n",
        "dec = Decoder(OUTPUT_DIM,\n",
        "              D_MODEL,\n",
        "              NUM_LAYERS,\n",
        "              NUM_HEADS,\n",
        "              D_FF,\n",
        "              DROPOUT)\n",
        "\n",
        "# 3. 实例化最终的 Transformer 模型。\n",
        "#    它接收我们刚刚创建的编码器和解码器实例，以及填充索引。\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX)\n",
        "\n",
        "# 4. 将模型移动到我们之前设置好的设备上（GPU 或 CPU）。\n",
        "#    .to(device) 是一个 PyTorch 方法，用于将模型的所有参数和缓冲区移动到指定的设备。\n",
        "#    这是使用 GPU 加速所必需的步骤。\n",
        "model.to(device)\n",
        "\n",
        "# (可选) 打印一下模型的总参数数量，感受一下它的规模。\n",
        "def count_parameters(model):\n",
        "    # sum(p.numel() for p in model.parameters() if p.requires_grad) 是一个 Python 的生成器表达式。\n",
        "    # model.parameters() 会返回模型所有可学习的参数（权重和偏置）。\n",
        "    # p.requires_grad 检查这个参数是否需要计算梯度（即是否在训练中被更新）。\n",
        "    # p.numel() 返回参数 p 中元素的总数。\n",
        "    # sum(...) 将所有参数的元素数量加起来，得到模型的总参数量。\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# 打印总参数量，并用逗号进行格式化，使其更易读。\n",
        "print(f'这个模型共有 {count_parameters(model):,} 个可训练的参数')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc8435SgoSrr",
        "outputId": "b180d11c-6f9e-4fca-b1ad-07a3ed103d2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "这个模型共有 51,823,496 个可训练的参数\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b09SFFoooStc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 健全性检查 ---\n",
        "\n",
        "# 设置一些用于测试的维度\n",
        "BATCH_SIZE = 128  # 批次大小，即一次处理 128 个句子。\n",
        "SRC_LEN = 30      # 源语言句子的长度，假设为 30 个词。\n",
        "TRG_LEN = 35      # 目标语言句子的长度，假设为 35 个词。\n",
        "\n",
        "# 创建一个假的源语言输入张量。\n",
        "# torch.randint(low, high, size) 会生成一个在 [low, high) 区间内的随机整数张量。\n",
        "# 我们生成的值在 [0, INPUT_DIM) 之间，模拟词典中的词索引。\n",
        "# size=(BATCH_SIZE, SRC_LEN) 指定了张量的形状。\n",
        "# .to(device) 确保这个数据张量也在 GPU 上，与模型在同一个设备。\n",
        "src = torch.randint(0, INPUT_DIM, (BATCH_SIZE, SRC_LEN)).to(device)\n",
        "\n",
        "# 创建一个假的目标语言输入张量。\n",
        "trg = torch.randint(0, OUTPUT_DIM, (BATCH_SIZE, TRG_LEN)).to(device)\n",
        "\n",
        "# 打印一下输入数据的形状，以便对比。\n",
        "print(\"输入 src 的形状:\", src.shape)\n",
        "print(\"输入 trg 的形状:\", trg.shape)\n",
        "print(\"-\" * 30) # 打印一条分割线\n",
        "\n",
        "# 核心步骤：将假数据喂给模型，进行一次完整的前向传播。\n",
        "# model(src, trg) 会自动调用我们 Transformer 类中定义的 forward 方法。\n",
        "output = model(src, trg)\n",
        "\n",
        "# 打印输出数据的形状。\n",
        "print(\"模型输出 output 的形状:\", output.shape)\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# --- 验证输出形状 ---\n",
        "# 我们期望的输出形状应该是 (BATCH_SIZE, TRG_LEN, OUTPUT_DIM)\n",
        "# 因为对于目标序列中的每一个词，模型都应该预测一个在整个目标词典上的得分分布。\n",
        "expected_shape = (BATCH_SIZE, TRG_LEN, OUTPUT_DIM)\n",
        "if output.shape == expected_shape:\n",
        "    print(\"🎉 恭喜！模型结构正确，健全性检查通过！\")\n",
        "    print(f\"输出形状 ({output.shape}) 与期望形状 ({expected_shape}) 完全一致。\")\n",
        "else:\n",
        "    print(\"😥 模型结构似乎有问题，请检查代码。\")\n",
        "    print(f\"输出形状是 {output.shape}, 但我们期望的是 {expected_shape}。\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU8hWbyyoSvs",
        "outputId": "92f644b3-b888-4e4c-a9fc-26e7723fc346"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "输入 src 的形状: torch.Size([128, 30])\n",
            "输入 trg 的形状: torch.Size([128, 35])\n",
            "------------------------------\n",
            "模型输出 output 的形状: torch.Size([128, 35, 5000])\n",
            "------------------------------\n",
            "🎉 恭喜！模型结构正确，健全性检查通过！\n",
            "输出形状 (torch.Size([128, 35, 5000])) 与期望形状 ((128, 35, 5000)) 完全一致。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u03PhZaToSxu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-6HkZ7svoSz0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NVqnP1qboS2C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_A4U3-MRoS4I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xGJa4VxVoS6L"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__sE93H2oS8I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nih7gS_goS-W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPkfP4-roTAf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YT0utu8oTCd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0N_s8z6oTEb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3c9QGaLcoTGw"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}